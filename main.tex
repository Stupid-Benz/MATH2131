%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.1.1 (14/2/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

\usepackage{tabularx} % For tabbing to a certain point
\usepackage{stmaryrd} % For \mapsfrom symbol

\newcommand{\End}[1]{\text{End}(#1)} % Endomorphism ring
\newcommand{\Hom}[2]{\text{Hom}(#1, #2)} % Hom-set


\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
    \coordinate [below=12cm] (midpoint) at (current page.north);
    \node at (current page.north west)
    {\begin{tikzpicture}[remember picture,overlay]
            \node[anchor=north west,inner sep=0pt] at (0,0) {\includegraphics[width=\paperwidth]{background}}; % Background image
            \draw[anchor=north] (midpoint) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering MATH 2131\\[15pt] % Book title
            {\Large Honors in Linear and Abstract Algebra I}\\[20pt] % Subtitle
            {\huge Prof. Meng}}}; % Author name
        \end{tikzpicture}};
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2013 John Smith\\ % Copyright notice

\noindent \textsc{Published by Publisher}\\ % Publisher

\noindent \textsc{book-website.com}\\ % URL

\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information

\noindent \textit{First printing, March 2013} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\usechapterimagefalse % If you don't want to include a chapter image, use this to toggle images off - it can be enabled later with \usechapterimagetrue

\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Part One}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Abstract Linear Spaces}

\section{Binary Operation}\index{Binary Operation}

\begin{definition}[Binary Operation]
    A \emph{binary operation} on a set $S$ is a mapping of the elements of the Cartesian product $S \times S$ to $S$.
    \[ \begin{split}
            f : S \times S & \to S \\ (x,y) &\mapsto f(x,y)
        \end{split}\]
\end{definition}

\begin{example}
    A common example of a binary operation is addition on the set of natural numbers $\mathbb{N}$.
    \begin{equation}
        \begin{split}
            + : \mathbb{N} \times \mathbb{N} & \to \mathbb{N} \\ (x,y) &\mapsto x+y
        \end{split}
    \end{equation}
\end{example}

\begin{definition}[Associative Operation]
    A binary operation $f: S \times S \to S$ is said to be \emph{associative} if, for all $x,y,z \in S$, 
    \[ f(x,f(y,z)) = f(f(x,y),z) \]
\end{definition}

\begin{example}
    A common example of an associative (binary) operation is addition on the set of natural numbers $\mathbb{N}$. For all $x,y,z \in \mathbb{N}$, we have $x + (y + z) = (x + y) + z$.
\end{example}

\begin{definition}[Identifiable Operation]
    A binary operation $f: S \times S \to S$ is said to be \emph{identifiable}, or \emph{unital}, if there exists an element $e \in S$, the \emph{identity} or \emph{unit element}, such that, for all $x \in S$
    \[ f(e,x) = x = f(x,e) \]
\end{definition}

\begin{example}
    A common example of an identifiable (binary) operation is multiplication on the set of natural numbers $\mathbb{N}$. The identity element is $1$, and for all $x \in \mathbb{N}$, we have $x \cdot 1 = x = 1 \cdot x$.
\end{example}

\begin{proposition}
    The identity element of an identifiable operation is unique.
\end{proposition}

\begin{proof}
    Let $e_1$ and $e_2$ be two identity elements for the operation $f$. Then, for any element $x \in S$, we have:
    \[ f(x,e_1) = x = f(e_1,x) \]
    \[ f(x,e_2) = x = f(e_2,x) \]
    Now, consider the element $e_1$:
    \[ f(e_1,e_2) = e_1 \]
    But since $e_2$ is an identity element, we also have:
    \[ f(e_1,e_2) = e_2 \]
    Therefore, we conclude that $e_1 = e_2$, proving the uniqueness of the identity element.
\end{proof}

\begin{remark}
    Two-sided identity must be unique, but one-sided identities need not be.
\end{remark}

\begin{example}
    
\end{example}

\begin{definition}[Inverse Operation]
    A binary operation $f: S \times S \to S$ is said to be \emph{invertible} if, for every element $x \in S$, there exists an element $y \in S$, called the two-sided \emph{inverse} of $x$, denoted as $x^{-1}$, such that
    \[ f(x,y) = e = f(y,x) \]
    where $e$ is the identity element of the operation.
\end{definition}

\begin{remark}
    Invertible operation exists if inverse operation exists, i.e., there exists an identity element.
\end{remark}

\begin{example}
    A common example of an invertible (binary) operation is addition on the set of integers $\mathbb{Z}$. For every integer $x \in \mathbb{Z}$, there exists an integer $y = -x$ such that:
    \begin{equation}
        x + (-x) = 0 = (-x) + x
    \end{equation}
    where $0$ is the identity element for addition.
\end{example}

\begin{proposition}
    The inverse element of an invertible operation is unique.
\end{proposition}

\begin{proof}
    Let $y_1$ and $y_2$ be two inverses of an element $x \in S$. Then, by definition of inverse, we have:
    \[ f(x,y_1) = e = f(y_1,x) \]
    \[ f(x,y_2) = e = f(y_2,x) \]
    Now, consider the element $y_1$:
    \[ f(y_1,x) = e \]
    But since $y_2$ is also an inverse of $x$, we can substitute $e$ with $f(x,y_2)$:
    \[ f(y_1,x) = f(x,y_2) = e \]
    By the associativity of the operation, we can rearrange this to:
    \[ y_1 = f(y_1, e) = f(y_1,f(x,y_2)) = f(f(y_1,x),y_2) = f(e,y_2) = y_2 \]
    Thus, the inverse element is unique.
\end{proof}

\begin{definition}[Commutative Operation]
    A binary operation $f: S \times S \to S$ is said to be \emph{commutative} if, for all $x,y \in S$, the following holds:
    \[ f(x,y) = f(y,x) \]
\end{definition}

\begin{example}
    A common example of a commutative operation is addition on the set of integers $\mathbb{Z}$. For all $x,y \in \mathbb{Z}$, we have:
    \[ x + y = y + x \]
\end{example}

\begin{definition}[Distributive Operation (Harmonic Property)]
    A binary operation $g: S \times S \to S$ is said to be \emph{distributive} with respect to another binary operation $f: S \times S \to S$ if, for all $x,y,z \in S$, the following holds:
    \[ \begin{split}
        g(x,f(y,z)) &= f(g(x,y),g(x,z)) \\
        g(f(y,z),x) &= f(g(y,x),g(z,x))
    \end{split} \]
\end{definition}

\begin{example}
    A common example of a distributive operation is multiplication over addition on the set of integers $\mathbb{Z}$. For all $x,y,z \in \mathbb{Z}$, we have:
    \[ \begin{split}
        x \cdot (y + z) &= x \cdot y + x \cdot z \\
        (y + z) \cdot x &= y \cdot x + z \cdot x
    \end{split} \]
\end{example}

\newpage

\section{Groups, Rings, Fields}\index{Groups, Rings, Fields}

\begin{definition}[Monoid]
    A \emph{monoid} is a set $M$ equipped with a binary operation $f: M \times M \to M$ such that the following properties hold:
    \begin{enumerate}
        \item \emph{Closure Property:} For all $x,y \in M$, $f(x,y) \in M$.
        \item \emph{Associative Property}
        \item \emph{Identifiable Property}
    \end{enumerate}
    We say $(M, f)$ is a monoid, and $f$ is the \emph{monoid operation} on the set $M$. A set $M$ with a monoid operation $f$ is the \emph{monoid structure}.
\end{definition}

\begin{definition}[Group]
    A \emph{group} is a set $G$ equipped with a monoid operation $f: G \times G \to G$ with the additional property that every element has an inverse, \emph{Invertible Property}.
\end{definition}

\begin{example}
    $(\mathbb{R}\setminus\{0\}, \times)$ is a group, but $(\mathbb{R}, \times)$ is not a group since $0$ does not have a multiplicative inverse.
\end{example}

\begin{definition}[Abelian Monoid / Group]
    A monoid / group $(G, f)$ is said to be an \emph{abelian monoid / group} if the monoid / group operation $f$ is commutative, \emph{Commutative Property}.
\end{definition}

\begin{definition}[Unital Ring]
    A (unital) ring is a set $R$ equipped with two binary operations $f: R \times R \to R$ (addition) and $g: R \times R \to R$ (multiplication) such that the following properties hold:
    \begin{enumerate}
        \item \emph{Additive Group:} $(R, f)$ is an abelian group.
        \item \emph{Multiplicative Monoid:} $(R, g)$ is a monoid.
        \item \emph{Distributive Property:} $g$ with respect to $f$.
    \end{enumerate}
\end{definition}

\begin{definition}[Commutative Ring]
    A \emph{commutative ring} is a unital ring $R$ such that the multiplication operation $g: R \times R \to R$ is commutative.
\end{definition}

\begin{example}
    $(\mathbb{Z}, +, \times)$ is a unital commutative ring.
\end{example}

\begin{definition}[Field]
    A \emph{field} is a unital commutative ring $F$ such that every non-zero element has a multiplicative inverse.
\end{definition}

\begin{example}
    $(\mathbb{Q}, +, \times)$, $(\mathbb{R}, +, \times)$ and $(\mathbb{C}, +, \times)$ are fields.
\end{example}

\begin{example}
    $(\mathbb{Z}/2\mathbb{Z}, +, \times)$ is a field, where $\mathbb{Z}/2\mathbb{Z} = \{\bar{0}, \bar{1}\}$, $\bar{0}$ is the set of even integers and $\bar{1}$ is the set of odd integers.
    It follows the additions and multiplications below:
    \begin{equation}
        \begin{array}{c|cc}
              + & \bar{0} & \bar{1} \\ \hline
            \bar{0} & \bar{0} & \bar{1} \\
            \bar{1} & \bar{1} & \bar{0}
        \end{array}
        \quad
        \begin{array}{c|cc}
              \times & \bar{0} & \bar{1} \\ \hline
            \bar{0} & \bar{0} & \bar{0} \\
            \bar{1} & \bar{0} & \bar{1}
        \end{array}
    \end{equation}
\end{example}

\newpage

\section{Morphisms}\index{Morphisms}

\begin{definition}[Morphisms]
    A \emph{morphism} is a structure-preserving map between two algebraic structures (e.g., groups, rings, fields). Formally, let $(A, \cdot_A)$ and $(B, \cdot_B)$ be two algebraic structures. A morphism $f: A \to B$ is a set map / function such that:
    \[
        f(x \cdot_A y) = f(x) \cdot_B f(y) \quad \forall x, y \in A
    \]
\end{definition}

\begin{definition}[Monoid Homomorphism]
    A \emph{monoid homomorphism} is a morphism between two monoids that preserves the monoid structure. Formally, let $(M_1, \cdot_1)$ and $(M_2, \cdot_2)$ be two monoids with identity elements $e_1$ and $e_2$, respectively. A function $f: M_1 \to M_2$ is a monoid homomorphism if:
    \begin{enumerate}
        \item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in M_1$
        \item $f(e_1) = e_2$
    \end{enumerate}
\end{definition}

\begin{definition}[Group Homomorphism]
    A \emph{group homomorphism} is a morphism between two groups that preserves the group structure. Formally, let $(G_1, \cdot_1)$ and $(G_2, \cdot_2)$ be two groups with identity elements $e_1$ and $e_2$, respectively. A function $f: G_1 \to G_2$ is a group homomorphism if:
    \begin{enumerate}
        \item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in G_1$
        \item $f(e_1) = e_2$
        \item $f(x^{-1}) = (f(x))^{-1} \quad \forall x \in G_1$
    \end{enumerate}
\end{definition}

\begin{proposition}
    The second and third properties of a group homomorphism are consequences of the first property.
\end{proposition}

\begin{proof}
    Let $f: G_1 \to G_2$ be a group homomorphism satisfying the first property. We will show that the second and third properties follow from it.

    \textbf{Second Property:} To show that $f(e_1) = e_2$, we use the fact that $e_1$ is the identity element in $G_1$. For any element $x \in G_1$, we have:
    \[
        f(x) = f(x \cdot_1 e_1) = f(x) \cdot_2 f(e_1)
    \]
    Since $f(x)$ is an arbitrary element in $G_2$, this implies that $f(e_1)$ must be the identity element in $G_2$, i.e., $f(e_1) = e_2$.

    \textbf{Third Property:} To show that $f(x^{-1}) = (f(x))^{-1}$ for all $x \in G_1$, we use the fact that $x^{-1}$ is the inverse of $x$ in $G_1$. We have:
    \[
        e_2 = f(e_1) = f(x \cdot_1 x^{-1}) = f(x) \cdot_2 f(x^{-1})
    \]
    This shows that $f(x^{-1})$ is the inverse of $f(x)$ in $G_2$, i.e., $f(x^{-1}) = (f(x))^{-1}$.

    Therefore, both the second and third properties of a group homomorphism are indeed consequences of the first property.
\end{proof}

\begin{remark}
    For monoid homomorphisms, the second property cannot be derived from the first property. Consider the identity element $e_1$ in $M_1$. If we apply the first property, we get $f(e_1 \cdot_1 e_1) = f(e_1) \cdot_2 f(e_1)$. This simplifies to $f(e_1) = f(e_1) \cdot_2 f(e_1)$, which does not necessarily imply that $f(e_1)$ is the identity element in $M_2$, i.e., $f(e_1) \neq e_2$, but $f(e_1)$ is the idempotent element in $M_2$. Therefore, the second property must be explicitly stated for monoid homomorphisms.

    However in the case of group homomorphisms, the existence of inverses ensures that there is only one element that can be idempotent under the group operation, which is the identity element. Thus, for group homomorphisms, the second property can be derived from the first property.
\end{remark}

\begin{definition}[Ring Homomorphism]
    A \emph{ring homomorphism} is a morphism between two rings that preserves both the additive and multiplicative structures. Formally, let $(R_1, +_1, \cdot_1)$ and $(R_2, +_2, \cdot_2)$ be two rings with identity elements $0_1$, $1_1$ and $0_2$, $1_2$, respectively. A function $f: R_1 \to R_2$ is a ring homomorphism if:
    \begin{enumerate}
        \item $f(x +_1 y) = f(x) +_2 f(y) \quad \forall x, y \in R_1$
        \item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in R_1$
        \item $f(1_1) = 1_2$
    \end{enumerate}    
\end{definition}

\begin{definition}[Endomorphism]
    An \emph{endomorphism} is a morphism from an algebraic structure to itself. Formally, let $(A, \cdot)$ be an algebraic structure. An endomorphism $f: A \to A$ is a set map such that:
    \[
        f(x \cdot y) = f(x) \cdot f(y) \quad \forall x, y \in A
    \]
\end{definition}

\begin{definition}[Hom-set]
    The set of all morphisms from an algebraic structure $A$ to another algebraic structure $B$ is called the \emph{hom-set}, denoted by $\Hom{A}{B}$. 
\end{definition}

\begin{definition}[Endomorphism Ring]
    The set of all endomorphisms of an abelian group $(A, +)$, denoted by $\End{A}$, forms a (non-commutative) ring under pointwise addition and composition of set maps. The addition and multiplication operations are defined as follows:
    \[
        \begin{split}
            + : \End{A} \times \End{A} &\to \End{A} \\
            (f,g) &\mapsto (f+g: x \mapsto f(x) + g(x)) \qquad &f + g : A \to A \\ \\
            \circ : \End{A} \times \End{A} &\to \End{A} \\
            (f,g) &\mapsto (f \circ g: x \mapsto f(g(x))) \qquad &f \circ g : A \to A
        \end{split}
    \]
    The identity element for addition is the zero endomorphism, which maps every element to the identity element of the group. 
    \[ \begin{split}
        0: A &\to A \\
        x &\mapsto 0
    \end{split}
    \]
    The identity element for multiplication is the identity endomorphism, which maps every element to itself. 
    \[
    \begin{split}
        1: A &\to A \\
        x &\mapsto x
    \end{split}
    \]
    Note that all endomorphisms in $\End{A}$ are group homomorphisms and $\End{A} = \Hom{A}{A}$.
\end{definition}

\newpage

\section{Vector Spaces}\index{Vector Spaces}

\begin{definition}[Linear Structure]
    A \emph{linear structure} over a field $F$ on a set $V$ is a pair $(+, \cdot)$ where $(V, +)$ is an abelian group with a ring homomorphism $F \to \End{V}$, where $\End{V}$ is the endomorphism ring of the abelian group $(V, +)$.
    \[ \begin{split}
            \cdot : F &\to \End{V} \\
            \alpha &\mapsto (\alpha\cdot : \vec{x} \mapsto \alpha \vec{x}) \qquad \alpha \cdot : V \to V
        \end{split}
    \]
    The ring homomorphism is a (ring) action of the field $F$ on the abelian group $(V, +)$, called \emph{scalar multiplication}. The ring action can be written as a binary operation:
    \[
        \begin{split}
            \cdot : F \times V &\to V \\
            (\alpha, \vec{x}) &\mapsto \alpha \vec{x}
        \end{split}
    \]
\end{definition}

\begin{definition}[Linear Spaces / Vector Spaces]
    A linear space / vector space is a set with a linear structure over a field on the set.
\end{definition}

\begin{corollary}[Linear Spaces]
    A linear space over a field $F$ is a set $V$ equipped with two operations: vector addition $+: V \times V \to V$ and scalar multiplication $\cdot : F \times V \to V$, satisfying the following axioms for all $\vec{u}, \vec{v}, \vec{w} \in V$ and $\alpha, \beta \in F$:
    \begin{center}
        \begin{tabularx}{\textwidth}{XX}
            \toprule
            \textbf{Axiom} & \textbf{Statement} \\
            \midrule
            1. Associativity of addition & $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$ \\
            2. Existence of additive identity & $\exists \vec{0} \in V$ such that $\forall \vec{u} \in V$, $\vec{u} + \vec{0} = \vec{u}$ \\
            3. Existence of additive inverses & $\forall \vec{u} \in V$, $\exists -\vec{u} \in V$ such that $\vec{u} + (-\vec{u}) = \vec{0}$ \\
            4. Commutativity of addition & $\vec{u} + \vec{v} = \vec{v} + \vec{u}$ \\
            5. Distributivity of scalar multiplication with respect to vector addition & $\alpha (\vec{u} + \vec{v}) = \alpha \vec{u} + \alpha \vec{v}$ \\
            6. Distributivity of scalar multiplication with respect to field addition & $(\alpha + \beta) \cdot = \alpha \cdot + \beta \cdot$ \\
            7. Compatibility of scalar multiplication with field multiplication & $(\alpha \beta) \cdot = (\alpha \cdot) \circ (\beta \cdot)$ \\
            8. Identity element of scalar multiplication & $F \ni 1 = (1\cdot : x \mapsto x) \in \End{V}$ \\
            \bottomrule
        \end{tabularx}
    \end{center}
\end{corollary}

\begin{remark}
    Note that the first four axioms ensure that $(V, +)$ is an abelian group, while the fifth axiom describes the endomorphism structure and the last three axioms describe the ring homomorphism.
\end{remark}

\begin{example}
    $F$ is a linear space over itself with the usual addition and multiplication operations.
    \[
        \begin{split}
            \cdot : F \times F &\to F \\
            (\alpha,\beta) &\mapsto \alpha \beta
        \end{split}
    \]
    The first $F$ is the field acting on the second $F$, which is the abelian group.
\end{example}

\newpage

\begin{example}
    Let $X$ be a set and $F$ be a field. ($f$ is a set map)
    \[
    \begin{split}
        F[[X]] = \text{Map}(X, F) &\overset{\mathrm{def}}{=\joinrel=} \text{the set of all $F$-valued functions on } X \\
        &=\joinrel= \left\{ f : X \to F \right\}
    \end{split}
    \]
    $F[[X]]$ is a linear space over $F$ with the following operations defined pointwisely:
    \[
        \begin{split}
            + : F[[X]] \times F[[X]] &\to F[[X]] \\
            (f,g) &\mapsto (f+g: x \mapsto f(x) + g(x)) \qquad f + g : X \to F \\ \\
            \cdot : F \times F[[X]] &\to F[[X]] \\
            (\alpha,f) &\mapsto (\alpha f: x \mapsto \alpha f(x)) \qquad \alpha f : X \to F
        \end{split}
    \]
\end{example}

\begin{example}
    Let $X$ be a set and $F$ be a field.
    \[
        \begin{split}
            F[X] = \text{Map}_{\text{fin}}(X, F) &\overset{\mathrm{def}}{=\joinrel=} \text{the set of all finitely supported $F$-valued functions on } X \\
            &=\joinrel= \left\{ f : X \to F \mid f \text{ is finitely supported} \right\}
        \end{split}
    \]
    $F[X]$ is a linear space over $F$ as $F[X] \subseteq F[[X]]$ and the operations are defined pointwisely as in the previous example.

    $f: X \to F$ is finitely supported if the set $\{ x \in X \mid f(x) \neq 0 \}$ is finite or $f(x) \neq 0$ for only finitely many $x \in X$.
\end{example}

\begin{example}
    Let $t$ be a formal variable. Then $F[[t]] \overset{\mathrm{def}}{=\joinrel=} F[[\{ 1, t, t^2, \cdots \}]] = \sum_{n = 0}^{\infty} a_n t^n$ is the set of all formal power series in $t$ with coefficients in $F$ and $F[t] \overset{\mathrm{def}}{=\joinrel=} F[\{ 1, t, t^2, \cdots \}] = \sum_{n = 0}^{N} a_n t^n$ is the set of all polynomials in $t$ with coefficients in $F$. Both $F[[t]]$ and $F[t]$ are linear spaces over $F$.
\end{example}

\begin{example}
    Let $n$ be a positive integer and $F$ be a field. Then 
    \[
        F^n \overset{\mathrm{def}}{=\joinrel=} \left\{ 
        \begin{bmatrix}
        c_1 \\
        \vdots \\
        c_n
        \end{bmatrix}
        \;\middle|\; c_i \in F
        \right\}
    \]
    is the set of all \emph{column matrices} with $n$ entries in $F$. Elements in $F^n$ are written as $\vec{x}$ and are called \emph{column vectors}. $F^n$ is a linear space over $F$ with the following operations defined entrywisely:
    \[
        \begin{split}
            + : F^n \times F^n &\to F^n \\
            (\vec{a}, \vec{b}) &\mapsto \vec{a} + \vec{b} = \begin{bmatrix}
            a_1 + b_1 \\
            \vdots \\
            a_n + b_n
            \end{bmatrix} \\ \\
            \cdot : F \times F^n &\to F^n \\
            (\alpha, \vec{a}) &\mapsto \alpha \vec{a} = \begin{bmatrix}
            \alpha a_1 \\
            \vdots \\
            \alpha a_n
            \end{bmatrix}
        \end{split}
    \]
    $F^n$ is a linear space over $F$ automatically as $F$ is a linear space over itself.
\end{example}

%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapter{Linear Maps and Matrices}

\section{Linear Maps}\index{Linear Maps}

\begin{definition}[Linear Maps]
    Let $V$ and $W$ be two linear spaces over a field $F$. A linear map is a set map $f: V \to W$ such that for all $\vec{u}, \vec{v} \in V$ and $\alpha \in F$, the following holds:
    \[
        \begin{split}
            f(\vec{u} + \vec{v}) &= f(\vec{u}) + f(\vec{v}) \\
            f(\alpha \vec{u}) &= \alpha f(\vec{u})
        \end{split}
    \]
    The set of all linear maps from $V$ to $W$ is denoted by $\mathcal{L}(V, W) = \Hom{V}{W}$.
\end{definition}

\begin{definition}[Linear Combinations]
    Let $V$ be a linear space over a field $F$. A \emph{linear combination} of vectors $\vec{v_1}, \vec{v_2}, \cdots, \vec{v_n} \in V$ is a vector of the form:
    \[
        \alpha_1 \vec{v_1} + \alpha_2 \vec{v_2} + \cdots + \alpha_n \vec{v_n}
    \]
    where $\alpha_1, \alpha_2, \cdots, \alpha_n \in F$ are scalars.
\end{definition}

\begin{corollary}[Linear Maps and Linear Combinations]
    A set map $f: V \to W$ between two linear spaces over a field $F$ is a linear map if and only if $f$ respects linear combinations, i.e., for all $\vec{v_1}, \vec{v_2} \in V$ and all scalars $\alpha_1, \alpha_2 \in F$, the following holds:
    \[
        f(\alpha_1 \vec{v_1} + \alpha_2 \vec{v_2}) = \alpha_1 f(\vec{v_1}) + \alpha_2 f(\vec{v_2})
    \]
\end{corollary}

\begin{example}
    Let $A$ be an $m \times n$ matrix with entries in a field $F$. The map $T: F^n \to F^m$ defined by
    \[
        T\vec{x} = T(\vec{x}) = A \vec{x}
    \]
    where the multiplication on the right-hand side is the usual matrix multiplication, is a linear map over $F$.
\end{example}

\begin{proposition}
    A linear map $T: F^n \to F^m$ is a matrix multiplication by a unique $m \times n$ matrix $A$ with entries in $F$. The matrix $A$ is called the \emph{standard matrix} of the linear map $T$.
    \[
        \begin{split}
            \{ \text{Linear maps over } F \} &\equiv \{ m \times n \text{ matrices over } F \} \\
            A\cdot : \vec{x} \mapsto A\vec{x} &\mapsfrom A \\
            T &\mapsto A = \begin{bmatrix}
                T\vec{e_1} & T\vec{e_2} & \cdots & T\vec{e_n}
            \end{bmatrix}
        \end{split}
    \]
    where the $\equiv$ sign means they are natural identifications and $\vec{e_i} \in F^n$ is the $i$-th standard basis vector, i.e., the column matrix with $1$ in the $i$-th row and $0$ elsewhere.
\end{proposition}

\begin{proof}
    Consider a column matrix $\vec{x} \in F^n$ with entries $x_1, x_2, \cdots, x_n \in F$. Then $\vec{x}$ can be expressed as a linear combination of the standard basis vectors $\vec{e_1}, \vec{e_2}, \cdots, \vec{e_n}$:
    \[
        \vec{x} = x_1 \vec{e_1} + x_2 \vec{e_2} + \cdots + x_n \vec{e_n} = \sum_{i=1}^{n} x_i \vec{e_i}
    \]
    Since $T$ is a linear map, it respects linear combinations. Therefore, we have:
    \[
        T\vec{x} = T\left( \sum_{i=1}^{n} x_i \vec{e_i} \right) = \sum_{i=1}^{n} x_i T(\vec{e_i}) = \sum_{i=1}^{n} x_i \vec{a_i} = A\vec{x}
    \]
    where $\vec{a_i} = T(\vec{e_i})$ is the $i$-th column of the matrix $A = \begin{bmatrix}
        T\vec{e_1} & T\vec{e_2} & \cdots & T\vec{e_n}
    \end{bmatrix}$. Thus, we have $T\vec{x} = A\vec{x}$ for all $\vec{x} \in F^n$. This shows that $T$ can be represented as a matrix multiplication by the matrix $A$.
\end{proof}

\begin{remark}
    There is a simpler way to write $\sum_{i=1}^{n} x_i \vec{e_i}$: The Eienstein summation convention. When an index variable appears twice in a single term and is not otherwise defined, it implies summation of that term over all the values of the index. Therefore, we can write:
    \[
        \vec{x} = x_i \vec{e_i}
    \]
    where $i$ is summed from $1$ to $n$.
\end{remark}

\begin{definition}[Homogeneous Linear Functions]
    A linear map $f: F^n \to F$ is called a \emph{homogeneous linear function} or a \emph{linear functional} if it satisfies the property:
    \[
        f(\alpha \vec{x}) = \alpha f(\vec{x}) \quad \text{for all } \alpha \in F \text{ and } \vec{x} \in F^n.
    \]
\end{definition}

\begin{corollary}[Standard Matrix of a Linear Map]
    The standard matrix of a linear map $T: F^n \to F^m$ can be written as:
    \[
        A = \begin{bmatrix}
            f_1(\vec{e_1}) & f_1(\vec{e_2}) & \cdots & f_1(\vec{e_n}) \\
            f_2(\vec{e_1}) & f_2(\vec{e_2}) & \cdots & f_2(\vec{e_n}) \\
            \vdots & \vdots & \ddots & \vdots \\
            f_m(\vec{e_1}) & f_m(\vec{e_2}) & \cdots & f_m(\vec{e_n})
        \end{bmatrix}
    \]
    where $f_i: F^n \to F$ is the $i$-th component function of $T$, i.e., $T\vec{x} = \begin{bmatrix}
        f_1(\vec{x}) \\
        f_2(\vec{x}) \\
        \vdots \\
        f_m(\vec{x})
    \end{bmatrix}$ for all $\vec{x} \in F^n$.
\end{corollary}

\begin{remark}
    Each component function $f_i$ is a homogeneous linear function, and the standard matrix $A$ is constructed by evaluating these functions at the standard basis vectors $\vec{e_1}, \vec{e_2}, \cdots, \vec{e_n}$ of $F^n$.
\end{remark}

\begin{example}
    Let $D: F[t] \to F[t]$ be the differentiation operator defined by:
    \[
        D\left( \sum_{n=0}^{N} a_n t^n \right) = \sum_{n=1}^{N} n a_n t^{n-1}
    \]
    for all polynomials $\sum_{n=0}^{N} a_n t^n \in F[t]$. The differentiation operator $D$ is a linear map over $F$. The standard matrix of $D$ with respect to the standard basis $\{1, t, t^2, \cdots, t^N\}$ of $F[t]$ is given by:
    \[
        A = \begin{bmatrix}
            0 & 1 & 0 & 0 & \cdots & 0 \\
            0 & 0 & 2 & 0 & \cdots & 0 \\
            0 & 0 & 0 & 3 & \cdots & 0 \\
            \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & 0 & \cdots & N \\
            0 & 0 & 0 & 0 & \cdots & 0
        \end{bmatrix}
    \]
\end{example}

\newpage

\section{Injective and Surjective Linear Maps and Linear Equivalences}\index{Injective and Surjective Linear Maps and Linear Equivalences}

\begin{definition}[Injective Linear Maps]
    A linear map $f: V \to W$ between two linear spaces over a field $F$ is said to be \emph{injective} (or one-to-one) if for all $\vec{u}, \vec{v} \in V$, the following holds:
    \[
        f(\vec{u}) = f(\vec{v}) \implies \vec{u} = \vec{v}
    \]
    Equivalently, $f$ is injective if the only vector in $V$ that maps to the zero vector in $W$ is the zero vector itself:
    \[
        f(\vec{u}) = 0 \implies \vec{u} = 0
    \]
\end{definition}

\begin{definition}[Surjective Linear Maps]
    A linear map $f: V \to W$ is said to be \emph{surjective} (or onto) if for every $\vec{w} \in W$, there exists at least one $\vec{v} \in V$ such that:
    \[
        f(\vec{v}) = \vec{w}
    \]
\end{definition}

\begin{definition}[Linear Equivalences / Isomorphisms]
    A linear map $T: V \to W$ is called a \emph{linear equivalence}, \emph{isomorphism} or \emph{invertible linear map} if $T$ has a unique two-sided inverse $S$, denoted by $T^{-1}$, i.e., there exists a linear map $S: W \to V$ such that:
    \[
        TS = e_W \quad \text{and} \quad ST = e_V
    \]
    where $e_V: V \to V$ and $e_W: W \to W$ are the identity maps on $V$ and $W$, respectively.
    In this case, we say that the linear spaces $V$ and $W$ are \emph{isomorphic}, denoted by $V \cong W$.
\end{definition}

\begin{corollary}[Linear Equivalences]
    A linear map $T: V \to W$ is a linear equivalence if and only if $T$ is both injective and surjective, i.e., bijective / one-to-one correspondence.
\end{corollary}

\begin{proof}
    (\(\Rightarrow\)) Assume \(T: V \to W\) is a linear equivalence. By definition, there exists a linear map \(S: W \to V\) such that \(TS = e_W\) and \(ST = e_V\).

    To show that \(T\) is injective, suppose \(T(\vec{u}) = T(\vec{v})\) for some \(\vec{u}, \vec{v} \in V\). Applying \(S\) to both sides, we have:
    \[
        S(T(\vec{u})) = S(T(\vec{v})) \implies (ST)(\vec{u}) = (ST)(\vec{v}) \implies e_V(\vec{u}) = e_V(\vec{v}) \implies \vec{u} = \vec{v}
    \]
    Thus, \(T\) is injective.

    To show that \(T\) is surjective, let \(\vec{w} \in W\). Since \(TS = e_W\), we have:
    \[
        T(S(\vec{w})) = e_W(\vec{w}) = \vec{w}
    \]
    This shows that for every \(\vec{w} \in W\), there exists a \(\vec{v} = S(\vec{w}) \in V\) such that \(T(\vec{v}) = \vec{w}\). Thus, \(T\) is surjective.

    (\(\Leftarrow\)) Now assume that \(T: V \to W\) is both injective and surjective. We need to show that there exists a linear map \(S: W \to V\) such that \(TS = e_W\) and \(ST = e_V\).

    Since \(T\) is surjective, for each \(\vec{w} \in W\), there exists at least one \(\vec{v} \in V\) such that \(T(\vec{v}) = \vec{w}\). Define the map \(S: W \to V\) by choosing one such preimage for each \(\vec{w}\):
    \[
        S(\vec{w}) = \text{a chosen } \vec{v} \text{ such that } T(\vec{v}) = \vec{w}
    \]
    To show that \(S\) is well-defined, we need to ensure that if \(T(\vec{v_1}) = T(\vec{v_2})\), then \(\vec{v_1} = \vec{v_2}\). This follows from the injectivity of \(T\).
    Now we verify that \(TS = e_W\):
    \[
        (TS)(\vec{w}) = T(S(\vec{w})) = \vec{w}
    \]
    for all \(\vec{w} \in W\). Thus, \(TS = e_W\).
    Next, we verify that \(ST = e_V\):
    \[
        (ST)(\vec{v}) = S(T(\vec{v})) = \vec{v}
    \]
    for all \(\vec{v} \in V\). Thus, \(ST = e_V\).
    Therefore, \(T\) has a two-sided inverse \(S\), and hence \(T\) is a linear equivalence.
\end{proof}

\begin{proposition}
    Let $T: V \to W$ be a linear map between two linear spaces over a field $F$ and $T^{-1}: W \to V$ be the set-theoretical inverse of $T$. Then $T^{-1}$ is also a linear map.
\end{proposition}

\begin{proof}
    Since \(T\) is a linear equivalence, it is bijective, and thus has a well-defined set-theoretical inverse \(T^{-1}: W \to V\). We need to show that \(T^{-1}\) is a linear map, i.e., it respects vector addition and scalar multiplication.

    Let \(\vec{w_1}, \vec{w_2} \in W\) and \(\alpha \in F\). We will show that:
    \[
        T^{-1}(\vec{w_1} + \vec{w_2}) = T^{-1}(\vec{w_1}) + T^{-1}(\vec{w_2})
    \]
    and
    \[
        T^{-1}(\alpha \vec{w_1}) = \alpha T^{-1}(\vec{w_1})
    \]

    Since \(T\) is surjective, there exist \(\vec{v_1}, \vec{v_2} \in V\) such that \(T(\vec{v_1}) = \vec{w_1}\) and \(T(\vec{v_2}) = \vec{w_2}\). Then we have:
    \[
        T^{-1}(\vec{w_1} + \vec{w_2}) = T^{-1}(T(\vec{v_1}) + T(\vec{v_2})) = T^{-1}(T(\vec{v_1} + \vec{v_2})) = \vec{v_1} + \vec{v_2}
    \]
    On the other hand, we also have:
    \[
        T^{-1}(\vec{w_1}) + T^{-1}(\vec{w_2}) = \vec{v_1} + \vec{v_2}
    \]
    Thus, we conclude that:
    \[
        T^{-1}(\vec{w_1} + \vec{w_2}) = T^{-1}(\vec{w_1}) + T^{-1}(\vec{w_2})
    \]

    Next, we show that \(T^{-1}\) respects scalar multiplication. For any scalar \(\alpha \in F\), we have:
    \[
        T^{-1}(\alpha \vec{w_1}) = T^{-1}(\alpha T(\vec{v_1})) = T^{-1}(T(\alpha \vec{v_1})) = \alpha \vec{v_1}
    \]
    On the other hand, we also have:
    \[
        \alpha T^{-1}(\vec{w_1}) = \alpha \vec{v_1}
    \]
    Thus, we conclude that:
    \[
        T^{-1}(\alpha \vec{w_1}) = \alpha T^{-1}(\vec{w_1})
    \]
    Therefore, \(T^{-1}\) respects both vector addition and scalar multiplication, and hence \(T^{-1}\) is a linear map.
\end{proof}

\begin{proposition}
    Let $X$ be a set and $W$ be a linear space over a field $F$. Then the set of all set maps from $X$ to $W$, denoted by $\text{Map}(X, W)$, is a linear space over $F$ with the following operations defined pointwisely:
    \[
        \begin{split}
            + : \text{Map}(X, W) \times \text{Map}(X, W) &\to \text{Map}(X, W) \\
            (f,g) &\mapsto (f+g: x \mapsto f(x) + g(x)) \qquad f + g : X \to W \\ \\
            \cdot : F \times \text{Map}(X, W) &\to \text{Map}(X, W) \\
            (\alpha,f) &\mapsto (\alpha f: x \mapsto \alpha f(x)) \qquad \alpha f : X \to W
        \end{split}
    \]
\end{proposition}

\begin{proposition}
    Let $V$ and $W$ be two linear spaces over a field $F$. Then $\Hom{V}{W}$ is a linear space over $F$ with the following operations defined pointwisely:
    \[
        \begin{split}
            + : \Hom{V}{W} \times \Hom{V}{W} &\to \Hom{V}{W} \\
            (f,g) &\mapsto (f+g: \vec{v} \mapsto f(\vec{v}) + g(\vec{v})) \qquad f + g : V \to W \\ \\
            \cdot : F \times \Hom{V}{W} &\to \Hom{V}{W} \\
            (\alpha,f) &\mapsto (\alpha f: \vec{v} \mapsto \alpha f(\vec{v})) \qquad \alpha f : V \to W
        \end{split}
    \]
\end{proposition}

\begin{proof}
    Note that $\Hom{V}{W} \subseteq \text{Map}(V, W)$. We need to show that the operations defined above are closed in $\Hom{V}{W}$, i.e., for all $f, g \in \Hom{V}{W}$ and $\alpha \in F$, $f + g \in \Hom{V}{W}$ and $\alpha f \in \Hom{V}{W}$ or equivalently, $f$ respects linear combinations.

    Let $\vec{u}, \vec{v} \in V$ and $\alpha, \beta \in F$. Since $f, g \in \Hom{V}{W}$, we have:
    \[
        \begin{split}
            (f + g)(\alpha\vec{u} + \beta\vec{v}) &\overset{\mathrm{def}}{=\joinrel=} f(\alpha\vec{u} + \beta\vec{v}) + g(\alpha\vec{u} + \beta\vec{v}) \\
            &\overset{\mathrm{lin}}{=\joinrel=} \alpha f(\vec{u}) + \beta f(\vec{v}) + \alpha g(\vec{u}) + \beta g(\vec{v}) \\
            &=\joinrel= \alpha (f(\vec{u}) + g(\vec{u})) + \beta (f(\vec{v}) + g(\vec{v})) \\
            &\overset{\mathrm{def}}{=\joinrel=} \alpha (f + g)(\vec{u}) + \beta (f + g)(\vec{v})
        \end{split}
    \]
    where "lin" denotes the linearity of $f$ and $g$. Thus, $f + g \in \Hom{V}{W}$ and $\alpha f \in \Hom{V}{W}$.
\end{proof}

\begin{remark}
    Note that $\End{V} = \Hom{V}{V}$ is a linear space over $F$ and also a ring with the addition and multiplication operations defined in the previous section. The addition operation is commutative, but the multiplication operation is not necessarily commutative.
\end{remark}

\begin{definition}[Characteristic of a Field]
    The \emph{characteristic} of a field $F$ is the smallest positive integer $n$ such that:
    \[
        \underbrace{1 + 1 + \cdots + 1}_{n \text{ times}} = 0
    \]
    If no such positive integer exists, the characteristic of $F$ is defined to be $0$.    
\end{definition}

\begin{example}
    The differentiation operator $D: F[t] \to F[t]$ is not an injective linear map as $D(1) = 0 = D(2)$ but is a surjective linear map if $F$ is a field of characteristic $0$.
\end{example}

\newpage

\section{Dimension of Vector Spaces}\index{Dimension of Vector Spaces}

\begin{definition}[Finite Dimensional Vector Spaces]
    A linear space $V$ over a field $F$ is said to be \emph{finite dimensional} if there exists a linear equivalence $T: V \to F^n$ for some positive integer $n$. In this case, we say that the dimension of $V$ is $n$, denoted $\dim V = n$.
\end{definition}

\begin{definition}[Infinite Dimensional Vector Spaces]
    A linear space $V$ over a field $F$ is said to be \emph{infinite dimensional} if $V$ is not finite dimensional.
\end{definition}

%----------------------------------------------------------------------------------------

\end{document}
