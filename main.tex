%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 3.1 (February 18, 2022)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@latextemplates.com)
% Mathias Legrand (legrand.mathias@gmail.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s indexstyle.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	11pt, % Default font size, select one of 10pt, 11pt or 12pt
	fleqn, % Left align equations
	a4paper, % Paper size, use either 'a4paper' for A4 size or 'letterpaper' for US letter size
	%oneside, % Uncomment for oneside mode, this doesn't start new chapters and parts on odd pages (adding an empty page if required), this mode is more suitable if the book is to be read on a screen instead of printed
]{LegrandOrangeBook}

% Book information for PDF metadata, remove/comment this block if not required 
\hypersetup{
	pdftitle={Title}, % Title field
	pdfauthor={Author}, % Author field
	pdfsubject={Subject}, % Subject field
	pdfkeywords={Keyword1, Keyword2, ...}, % Keywords
	pdfcreator={LaTeX}, % Content creator field
}

\addbibresource{sample.bib} % Bibliography file

\definecolor{ocre}{RGB}{0,83,166} % Define the color used for highlighting throughout the book

\chapterimage{orange1.jpg} % Chapter heading image
\chapterspaceabove{8cm} % Default whitespace from the top of the page to the chapter title on chapter pages
\chapterspacebelow{8cm} % Default amount of vertical whitespace from the top margin to the start of the text on chapter pages

%----------------------------------------------------------------------------------------

\usepackage{tabularx} % For tabbing to a certain point
\usepackage{tikz}
\usepackage{tikz-cd} % For commutative diagrams
\usepackage{tikz-3dplot}
\usetikzlibrary{matrix, positioning, arrows.meta, decorations.pathreplacing, decorations.markings} % <-- Add this line to enable matrix of math nodes

\newcommand{\smallbullet}{\scalebox{0.5}{$\bullet$}}

\newcommand{\End}[1]{\mathsf{End}(#1)} % Endomorphism ring
\newcommand{\Hom}{\mathsf{Hom}} % Hom-set
\renewcommand{\ker}[1]{\mathsf{Ker}(#1)} % Kernel of a linear map
\newcommand{\im}[1]{\mathsf{Im}(#1)} % Image of a linear map
\renewcommand{\span}[1]{\mathsf{Span}(#1)} % Span of a set of vectors
\renewcommand{\bar}[1]{\overline{#1}} % Bar notations
\newcommand{\quotient}[2]{#1/\, #2} % Quotient notation
\newcommand{\F}{\mathbb{F}} % Field notation
\newcommand{\M}[2]{\mathsf{M}_{#1}(#2)} % Matrix set notation
\newcommand{\Map}{\mathsf{Map}} % Map notation
\newcommand{\rank}[1]{\mathsf{Rank}(#1)} % Rank of a matrix
\newcommand{\coker}[1]{\mathsf{Coker}(#1)} % Cokernel of a linear map
\newcommand{\coim}[1]{\mathsf{Coim}(#1)} % Coimage of a linear map
\newcommand{\nullity}[1]{\mathsf{Nullity}(#1)} % Nullity of a linear map
\newcommand{\id}{\mathsf{id}} % Identity map notation
\newcommand{\nul}[1]{\mathsf{Nul}(#1)} % Null space of a linear map
\newcommand{\col}[1]{\mathsf{Col}(#1)} % Column space of a matrix
\newcommand{\C}{\mathcal{C}} % Category notation
\newcommand{\D}{\mathcal{D}} % Another category notation
\newcommand{\Set}{\boldsymbol{\mathsf{Set}}} % Set category notation
\newcommand{\Vect}{\boldsymbol{\mathsf{Vec}}} % Vector space category notation
\newcommand{\Mor}{\mathsf{Mor}} % Morphism set notation
\newcommand{\Ob}[1]{\mathsf{Ob}(#1)} % Object set notation
\newcommand{\B}{\mathcal{B}} % Basis notation
\newcommand{\R}{\mathbb{R}} % Real number notation
\newcommand{\T}{\mathcal{T}^{\smallbullet}} % Tensor algebra notation
\newcommand{\BL}{\mathsf{BL}} % Bilinear map notation
\newcommand{\chart}{\mathsf{char}} % Characteristic notation
\renewcommand{\deg}{\mathsf{deg}} % Degree notation
\newcommand{\Sym}{\mathcal{S}^{\smallbullet}} % Symmetric algebra notation
\newcommand{\Ext}{{\bigwedge}^{\smallbullet}} % Exterior algebra notation
\newcommand{\Env}{\mathcal{U}} % Universal enveloping algebra notation
\newcommand{\ideal}{\mathcal{I}} % Ideal notation
\newcommand{\Z}{\mathbb{Z}} % Integer notation
\newcommand{\Alg}{\boldsymbol{\mathsf{Alg}}} % Algebra category notation
\newcommand{\CAlg}{\boldsymbol{\mathsf{CAlg}}} % Commutative algebra category notation
\newcommand{\SAlg}{\boldsymbol{\mathsf{SAlg}}} % Skew-commutative algebra category notation
\renewcommand{\dim}{\mathsf{dim\;}} % Dimension notation
\renewcommand{\det}{\mathsf{det\;}} % Determinant notation
\newcommand{\GL}{\mathsf{GL}} % General linear group notation
\newcommand{\Aut}{\mathsf{Aut}} % Automorphism group notation
\newcommand{\sgn}{\mathsf{Sgn}} % Sign function notation
\newcommand{\adj}{\mathsf{Adj\;}} % Adjoint matrix notation
\newcommand{\tr}{\mathsf{tr\;}} % Trace notation
\newcommand{\A}{\mathbb{A}} % Affine space notation
\newcommand{\proj}{\mathsf{proj}} % Projection map notation
\newcommand{\Orth}{\mathsf{O}} % Orthogonal group notation
\newcommand{\SO}{\mathsf{SO}} % Special orthogonal group notation
\newcommand{\Uni}{\mathsf{U}} % Unitary group notation
\newcommand{\SU}{\mathsf{SU}} % Special unitary group notation
\newcommand{\SL}{\mathsf{SL}} % Special linear group notation
\newcommand{\J}{\mathcal{J}} % Complex structure notation
\newcommand{\Symp}{\mathsf{Sp}} % Symplectic group notation
\newcommand{\Cl}{\mathsf{Cl}} % Clifford algebra notation


\DeclareMathOperator{\Span}{\mathsf{Span}}

% For arrows pointing
\tikzset{%
    myarrow/.style = {-Stealth, shorten >=5pt}
}
\newcommand{\mypoint}[2]{\tikz[remember picture]{\node[inner sep=0, anchor=base](#1){$#2$};}}

% For arrows inside paths
\tikzset{
    set arrow inside/.code={\pgfqkeys{/tikz/arrow inside}{#1}},
    set arrow inside={end/.initial=>, opt/.initial=},
    /pgf/decoration/Mark/.style={
        mark/.expanded=at position #1 with
        {
            \noexpand\arrow[\pgfkeysvalueof{/tikz/arrow inside/opt}]{\pgfkeysvalueof{/tikz/arrow inside/end}}
        }
    },
    arrow inside/.style 2 args={
        set arrow inside={#1},
        postaction={
            decorate,decoration={
                markings,Mark/.list={#2}
            }
        }
    },
}

\usepackage{mathtools} % for \xhookrightarrow

% For fake coproduct symbol
\DeclareRobustCommand{\coprod}{\mathop{\text{\fakecoprod}}}
\newcommand{\fakecoprod}{%
    \sbox0{$\prod$}%
    \smash{\raisebox{\dimexpr.9625\depth-\dp0}{\scalebox{1}[-1]{$\prod$}}}%
    \vphantom{$\prod$}%
}

% For vertical equivalence symbol
\newcommand{\vequiv}{\rotatebox{90}{$\equiv$}}
\newcommand{\hdash}{\rotatebox{90}{$|$}}

% For quotes
\usepackage{epigraph}
\renewcommand{\epigraphsize}{\small}

\let\originalepigraph\epigraph 
\renewcommand\epigraph[2]{\originalepigraph{#1}{\textsc{#2}}}

% For tikzcd
\tikzcdset{
    /tikz/commutative diagrams/every diagram/.append style={
        row sep=huge,
        column sep=huge
    },
    row sep/subtext/.initial = 1pt,
}

% For tabularx centering
\newcolumntype{C}{>{\centering\arraybackslash}X}

\usepackage{musicography} % For music natural symbol

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n} % Reset mathcal font to the default one

%----------------------------------------------------------------------------------------

\begin{document}

\chapterimage{UST.jpg} % Chapter heading image
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\titlepage % Output the title page
	% {\includegraphics[width=\paperwidth]{background.pdf}} % Code to output the background image, which should be the same dimensions as the paper to fill the page entirely; leave empty for no background image
	{ % Title(s) and author(s)
		\centering\sffamily % Font styling
		\vspace{3cm}
		{\huge\color{ocre} Honors in Linear and Abstract Algebra I\par} % Book title
		\vspace{2cm} % Vertical whitespace
		{Lecture Notes for\\MATH 2131\par} % Subtitle
        \vspace{2cm}
        {Department of Mathematics\\Hong Kong University of Science and Technology\par}
        \vspace{5cm}
        {\today \par}
		\vfill
		% \vspace{24pt} % Vertical whitespace
		% {\huge\bfseries \par} % Author name
	}

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\thispagestyle{fancy} % Suppress headers and footers on this page

% ~\vfill % Push the text down to the bottom of the page

\noindent {\Large \bf \sffamily Prefaces}

\vspace{8pt}

\noindent This lecture notes was written by a student in the course MATH 2131 -- Honors in Linear and Abstract Algebra by Professor Meng Guowu in HKUST in 2025-26 Fall. 

\noindent All diagrams in this lecture notes are written in LaTeX TikZ code.

\noindent The notes is with reference to textbooks \textit{`Linear Algebra'} by Friedberg, Insel and Spence, \textit{`Abstract Algebra'} by Artin and \textit{`A First Course in Abstract Algebra'} by Fraleigh. 

\noindent Also, this notes is with reference to the other the notes of two other professors teaching the same course before, Professor Ivan Ip and Professor Min Yan.


%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\pagestyle{empty} % Disable headers and footers for the following pages

\tableofcontents % Output the table of contents

\cleardoublepage % Start the following content on a new page

\pagestyle{fancy} % Enable default headers and footers again

\newpage

\noindent {\Large \bf \sffamily List of Symbols}

\vspace{8pt}

\noindent \begin{tabularx}{\textwidth}{cX}
    \toprule
    \textbf{Symbols} & \textbf{Meaning} \\
    \midrule
    $\F$ & a field \\
    $U, V, W$ & vector spaces \\
    $\alpha, \beta$ & elements in $\F$ \\
    $\F^n$ & the set of all column matrices with $n$ entries in $\F$ \\
    $(\F^n)^*$ & the set of all row matrices with $n$ entries in $\F$ \\
    $\F[X]$ & the polynomial ring \\
    $\F[[X]]$ & the formal power series ring \\
    \midrule
    $\C, \D$ & categories \\
    $\Set$ & the category of sets \\
    $\Vect_{\F}$ & the category of vector spaces over a field $\F$ \\
    \midrule
    $0_V$ & additive identity of vector space $V$ \\
    $1_V$ & multiplicative identity of vector space $V$ \\
    \midrule
    $\subset$ & proper subset \\
    $\subseteq$ & subset, i.e. can be equal \\
    \midrule
    $\iota$ & Inclusion map \\
    $\xhookrightarrow{}$ & Injective arrow \\
    $\pi$ & Projection map \\
    $\twoheadrightarrow$ & Surjective arrow \\
    \midrule
    $S, T$ & Linear maps \\
    $A, B$ & Matrices \\
    \midrule
    $\Mor_{\C}(V, W)$ & the set of all morphisms from $V$ to $W$ in category $\C$ \\
    $\Hom(V, W)$ & Hom-set of $V$ to $W$ \\
    $\End{A}$ & Endomorphism ring of $A$ \\
    $\M{m \times n}{\F}$ & the set of all $m \times n$ matrices over $\F$ \\
    \midrule
    $\vec{x}$ & column vector with entries $x_i$ \\
    $\hat{x}$ & row vector with entries $x_i$ \\
    $\vec{e_i}$ & column vector with only 1 at the $i$-th row and 0 at other places \\
    $\hat{e_i}$ & row vector with only 1 at the $i$-th column and 0 at other places \\
    \midrule
    $\alpha \cdot$ & a map that performs scalar multiplication \\
    $A \cdot$ & a map that performs matrix multiplication \\
    \midrule
    $\delta_x$ & the Kronecker delta function \\
    $\delta_X$ & the set of all Kronecker delta functions \\
    $\delta_{ij}$ & the Kronecker delta symbol \\
    \midrule
    $\ker{T}$ & Kernel of linear map $T$ \\
    $\im{T}$ & Image of linear map $T$ \\
    $\coker{T}$ & Cokernel of linear map $T$ \\
    $\coim{T}$ & Coimage of linear map $T$ \\
    $\span{S}$ & Span of a set of vectors $S$ \\
    \midrule
    $\prod$ & Product \\
    $\coprod$ & Coproduct \\
    $\oplus$ & Direct sum \\
    $\otimes$ & Tensor product \\
    \bottomrule
\end{tabularx}

\noindent \begin{tabularx}{\textwidth}{cX}
    \toprule
    \textbf{Symbols} & \textbf{Meaning} \\
    \midrule
    $\T$ & Tensor algebra \\
    \midrule
    $V^*$ & Dual space of $V$ \\
    $V^{**}$ & Double dual space of $V$ \\
    $D(V)$ & Double \\
    \midrule
    $\id_{\C}$ & Identity functor in category $\C$ \\
    $\F[-]$ & Free vector space functor \\
    $|-|$ & Forgetful functor \\
    $(-)^*$ & Dual space functor \\
    \bottomrule
\end{tabularx}


%----------------------------------------------------------------------------------------
%	SECTIONING EXAMPLES CHAPTER
%----------------------------------------------------------------------------------------

\chapterspaceabove{8cm} % Whitespace from the top of the page to the chapter title on chapter pages
\chapterspacebelow{8cm} % Amount of vertical whitespace from the top margin to the start of the text on chapter pages

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------
\chapter{Abstract Linear Spaces}

\epigraph{``I assume you have learnt linear algebra.''}{Guowu Meng}

\section{Binary Operation}\index{Binary Operation}

We start with the definition of a binary operation.

\begin{definition}[Binary Operation]
    A \emph{binary operation} on a set $S$ is a mapping of the elements of the Cartesian product $S \times S$ to $S$.
    \[ \begin{split}
            \cdot : S \times S & \to S \\ (x,y) &\mapsto x \cdot y
        \end{split}\]
\end{definition}

For easier understanding, binary operation is combining two objects into one. Hence, there is something called unary and ternary operations, corresponding to the action of combining one and three objects into one respectively.

\begin{example}
    A common example of a binary operation is addition on the set of natural numbers $\mathbb{N}$.
    \begin{equation}
        \begin{split}
            + : \mathbb{N} \times \mathbb{N} & \to \mathbb{N} \\ (x,y) &\mapsto x+y
        \end{split}
    \end{equation}
\end{example}

\begin{definition}[Associative]
    A binary operation $\cdot: S \times S \to S$ is said to be \emph{associative} if, for all $x,y,z \in S$, 
    \[ x \cdot (y \cdot z) = (x \cdot y) \cdot z \]
\end{definition}

\begin{example}
    A common example of an associative (binary) operation is addition on the set of natural numbers $\mathbb{N}$. For all $x,y,z \in \mathbb{N}$, we have $x + (y + z) = (x + y) + z$.
\end{example}

\begin{definition}[Identifiable]
    A binary operation $\cdot: S \times S \to S$ is said to be \emph{identifiable}, or \emph{unital}, if there exists an element $e \in S$, the \emph{identity} or \emph{unit element}, such that, for all $x \in S$
    \[ e \cdot x = x = x \cdot e \]
\end{definition}

\begin{example}
    A common example of an identifiable (binary) operation is multiplication on the set of natural numbers $\mathbb{N}$. The identity element is $1$, and for all $x \in \mathbb{N}$, we have $x \cdot 1 = x = 1 \cdot x$.
\end{example}

\begin{proposition}
    The identity element of an identifiable operation is unique.
\end{proposition}

\begin{proof}
    Let $e_1$ and $e_2$ be two identity elements for the operation $\cdot$. Then, for any element $x \in S$, we have:
    \[ e_1 \cdot x = x = x \cdot e_1 \]
    \[ e_2 \cdot x = x = x \cdot e_2 \]
    Now, consider the element $e_1$: $e_1 \cdot e_2 = e_1$.
    But since $e_2$ is an identity element, we also have: $e_1 \cdot e_2 = e_2$.
    Therefore, we conclude that $e_1 = e_2$, proving the uniqueness of the identity element.
\end{proof}

Note that the two-sided identity must be unique, but one-sided identities need not be. The following is an example of it.

\begin{example}
    Consider a set $X = \left\{ \begin{bmatrix}
        1 & a \\
        0 & 0 
    \end{bmatrix} \; \middle| \; a \in \R \right\}$ with the binary operation defined as matrix multiplication. This set has many left identity elements, but no two-sided identity element.
\end{example}

\begin{definition}[Invertible]
    A binary operation $\cdot: S \times S \to S$ is said to be \emph{invertible} if, for every element $x \in S$, there exists an element $y \in S$, called the two-sided \emph{inverse} of $x$, denoted as $x^{-1}$, such that
    \[ x \cdot y = e = y \cdot x \]
    where $e$ is the identity element of the operation.
\end{definition}
\begin{remark}
    An invertible operation must be identifiable, since the identity element is required in the definition of invertibility.
\end{remark}

\begin{example}
    A common example of an invertible (binary) operation is addition on the set of integers $\Z$. For every integer $x \in \Z$, there exists an integer $y = -x$ such that:
    \begin{equation}
        x + (-x) = 0 = (-x) + x
    \end{equation}
    where $0$ is the identity element for addition.
\end{example}

\begin{proposition}
    The inverse element of an invertible operation is unique.
\end{proposition}

\begin{proof}
    Let $y_1$ and $y_2$ be two inverses of an element $x \in S$. Then, by definition of inverse, we have:
    \[ x \cdot y_1 = e = y_1 \cdot x \]
    \[ x \cdot y_2 = e = y_2 \cdot x \]
    Now, consider the element $y_1$: $y_1 \cdot x = e$.
    But since $y_2$ is also an inverse of $x$, we can substitute $e$ with $x \cdot y_2$: $y_1 \cdot x = x \cdot y_2 = e$.
    By the associativity of the operation, we can rearrange this to:
    \[ y_1 = y_1 \cdot e = y_1 \cdot (x \cdot y_2) = (y_1 \cdot x) \cdot y_2 = e \cdot y_2 = y_2 \]
    Thus, the inverse element is unique.
\end{proof}

Same for the inverse, one-sided need not be unique. The example is left as an exercise.

\begin{definition}[Commutative]
    A binary operation $\cdot: S \times S \to S$ is said to be \emph{commutative} if, for all $x,y \in S$, the following holds:
    \[ x \cdot y = y \cdot x \]
\end{definition}

\begin{example}
    A common example of a commutative operation is addition on the set of integers $\Z$. For all $x,y \in \Z$, we have: $x + y = y + x$
\end{example}

\begin{definition}[Distributive (Harmonic)]
    A binary operation $\cdot: S \times S \to S$ is said to be \emph{distributive} with respect to another binary operation $+: S \times S \to S$ if, for all $x,y,z \in S$, the following holds:
    \[ \begin{split}
        x \cdot (y + z) &= x \cdot y + x \cdot z \\
        (y + z) \cdot x &= y \cdot x + z \cdot x
    \end{split} \]
\end{definition}

The professor preferred to use the word ``harmonic'' instead of ``distributive''. Note that it is important to show that ``\emph{which binary operation} is distributive to \emph{which binary operation}''. (The two binary operation in this sentence is not commutative.)

\begin{example}
    A common example of a distributive operation is multiplication over addition on the set of integers $\Z$. For all $x,y,z \in \Z$, we have:
    \[ \begin{split}
        x \cdot (y + z) &= x \cdot y + x \cdot z \\
        (y + z) \cdot x &= y \cdot x + z \cdot x
    \end{split} \]
\end{example}

\newpage

\section{Groups, Rings, Fields}\index{Groups, Rings, Fields}

With those five properties, we can construct monoid and groups.

\begin{definition}[Monoid]
    A \emph{monoid} is a set $M$ equipped with a binary operation $f: M \times M \to M$ having the following properties:
    \begin{enumerate}
        \item \emph{Associative}
        \item \emph{Identifiable}
    \end{enumerate}
    We say $(M, f)$ is a monoid, and $f$ is the \emph{monoid operation} on the set $M$. A set $M$ with a monoid operation $f$ is the \emph{monoid structure}.
\end{definition}

\begin{definition}[Group]
    A \emph{group} is a set $G$ equipped with a monoid operation $f: G \times G \to G$ with the additional property that every element has an inverse.
\end{definition}

\begin{example}
    $(\R\setminus\{0\}, \times)$ is a group, but $(\R, \times)$ is not a group since $0$ does not have a multiplicative inverse.
\end{example}

\begin{definition}[Abelian Monoid / Group]
    A monoid / group $(S, f)$ is said to be an \emph{abelian} if the operation $f$ is commutative.
\end{definition}

\begin{definition}[Unital Ring]
    A \emph{unital ring} is a set $R$ equipped with two binary operations $f: R \times R \to R$ (addition) and $g: R \times R \to R$ (multiplication) such that the following properties hold:
    \begin{enumerate}
        \item \emph{Additive Group:} $(R, f)$ is an abelian group.
        \item \emph{Multiplicative Monoid:} $(R, g)$ is a monoid.
        \item \emph{Distributive Property:} $g$ with respect to $f$.
    \end{enumerate}
\end{definition}

\begin{definition}[Commutative Ring]
    A \emph{commutative ring} is a unital ring $R$ such that the multiplication operation $g: R \times R \to R$ is commutative.
\end{definition}

\begin{example}
    $(\Z, +, \times)$ is a commutative ring.
\end{example}

\begin{definition}[Field]
    A \emph{field} is a commutative ring $\F$ such that every non-zero element has a multiplicative inverse.
\end{definition}

\begin{example}
    $(\mathbb{Q}, +, \times)$, $(\R, +, \times)$ and $(\mathbb{C}, +, \times)$ are fields.
\end{example}

\begin{example}[Finite Field]
    $(\Z/2\Z, +, \times)$ is a field, where $\Z/2\Z = \{[0], [1]\}$, $[0]$ is the set of even integers and $[1]$ is the set of odd integers. Note that any $\Z/p\Z$ is a finite field, where $p$ is a prime number.
\end{example}

We may draw a diagram for the relationship between the algebraic structures.

\begin{center}
    \begin{tikzcd}[column sep=3.5em, row sep=huge, math mode=false]
        Set \arrow[r, "\scriptsize Closed Operation"] &
        Magma \arrow[r, "\scriptsize Associativity"] &
        Semigroup \arrow[d, "\scriptsize Identity"] \\

        Abelian Group &
        Group \arrow[swap, l, "\scriptsize Commutative"] &
        Monoid \arrow[swap, l, "\scriptsize Inverse"] \arrow[r, "\scriptsize Commutative"] \arrow[dl, "$\times$", ocre] &
        Abelian Monoid \arrow[dl, red, "$\times$", swap] \\

        Rng \arrow[from=ur, "$+$" near end, swap] \arrow[from=urr] 
        & {\color{ocre} Unital Ring} \arrow[ocre, from=ul, crossing over, "$+$" near start, swap] \arrow[r] 
        & {\color{red} Commutative Ring} \arrow[from=ull, crossing over, red, "$+$", swap] \arrow[r, "\scriptsize Multiplicative", "\scriptsize Inverse" swap] 
        & Field
    \end{tikzcd}
\end{center}

\newpage

\section{Morphisms}\index{Morphisms}

Normally, when we have two sets we can have a set map. Then if the two are in the same algebraic structures? They are called the homomorphisms.

\begin{definition}[Monoid Homomorphism]
    A \emph{monoid homomorphism} is a morphism between two monoids that preserves the monoid structure. Formally, let $(M_1, \cdot_1)$ and $(M_2, \cdot_2)$ be two monoids with identity elements $e_1$ and $e_2$, respectively. A function $f: M_1 \to M_2$ is a monoid homomorphism if:
    \begin{enumerate}
        \item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in M_1$
        \item $f(e_1) = e_2$
    \end{enumerate}
\end{definition}

\begin{definition}[Group Homomorphism]
    A \emph{group homomorphism} is a morphism between two groups that preserves the group structure. Formally, let $(G_1, \cdot_1)$ and $(G_2, \cdot_2)$ be two groups with identity elements $e_1$ and $e_2$, respectively. A function $f: G_1 \to G_2$ is a group homomorphism if:
    \begin{enumerate}
        \item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in G_1$
        \item $f(e_1) = e_2$
        \item $f(x^{-1}) = (f(x))^{-1} \quad \forall x \in G_1$
    \end{enumerate}
\end{definition}

\begin{proposition}
    The second and third properties of a group homomorphism are consequences of the first property.
\end{proposition}

\begin{proof}
    Let $f: G_1 \to G_2$ be a group homomorphism satisfying the first property.

    \textbf{Second Property:} For any element $x \in G_1$, we have:
    \[
        f(x) = f(x \cdot_1 e_1) = f(x) \cdot_2 f(e_1)
    \]
    So for any $f(x) \in G_2$, this implies that $f(e_1)$ must be the identity element in $G_2$, i.e., $f(e_1) = e_2$.

    \textbf{Third Property:} We have:
    \[
        e_2 = f(e_1) = f(x \cdot_1 x^{-1}) = f(x) \cdot_2 f(x^{-1})
    \]
    This shows that $f(x^{-1})$ is the inverse of $f(x)$ in $G_2$, i.e., $f(x^{-1}) = (f(x))^{-1}$.
\end{proof}

For monoid homomorphisms, the second property cannot be derived from the first property. Consider the identity element $e_1$ in $M_1$. If we apply the first property, we get $f(e_1 \cdot_1 e_1) = f(e_1) \cdot_2 f(e_1)$. This simplifies to $f(e_1) = f(e_1) \cdot_2 f(e_1)$, which does not necessarily imply that $f(e_1)$ is the identity element in $M_2$, i.e., $f(e_1) \neq e_2$, but $f(e_1)$ is the idempotent element in $M_2$. Therefore, the second property must be explicitly stated for monoid homomorphisms.

However in the case of group homomorphisms, the existence of inverses ensures that there is only one element that can be idempotent under the group operation, which is the identity element. Thus, for group homomorphisms, the second property can be derived from the first property.

\begin{definition}[Idempotent Elements]
    An element $a$ is said to be \emph{idempotent} if $a = a^2$.
\end{definition}

\newpage

To introduce the vector space, the following two morphisms are required.

\begin{definition}[Ring Homomorphism]
    A \emph{ring homomorphism} is a morphism between two rings that preserves both the additive and multiplicative structures. Formally, let $(R_1, +_1, \cdot_1)$ and $(R_2, +_2, \cdot_2)$ be two rings with identity elements $0_1$, $1_1$ and $0_2$, $1_2$, respectively. A function $f: R_1 \to R_2$ is a ring homomorphism if:
    \begin{enumerate}
        \item $f(x +_1 y) = f(x) +_2 f(y) \quad \forall x, y \in R_1$
        \item $f(x \cdot_1 y) = f(x) \cdot_2 f(y) \quad \forall x, y \in R_1$
        \item $f(1_1) = 1_2$
    \end{enumerate}    
\end{definition}

\begin{definition}[Endomorphism]
    An \emph{endomorphism} is a morphism from an algebraic structure to itself. Formally, let $(A, \cdot)$ be an algebraic structure. An endomorphism $f: A \to A$ is a set map such that:
    \[
        f(x \cdot y) = f(x) \cdot f(y) \quad \forall x, y \in A
    \]
\end{definition}

The following two sets are the sets of all structure-preserving maps.

\begin{definition}[Hom-set]
    The set of all morphisms from an algebraic structure $A$ to another algebraic structure $B$ is called the \emph{hom-set}, denoted by $\Hom(A, B)$. 
\end{definition}

\begin{definition}[Endomorphism Ring]
    The set of all endomorphisms of an abelian group $(A, +)$, denoted by $\End{A}$, forms a (non-commutative) ring under pointwise addition and composition of set maps. The addition and multiplication operations are defined as follows:
    \[
        \begin{split}
            + : \End{A} \times \End{A} &\to \End{A} \\
            (f,g) &\mapsto (f+g: x \mapsto f(x) + g(x)) \qquad &f + g : A \to A \\ \\
            \circ : \End{A} \times \End{A} &\to \End{A} \\
            (f,g) &\mapsto (f \circ g: x \mapsto f(g(x))) \qquad &f \circ g : A \to A
        \end{split}
    \]
    The identity element for addition is the zero endomorphism, which maps every element to the identity element of the group. 
    \[ \begin{split}
        0: A &\to A \\
        x &\mapsto 0
    \end{split}
    \]
    The identity element for multiplication is the identity endomorphism, which maps every element to itself. 
    \[
    \begin{split}
        1: A &\to A \\
        x &\mapsto x
    \end{split}
    \]
    Note that all endomorphisms in $\End{A}$ are group homomorphisms and $\End{A} = \Hom(A, A)$.
\end{definition}

\newpage

\section{Linear Spaces}\index{Linear Spaces}

Then we can define what a linear structure is.

\begin{definition}[Linear Structure]
    A \emph{linear structure} over a field $\F$ on a set $V$ is a pair $(+, \cdot)$ where $(V, +)$ is an abelian group with a ring homomorphism $\F \to \End{V}$, where $\End{V}$ is the endomorphism ring of the abelian group $(V, +)$.
    \[ \begin{split}
            \cdot : \F &\to \End{V} \\
            \alpha &\mapsto (\alpha\cdot : \vec{x} \mapsto \alpha \vec{x}) \qquad \alpha \cdot : V \to V
        \end{split}
    \]
    The ring homomorphism is a (ring) action of the field $\F$ on the abelian group $(V, +)$, called \emph{scalar multiplication}. The ring action can be written as a binary operation:
    \[
        \begin{split}
            \cdot : \F \times V &\to V \\
            (\alpha, \vec{x}) &\mapsto \alpha \vec{x}
        \end{split}
    \]
\end{definition}

A linear space / vector space is a set with a linear structure over a field on the set. In normal textbook, a linear space will be defined as follows:

\begin{corollary}[Linear Spaces]
    A linear space over a field $\F$ is a set $V$ equipped with two operations: vector addition $+: V \times V \to V$ and scalar multiplication $\cdot : \F \times V \to V$, satisfying the following axioms for all $\vec{u}, \vec{v}, \vec{w} \in V$ and $\alpha, \beta \in \F$:
    \begin{center}
        \begin{tabularx}{\textwidth}{XX}
            \toprule
            \textbf{Axiom} & \textbf{Statement} \\
            \midrule
            1. Associativity of addition & $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$ \\
            2. Existence of additive identity & $\exists \vec{0} \in V$ such that $\forall \vec{u} \in V$, $\vec{u} + \vec{0} = \vec{u}$ \\
            3. Existence of additive inverses & $\forall \vec{u} \in V$, $\exists -\vec{u} \in V$ such that $\vec{u} + (-\vec{u}) = \vec{0}$ \\
            4. Commutativity of addition & $\vec{u} + \vec{v} = \vec{v} + \vec{u}$ \\
            5. Distributivity of scalar multiplication with respect to vector addition & $\alpha (\vec{u} + \vec{v}) = \alpha \vec{u} + \alpha \vec{v}$ \\
            6. Distributivity of scalar multiplication with respect to field addition & $(\alpha + \beta) \cdot = \alpha \cdot + \beta \cdot$ \\
            7. Compatibility of scalar multiplication with field multiplication & $(\alpha \beta) \cdot = (\alpha \cdot) \circ (\beta \cdot)$ \\
            8. Identity element of scalar multiplication & $\F \ni 1 \mapsto (1\cdot : x \mapsto x) \in \End{V}$ \\
            \bottomrule
        \end{tabularx}
    \end{center}
\end{corollary}
\begin{remark}
    The first four axioms ensure that $(V, +)$ is an abelian group, while the fifth axiom describes the distributivity inside $\End{A}$ and the last three axioms describe the ring homomorphism.
\end{remark}

\begin{example}
    $\F$ is a linear space over itself with the usual addition and multiplication operations.
    \[
        \begin{split}
            \cdot : \F \times \F &\to \F \\
            (\alpha,\beta) &\mapsto \alpha \beta
        \end{split}
    \]
    The first $\F$ is the field acting on the second $\F$, which is the abelian group.
\end{example}

\begin{example}
    Let $X$ be a set and $\F$ be a field. ($f$ is a set map)
    \[
    \begin{split}
        \F[[X]] = \Map(X, \F) &\overset{\mathrm{def}}{=\joinrel=} \text{the set of all $\F$-valued functions on } X \\
        &=\joinrel= \left\{ f : X \to \F \right\}
    \end{split}
    \]
    $\F[[X]]$ is a linear space over $\F$ with the following operations defined pointwisely:
    \[
        \begin{split}
            + : \F[[X]] \times \F[[X]] &\to \F[[X]] \\
            (f,g) &\mapsto (f+g: x \mapsto f(x) + g(x)) \qquad f + g : X \to \F \\ \\
            \cdot : \F \times \F[[X]] &\to \F[[X]] \\
            (\alpha,f) &\mapsto (\alpha f: x \mapsto \alpha f(x)) \qquad \alpha f : X \to \F
        \end{split}
    \]
\end{example}

\begin{example}
    Let $X$ be a set and $\F$ be a field.
    \[
        \begin{split}
            \F[X] = \text{Map}_{\text{fin}}(X, \F) &\overset{\mathrm{def}}{=\joinrel=} \text{the set of all finitely supported $\F$-valued functions on } X \\
            &=\joinrel= \left\{ f : X \to \F \mid f \text{ is finitely supported} \right\}
        \end{split}
    \]
    $\F[X]$ is a linear space over $\F$ as $\F[X] \subseteq \F[[X]]$ and the operations are defined pointwisely as in the previous example.

    $f: X \to \F$ is finitely supported if the set $\{ x \in X \mid f(x) \neq 0 \}$ is finite or $f(x) \neq 0$ for only finitely many $x \in X$.
\end{example}

\begin{example}
    Let $t$ be a formal variable. Then $\F[[t]] \overset{\mathrm{def}}{=\joinrel=} \F[[\{ 1, t, t^2, \cdots \}]] = \sum_{n = 0}^{\infty} a_n t^n$ is the set of all formal power series in $t$ with coefficients in $\F$ and $\F[t] \overset{\mathrm{def}}{=\joinrel=} \F[\{ 1, t, t^2, \cdots \}] = \sum_{n = 0}^{N} a_n t^n$ is the set of all polynomials in $t$ with coefficients in $\F$. Both $\F[[t]]$ and $\F[t]$ are linear spaces over $\F$.
\end{example}

There are another names for $\F[X]$ and $\F[[X]]$: Polynomial ring and Formal Power Series ring, respectively.

\begin{example}
    Let $n$ be a positive integer and $\F$ be a field. Then 
    \[
        \F^n \overset{\mathrm{def}}{=\joinrel=} \left\{ 
        \begin{bmatrix}
        c_1 \\
        \vdots \\
        c_n
        \end{bmatrix}
        \;\middle|\; c_i \in \F
        \right\}
    \]
    is the set of all \emph{column matrices} with $n$ entries in $\F$. Elements in $\F^n$ are written as $\vec{x}$ and are called \emph{column vectors}. $\F^n$ is a linear space over $\F$ with the following operations defined entrywisely:
    \[
        \begin{split}
            + : \F^n \times \F^n &\to \F^n \\
            (\vec{a}, \vec{b}) &\mapsto \vec{a} + \vec{b} = \begin{bmatrix}
            a_1 + b_1 \\
            \vdots \\
            a_n + b_n
            \end{bmatrix} \\ \\
            \cdot : \F \times \F^n &\to \F^n \\
            (\alpha, \vec{a}) &\mapsto \alpha \vec{a} = \begin{bmatrix}
            \alpha a_1 \\
            \vdots \\
            \alpha a_n
            \end{bmatrix}
        \end{split}
    \]
    $\F^n$ is a linear space over $\F$ automatically as $\F$ is a linear space over itself.
\end{example}

%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapter{Linear Maps and Matrices}

\epigraph{``Linear algebra is the easiest in Mathematics''}{Guowu Meng}

\section{Linear Maps}\index{Linear Maps}

Linear map, sometimes linear transformation, is a homomorphism preserving linear structure.

\begin{definition}[Linear Maps]
    Let $V$ and $W$ be two linear spaces over a field $\F$. A \emph{linear map} is a set map $T: V \to W$ such that for all $u, v \in V$ and $\alpha \in \F$, the following holds:
    \[
        \begin{split}
            T(u + v) &= T(u) + T(v) \\
            T(\alpha u) &= \alpha T(u)
        \end{split}
    \]
    The set of all linear maps from $V$ to $W$ is denoted by $\Hom(V, W)$. Some may write {$\mathcal{L}(V, W)$}.
\end{definition}

\begin{definition}[Linear Combinations]
    Let $V$ be a linear space over a field $\F$. A \emph{linear combination} of vectors $v_1, v_2, \cdots, v_n \in V$ is a vector of the form:
    \[
        \alpha^1 v_1 + \alpha^2 v_2 + \cdots + \alpha^n v_n
    \]
    where $\alpha^1, \alpha^2, \cdots, \alpha^n \in \F$ are scalars.
\end{definition}

The reason of using the superscript for scalars is to avoid confusion with the subscript of vectors. Also, it is due to the concept of dual space, which will be introduced later.

We can combine the two properties of linear maps into one property.

\begin{corollary}[Linear Maps and Linear Combinations]
    A set map $f: V \to W$ between two linear spaces over a field $\F$ is a linear map if and only if $T$ respects linear combinations, i.e., for all $v_1, v_2 \in V$ and all scalars $\alpha^1, \alpha^2 \in \F$, the following holds:
    \[
        T(\alpha^1 v_1 + \alpha^2 v_2) = \alpha^1 T(v_1) + \alpha^2 T(v_2)
    \]
\end{corollary}

\begin{example}
    Let $A$ be an $m \times n$ matrix with entries in a field $\F$. The map $T: \F^n \to \F^m$ defined by
    \[
        Tx = T(x) = Ax
    \]
    where right-hand side is the usual matrix multiplication, is a linear map over $\F$.
\end{example}

\begin{proposition}
    A linear map $T: \F^n \to \F^m$ is a matrix multiplication by a unique $m \times n$ matrix $A$ with entries in $\F$. The matrix $A$ is called the \emph{standard matrix} of the linear map $T$.
    
    \begin{center}
        \begin{tikzcd}[column sep=huge, row sep=tiny]
            \Hom(\F^n, \F^m) \arrow[r, equal, "\text{natural}", "\text{identification}" swap] & \M{m \times n}{\F} \\
            T \arrow[r, mapsto] & A \\
            A\cdot & A \arrow[l, mapsto]
        \end{tikzcd}
    \end{center}
    where $A\cdot : \vec{x} \mapsto A\vec{x}$ and $A$ can be expressed as follows:
    \[
        A = \begin{bmatrix}
            | & | & & | \\
            T\vec{e}_1 & T\vec{e}_2 & \cdots & T\vec{e}_n \\
            | & | & & |
        \end{bmatrix}
    \]
    The vector $\vec{e}_i$ is the column vectors where only has the value 1 at the $i$-th place and 0 at other places.
\end{proposition}

\begin{proof}
    Consider a column matrix $x \in \F^n$ with entries $x^1, x^2, \cdots, x^n \in \F$. Then $x$ can be expressed as a linear combination of the vectors $\vec{e}_1, \vec{e}_2, \cdots, \vec{e}_n$:
    \[
        x = x^1 \vec{e}_1 + x^2 \vec{e}_2 + \cdots + x^n \vec{e}_n = \sum_{i=1}^{n} x^i \vec{e}_i
    \]
    Since $T$ is a linear map, it respects linear combinations. Therefore, we have:
    \[
        Tx = T\left( \sum_{i=1}^{n} x^i \vec{e}_i \right) = \sum_{i=1}^{n} x^i T(\vec{e}_i) = \sum_{i=1}^{n} x^i \vec{a}_i = A\vec{x}
    \]
    where $\vec{a}_i = T\vec{e}_i$ is the $i$-th column of the matrix $A = \begin{bmatrix}
        | & | & & | \\
        T\vec{e}_1 & T\vec{e}_2 & \cdots & T\vec{e}_n \\
        | & | & & |
    \end{bmatrix}$. Thus, we have $T\vec{x} = A\vec{x}$ for all $\vec{x} \in \F^n$. This shows that $T$ can be represented as a matrix multiplication by the matrix $A$.
\end{proof}

There is a simpler way to write $\sum_{i=1}^{n} x^i \vec{e}_i$: The Einstein Summation Convention. When an index variable appears twice in a single term and is not otherwise defined, it implies summation of that term over all the values of the index. Therefore, we can write:
\[
    x = x^i \vec{e}_i
\]
where $i$ is summed from $1$ to $n$. 

\begin{definition}[Linear Functional / Homogeneous Linear Function]
    A linear map $f: \F^n \to \F$ is called a \emph{homogeneous linear function} or a \emph{linear functional} if for all $\alpha \in \F$ and $x \in \F^n$, the following holds:
    \[
        f(\alpha x) = \alpha f(x)
    \]
\end{definition}

\begin{corollary}[Standard Matrix of a Linear Map]
    The standard matrix of a linear map $T: \F^n \to \F^m$ can be written as:
    \[
        A = \begin{bmatrix}
            \hdash & f_1 & \hdash \\
            \hdash & f_2 & \hdash \\
            & \vdots & \\
            \hdash & f_m & \hdash
        \end{bmatrix}
    \]
    where $f_i: \F^n \to \F$ is the $i$-th component function of $T$, which is a linear functional.
\end{corollary}

\begin{example}
    Let $D: \F[t] \to \F[t]$ be the differentiation operator defined by:
    \[
        D\left( \sum_{n=0}^{N} a_n t^n \right) = \sum_{n=1}^{N} n a_n t^{n-1}
    \]
    for all polynomials $\sum_{n=0}^{N} a_n t^n \in \F[t]$. The differentiation operator $D$ is a linear map over $\F$. The standard matrix of $D$ with respect to the standard basis $\{1, t, t^2, \cdots, t^N\}$ of $\F[t]$ is given by:
    \[
        A = \begin{bmatrix}
            0 & 1 & 0 & 0 & \cdots & 0 \\
            0 & 0 & 2 & 0 & \cdots & 0 \\
            0 & 0 & 0 & 3 & \cdots & 0 \\
            \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & 0 & \cdots & N \\
            0 & 0 & 0 & 0 & \cdots & 0
        \end{bmatrix}
    \]
\end{example}

\begin{proposition}
    Let $X$ be a set and $W$ be a linear space over a field $\F$. Then the set of all set maps from $X$ to $W$, denoted by $\Map(X, W)$, is a linear space over $\F$ with the following operations defined pointwisely:
    \[
        \begin{split}
            + : \Map(X, W) \times \Map(X, W) &\to \Map(X, W) \\
            (f,g) &\mapsto (f + g) : x \mapsto f(x) + g(x) \\ \\
            \cdot : \F \times \Map(X, W) &\to \Map(X, W) \\
            (\alpha,f) &\mapsto (\alpha f) : x \mapsto \alpha f(x)
        \end{split}
    \]
\end{proposition}

\begin{proof}
    The $\Map(X, W)$ is defined pointwisely by $\F$, hence it is trivial to be a linear map.
\end{proof}

\begin{proposition}
    Let $V$ and $W$ be two linear spaces over a field $\F$. Then $\Hom(V, W)$ is a linear space over $\F$ with the following operations defined pointwisely:
    \[
        \begin{split}
            + : \Hom(V, W) \times \Hom(V, W) &\to \Hom(V, W) \\
            (f,g) &\mapsto (f + g): v \mapsto f(v) + g(v) \\ \\
            \cdot : \F \times \Hom(V, W) &\to \Hom(V, W) \\
            (\alpha,f) &\mapsto (\alpha f): v \mapsto \alpha f(v)
        \end{split}
    \]
\end{proposition}

\begin{proof}
    Note that $\Hom(V, W) \subseteq \Map(V, W)$. We need to show that the operations defined above are closed in $\Hom(V, W)$, i.e., for all $f, g \in \Hom(V, W)$ and $\alpha \in \F$, $f + g \in \Hom(V, W)$ and $\alpha f \in \Hom(V, W)$ or equivalently, $f$ respects linear combinations.

    Let $\vec{u}, \vec{v} \in V$ and $\alpha, \beta \in \F$. Since $f, g \in \Hom(V, W)$, we have:
    \[
        \begin{split}
            (f + g)(\alpha\vec{u} + \beta\vec{v}) &\overset{\mathrm{def}}{=\joinrel=} f(\alpha\vec{u} + \beta\vec{v}) + g(\alpha\vec{u} + \beta\vec{v}) \\
            &=\joinrel= \alpha f(\vec{u}) + \beta f(\vec{v}) + \alpha g(\vec{u}) + \beta g(\vec{v}) \\
            &=\joinrel= \alpha (f(\vec{u}) + g(\vec{u})) + \beta (f(\vec{v}) + g(\vec{v})) \\
            &\overset{\mathrm{def}}{=\joinrel=} \alpha (f + g)(\vec{u}) + \beta (f + g)(\vec{v})
        \end{split}
    \]
    where the second equality is due to the linearity of $f$ and $g$. Thus, $f + g \in \Hom(V, W)$ and $\alpha f \in \Hom(V, W)$.
\end{proof}
\begin{remark}
    Note that $\End{V} = \Hom(V, V)$ is a linear space over $\F$ and also a ring with the addition and multiplication operations defined in the previous section. The addition operation is commutative, but the multiplication operation is not necessarily commutative.
\end{remark}

Then we can say that 
\begin{center}
    $\Map(\F^n, \F^m) \supseteq \Hom(\F^n, \F^m) \cong \M{m \times n}{\F}$
\end{center}


\newpage

\section{Injections, Surjections and Isomorphisms}\index{Injections, Surjections and Isomorphisms}

Similar to normal maps, there are injective, surjective and bijective linear maps.

\begin{definition}[Injective Lienar Maps]
    A linear map $f: V \to W$ between two linear spaces over a field $\F$ is said to be \emph{injective} (or one-to-one) if for all $u, v \in V$, the following holds:
    \[
        f(u) = f(v) \implies u = v
    \]
    Equivalently, $f$ is injective if the only vector in $V$ that maps to the zero vector in $W$ is the zero vector itself:
    \[
        f(u) = 0 \implies u = 0
    \]
\end{definition}

\begin{definition}[Surjective Lienar Maps]
    A linear map $f: V \to W$ is said to be \emph{surjective} (or onto) if for every $w \in W$, there exists at least one $v \in V$ such that:
    \[
        w = f(v)
    \]
\end{definition}

\begin{definition}[Invertible Linear Maps / Linear Equivalences]
    A linear map $T: V \to W$ is said to be \emph{invertible} if $T$ has a unique two-sided inverse $S$, denoted by $T^{-1}$, i.e., there exists a linear map $S: W \to V$ such that:
    \[
        TS = 1_W \quad \text{and} \quad ST = 1_V
    \]
    where $1_V: V \to V$ and $1_W: W \to W$ are the identity maps on $V$ and $W$, respectively.
    In this case, we say that the linear spaces $V$ and $W$ are \emph{isomorphic} or \emph{linear equivalent}, denoted by $V \cong W$.
\end{definition}

\begin{corollary}[Invertible Linear Maps]
    A linear map $T: V \to W$ is invertible if and only if $T$ is both injective and surjective, i.e., bijective / one-to-one correspondence.
\end{corollary}

\begin{proof}
    ($\Rightarrow$) Assume $T: V \to W$ is invertible. By definition, there exists a linear map $S: W \to V$ such that $TS = 1_W$ and $ST = 1_V$.

    To show that $T$ is injective, suppose $T(u) = T(v)$ for some $u, v \in V$. We have:
    \[
        S(T(u)) = S(T(v)) \implies (ST)(u) = (ST)(v) \implies 1_V(u) = 1_V(v) \implies u = v
    \]
    Thus, $T$ is injective. Then, to show that $T$ is surjective, let $w \in W$. Since $TS = 1_W$, we have:
    \[
        T(S(w)) = 1_W(w) = w
    \]
    Then for every $w \in W$, there exists a $v = S(w) \in V$ such that $T(v) = w$. Thus, $T$ is surjective.

    ($\Leftarrow$) Now assume that $T: V \to W$ is both injective and surjective. We need to show that there exists a linear map $S: W \to V$ such that $TS = 1_W$ and $ST = 1_V$.

    Since $T$ is surjective, for each $w \in W$, there exists at least one $v \in V$ such that $T(v) = w$. Define the map $S: W \to V$ by choosing one such preimage for each $w$:
    \[
        S(w) = \text{a chosen } v \text{ such that } T(v) = w
    \]
    To show that $S$ is well-defined, we need to ensure that if $T(v_1) = T(v_2)$, then $v_1 = v_2$. This follows from the injectivity of $T$.

    Now we verify that $TS = 1_W$: $(TS)(w) = T(S(w)) = w$ for all $w \in W$. Thus, $TS = 1_W$. Next, we verify that $ST = 1_V$: $(ST)(v) = S(T(v)) = v$ for all $v \in V$. Thus, $ST = 1_V$.

    Therefore, $T$ has a two-sided inverse $S$, and hence $T$ is invertible.
\end{proof}

\begin{definition}[Characteristic of a Field]
    The \emph{characteristic} of a field $\F$ is the smallest positive integer $n$ such that:
    \[
        \underbrace{1 + 1 + \cdots + 1}_{n \text{ times}} = 0
    \]
    If no such positive integer exists, the characteristic of $\F$ is defined to be $0$.    
\end{definition}

\begin{example}
    The differentiation operator $D: \F[t] \to \F[t]$ is not an injective linear map as $D(1) = 0 = D(2)$ but is a surjective linear map if $\F$ is a field of characteristic $0$.
\end{example}


\newpage

\section{Matrix Multiplications and Compositions of Linear Maps}\index{Matrix Multiplications and Compositions of Linear Maps}

We consider two linear maps $T: \F^n \to \F^m$ and $S: \F^m \to \F^k$ with standard matrices $A$ and $B$, respectively. We want to find the standard matrix of the composition $ST: \F^n \to \F^k$.

\begin{center}
    \begin{tikzpicture}
        \matrix (m) [matrix of math nodes, row sep=3em, column sep=4em, minimum width=2em]
        {\F^n & \F^m & \F^k \\};
        \path[->]
        (m-1-1) edge node [above] {$T$} node [below] {$A$} (m-1-2)
                edge [bend left] node [above] {$ST$} (m-1-3)
                edge [bend right] node [below] {$BA$} (m-1-3)
        (m-1-2) edge node [above] {$S$} node [below] {$B$} (m-1-3);
    \end{tikzpicture}
\end{center}

\begin{proposition}
    The standard matrix of the composition $ST: \F^n \to \F^k$ is the matrix multiplication $BA$, i.e., for all $x \in \F^n$,
    \[
        (ST)x = B(Ax) = (BA)x
    \]
\end{proposition}

\begin{proof}
    Let $x \in \F^n$ be a column matrix with entries $x^1, x^2, \cdots, x^n \in \F$. Then $x$ can be expressed as a linear combination of the standard basis vectors $\vec{e}_1, \vec{e}_2, \cdots, \vec{e}_n$:
    \[
        x = x^1 \vec{e}_1 + x^2 \vec{e}_2 + \cdots + x^n \vec{e}_n = x^i \vec{e}_i
    \]
    Consider the $j$-th column of $BA$, it is given by:
    \[
        (ST)\vec{e}_j = S(T(\vec{e}_j)) = S(\vec{a}_j) = B\vec{a}_j = (BA)\vec{e}_j
    \]
    for all $j = 1, 2, \cdots, n$. This shows that the standard matrix of the composition $ST$ is indeed the matrix multiplication $BA$.
\end{proof}
\begin{remark}
    Note that $B$ is a $k \times m$ matrix and $A$ is an $m \times n$ matrix, so the matrix multiplication $BA$ is defined and results in a $k \times n$ matrix.
\end{remark}

The matrix multiplication $BA$ can be computed as follows:
\[
    BA = B\begin{bmatrix}
        | & | & & | \\
        \vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_n \\
        | & | & & |
    \end{bmatrix} = \begin{bmatrix}
        | & | & & | \\
        B\vec{a}_1 & B\vec{a}_2 & \cdots & B\vec{a}_n \\
        | & | & & |
    \end{bmatrix}
\]
where $\vec{a}_i = T(\vec{e}_i)$ is the $i$-th column of the matrix $A$. Also, 
\[
    Bx = x^1 \vec{b}_1 + x^2 \vec{b}_2 + \cdots + x^n \vec{b}_n = x^i \vec{b}_i
\]
where $\vec{b}_i = B\vec{a}_i$ is the $i$-th column of the matrix $B$. Note that $B$ is a $k \times m$ matrix, and $x \in \F^m$. Thus, the matrix multiplication $Bx$ is defined and results in a column matrix in $\F^k$.

\newpage

\section{Elementary Row Operations}\index{Elementary Row Operations}

\begin{definition}[Elementary Row Operations]
    Let $A$ be an $m \times n$ matrix over a field $\F$. An \emph{elementary row operation} on $A$ is one of the following operations:
    \begin{enumerate}
        \item Row Interchange: \qquad $R_i \leftrightarrow R_j$.
        \item Row Multiplication: \quad $R_i \to \alpha R_i$, where $\alpha \in \F \setminus \{0\}$.
        \item Row Addition: \qquad\quad\ $R_i \to R_i + \alpha R_j$, where $\alpha \in \F$ and $i \neq j$.
    \end{enumerate}
    Each elementary row operation can be represented by \emph{left multiplication} of $A$ by an appropriate $m \times m$ matrix over $\F$. Note that all of them are invertible linear maps from $\F^{m \times n}$ to $\F^{m \times n}$.
\end{definition}

For easier notations, we introduce the idea of matrix units, which is similar to the standard basis vectors $\vec{e_i}$.

\begin{definition}[Matrix Units]
    Let $m$ and $n$ be two positive integers and $\F$ be a field. The \emph{matrix unit} $E_i^j$ is the $m \times n$ matrix over $\F$ with $1$ in the $(i,j)$-th position and $0$ elsewhere, i.e.,
    \[
        (E_i^j)_k^l = \begin{cases}
            1 & \text{if } (k,l) = (i,j) \\
            0 & \text{otherwise}
        \end{cases}
    \]
    for all $1 \leq k \leq m$ and $1 \leq l \leq n$. The $(i,j)$-th position is the entry in the $i$-th row and $j$-th column.

    It can also be defined as $E_i^j = \vec{e}_i \hat{e}^j \in \M{m \times n}{\F}$ where $\vec{e}_i \in \F^m$ and $\vec{e}_j^T = \hat{e}^j \in (\F^n)^*$ are the $i$-th and $j$-th standard basis vectors, respectively. The $\hat{e}^j$ is the row matrix with 1 in the $j$-th column and 0 anywhere else.
\end{definition}
\begin{remark}
    Note that for any $m \times n$ matrix $A$ over a field $\F$, we have:
    \[
        \begin{split}
            A\vec{e}_j &= \text{the } j\text{-th column of } A \in \F^n \\
            \hat{e}^iA &= \text{the } i\text{-th row of } A \in (\F^m)^* \\
        \end{split}
    \]
    where $(\F^m)^*$ is the set of all row matrices with $n$ entries in $\F$. $\hat{e}^i$ is an element in $(\F^m)^*$ for any $1 \leq i \leq m$. Then we have: (Beaware of the difference between superscript and subscript)
    \[
        a_j^i = \hat{e}^i A \vec{e}_j = \text{the } (i,j)\text{-th entry of } A
    \]
\end{remark}

We can write the $E_i^j$ as:
\vspace{6ex}
\[
    E_i^j = \vec{e}_i \hat{e}^j = \begin{bmatrix}
        0 \\
        \vdots \\
        0 \\
        1 \\
        0 \\
        \vdots \\
        0
    \end{bmatrix} \begin{bmatrix}
        0 & \cdots & 0 & 1 & 0 & \cdots & 0
    \end{bmatrix} = \begin{bmatrix}
        0 & \cdots & 0 & \mypoint{herei}{0} & 0 & \cdots & 0 \\
        \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
        0 & \cdots & 0 & 1 & 0 & \cdots & \mypoint{herej}{0} \\
        0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
        \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & \cdots & 0 & 0 & 0 & \cdots & 0 \\
    \end{bmatrix}
\]
\begin{tikzpicture}[remember picture, overlay]
    \node[above=20pt of herei](textofhere1){the $j$-th column};
    \draw[myarrow] (textofhere1) -- (herei);
    \node[right=20pt of herej](textofhere2){the $i$-th row};
    \draw[myarrow] (textofhere2) -- (herej);
\end{tikzpicture}

Then we consider the row operations by using the matrix units.

\begin{proposition}
    The row operation $R_i \leftrightarrow R_j$ is a linear map where the standard matrix is $A_{R_i \leftrightarrow R_j} = I - E_i^i - E_j^j + E_i^j + E_j^i$.
\end{proposition}

\begin{proof}
    The linear map $T: \F^n \to \F^n$ is defined pointwisely. We can say the map is:
    \[
        \vec{e}_k \mapsto \begin{cases}
            \vec{e}_j & \text{if } k = i \\
            \vec{e}_i & \text{if } k = j \\
            \vec{e}_k & \text{if } k \neq i, j
        \end{cases}
    \]
    Then the standard matrix of $T$ is:
    \[
        A_{R_i \leftrightarrow R_j} = \begin{bmatrix}
            | & & | & & | & & | \\
            \vec{e}_1 & \cdots & \vec{e}_j & \cdots & \vec{e}_i & \cdots & \vec{e}_n \\
            | & & | & & | & & | 
        \end{bmatrix} = I - E_i^i - E_j^j + E_i^j + E_j^i
    \]
    where $I$ is the $n \times n$ identity matrix.
\end{proof}

\begin{proposition}
    The row operation $R_i \to \alpha R_i$ where $\alpha \in \F^{\times} := \F \setminus \{0\}$ is a linear map where the standard matrix is $A_{R_i \to \alpha R_i} = I + (\alpha - 1) E_i^i$.
\end{proposition}

\begin{proof}
    The linear map $T: \F^n \to \F^n$ is defined pointwisely. We can say the map is:
    \[
        \vec{e}_k \mapsto \begin{cases}
            \alpha \vec{e}_i & \text{if } k = i \\
            \vec{e}_k & \text{if } k \neq i
        \end{cases}
    \]
    Then the standard matrix of $T$ is:
    \[
        A_{R_i \to \alpha R_i} = \begin{bmatrix}
            | & & | & & | \\
            \vec{e}_1 & \cdots & \alpha\vec{e}_i & \cdots & \vec{e}_n \\
            | & & | & & |
        \end{bmatrix} = I + (\alpha - 1) E_i^i
    \]
    where $I$ is the $n \times n$ identity matrix.
\end{proof}

\begin{proposition}
    The row operation $R_i \to R_i + \alpha R_j$ where $\alpha \in \F$ and $i \neq j$ is a linear map where the standard matrix is $A_{R_i \to R_i + \alpha R_j} = I + \alpha E_i^j$.
\end{proposition}

\begin{proof}
    The linear map $T: \F^n \to \F^n$ is defined pointwisely. We can say the map is:
    \[
        \vec{e}_k \mapsto \begin{cases}
            \vec{e}_i + \alpha \vec{e_j} & \text{if } k = i \\
            \vec{e}_k & \text{if } k \neq i
        \end{cases}
    \]
    Then the standard matrix of $T$ is:
    \[
        A_{R_i \to R_i + \alpha R_j} = \begin{bmatrix}
            | & & | & & | & & | \\
            \vec{e}_1 & \cdots & \vec{e}_i + \alpha\vec{e}_j & \cdots & \vec{e}_n \\
            | & & | & & |
        \end{bmatrix} = I + \alpha E_i^j
    \]
    where $I$ is the $n \times n$ identity matrix.
\end{proof}

\newpage

\section{Dimensions of Vector Spaces}\index{Dimension of Vector Spaces}

\begin{definition}[Finite Dimensional Vector Spaces] \label{def:finite_dimensional_vector_space}
    A linear space $V$ over a field $\F$ is said to be \emph{finite dimensional} if there exists a linear equivalence $T: V \to \F^n$ for some positive integer $n$. In this case, we say that the dimension of $V$ is $n$, denoted $\dim_{\F} V = n$ or simply $\dim V = n$.
\end{definition}

\begin{definition}[Infinite Dimensional Vector Spaces]
    A linear space $V$ over a field $\F$ is said to be \emph{infinite dimensional} if $V$ is not finite dimensional.
\end{definition}

We have to proof if the dimension of a finite dimensional vector space is well-defined.

\begin{proposition}
    If there exists two linear equivalences $T: V \to \F^m$ and $S: V \to \F^n$, then $n = m$.
\end{proposition}

\begin{proof}
    Since $S$ is linear equivalence, it has a unique two-sided inverses $S^{-1}: \F^n \to V$. Consider the composition of this map:
    \[
        TS^{-1}: \F^n \to \F^m
    \]
    Since $TS^{-1}$ is compositions of linear equivalences, it is also a linear equivalence. Mutantis mutandis, for the opposite direction.

    Now, we know that a linear equivalence between two finite-dimensional vector spaces. Then we have $\dim \F^n = \dim \F^m$ or $n = m$. Thus, the dimension of a finite dimensional vector space is well-defined.
\end{proof}

Graphically, we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        V \arrow[swap, d, "S", hook, two heads] \arrow[r, "T", hook, two heads] & \F^m \\
        \F^n \arrow[ur, "TS^{-1}", hook, two heads, swap]
    \end{tikzcd}
\end{center}
\begin{remark}
    In drawing commutative diagram, we can use $\xhookrightarrow{}$ to denote an injective linear map, $\twoheadrightarrow$ to denote a surjective linear map, and $\cong$ or combining the two to denote an invertible linear map.
\end{remark}

\newpage

\section{Elementary Column Operations, Canonical Form and Rank}\index{Elementary Column Operations, Canonical Form and Rank}

\begin{definition}[Elementary Column Operations]
    Let $A$ be an $m \times n$ matrix over a field $\F$. An \emph{elementary column operation} on $A$ is one of the following operations:
    \begin{enumerate}
        \item Column Interchange: \qquad $C_i \leftrightarrow C_j$.
        \item Column Multiplication: \quad $C_i \to \alpha C_i$, where $\alpha \in \F \setminus \{0\}$.
        \item Column Addition: \qquad\quad\ $C_i \to C_i + \alpha C_j$, where $\alpha \in \F$ and $i \neq j$.
    \end{enumerate}
    Each elementary column operation can be represented by \emph{right multiplication} of $A$ by an appropriate $n \times n$ matrix over $\F$. Note that all of them are invertible linear maps from $\F^{m \times n}$ to $\F^{m \times n}$.
\end{definition}

\begin{proposition}
    Any $m \times n$ matrix $A$ can be transformed into a matrix of the form $\begin{bmatrix}
        I_r & 0 \\
        0 & 0
    \end{bmatrix}$ by a finite sequence of elementary row and column operations on $A$, where $r$ is the rank of $A$.
\end{proposition}

The following is the commutative diagram of the proposition above, where $B = \begin{bmatrix}
    I_r & 0 \\
    0 & 0
\end{bmatrix}$:

\begin{center}
    \begin{tikzcd}
        \F^n \arrow[r, "A", hook, two heads] \arrow[d, "Q", hook, two heads] & \F^m \arrow[d, "P", hook, two heads] \\
        \F^n \arrow[r, "B", hook, two heads] & \F^m
    \end{tikzcd}
\end{center}

Note that $P$ is the product of a finite sequence of elementary row operation matrices and $Q$ is the product of a finite sequence of elementary column operation matrices. Both $P$ and $Q$ are elementary and invertible matrices. Thus, we have:
\[
    \begin{bmatrix}
        I_r & 0 \\
        0 & 0
    \end{bmatrix} = P A Q^{-1}
\]

\begin{definition}[Canonical Form of a Matrix]
    The matrix $\begin{bmatrix}
        I_r & 0 \\
        0 & 0
    \end{bmatrix}$ obtained from an $m \times n$ matrix $A$ by a finite sequence of elementary row and column operations on $A$ is called the \emph{canonical form} of $A$.    
\end{definition}
\begin{remark}
    The canonical form of a matrix defined is also called the \emph{Smith Normal Form} or \emph{Normal Form} of a matrix.
\end{remark}

\begin{definition}[Rank of a Matrix]
    The \emph{rank} of an $m \times n$ matrix $A$ over a field $\F$, denoted by $\rank{A}$, is the number of leading 1's in the matrix $\begin{bmatrix}
        I_r & 0 \\
        0 & 0
    \end{bmatrix}$ obtained from $A$ by a finite sequence of elementary row and column operations on $A$.
\end{definition}
\begin{remark}
    The value $r$ is uniquely determined by $A$.
\end{remark}

% The graph of the canonical form of a matrix
\def\matriximg{%
    \begin{matrix}
        I_r & 0 \\
        0 & 0 
    \end{matrix}
}

\begin{proposition}
    Let $A$ be an $m \times n$ matrix over a field $\F$. Then the following statements are equivalent:
    \[
        A \text{ is invertible } \iff m \left\{\left[\vphantom{\matriximg}\right.\right.\kern-2\nulldelimiterspace
        \underbrace{\matriximg}_{\text{\normalsize $n$}}\kern-\nulldelimiterspace\left.\vphantom{\matriximg}\right] \text{ is invertible } \iff \rank{A} = m = n \iff \begin{bmatrix}
            I_r & 0 \\
            0 & 0
        \end{bmatrix} = I_m = I_n
    \]
\end{proposition}

\begin{proof}
    If $A$ is invertible, then the matrix $PAQ^{-1}$ is also invertible, as $P$ and $Q$ are elementary and invertible matrices, and hence the product is invertible. 
    
    If $PAQ^{-1}$ is invertible, and note that $m = n$ is automatically true. As only square matrix is invertible. Without the loss of generality, let say $PAQ^{-1}$ is a $m \times m$ matrix, then we have $\rank{PAQ^{-1}} = m$. Also note that the rank is invarient under multiplication by invertible matrices, so $\rank{A} = \rank{PAQ^{-1}}$. Hence, $\rank{A} = m = n$.

    If $\rank{A} = m = n$, as the canonical matrix remains the $m \times n$ structure, we know that the canonical form is actually a square matrix, let say $m \times m$. Also $r = \rank{A} = m$. Hence the whole canonical form become an identity matrix $I_m$.

    If the canonical form is an identity matrix $I$, i.e., it is invertible. Then the matrix $P^{-1}IQ = A$ is also invertible for some elementary and invertible matrices $P$ and $Q$.
\end{proof}

\begin{proposition}
    Let $A$ be an $m \times n$ matrix over a field $\F$. Then the following statements are equivalent:
    \[
        A \text{ has a left inverse } \iff A \text{ is injective } \iff \rank{A} = n \iff \begin{bmatrix}
            I_r & 0 \\
            0 & 0
        \end{bmatrix} = \begin{bmatrix}
            I_n \\
            0
        \end{bmatrix}
    \]
\end{proposition}

\begin{proof}
    If $A$ has a left inverse, let say $B$, then we have $BA = I_n$. Then for $B(A(x_1)) = B(A(x_2))$, we have $(BA)x_1 = (BA)x_2$, which implies $x_1 = x_2$. Hence it is injective.

    If $A$ is injective, we can consider $A = P^{-1}CQ$, where $C$ is the canonical form of the matrix $A$. Then we consider $P^{-1}CQ\vec{x} = \vec{0}$. Since $P^{-1}$ is invertible, it won't produce non-trivial solutions. We can consider $C(Q\vec{x}) = \vec{0} = C\vec{y}$. Then we have 
    \[
        \begin{bmatrix}
            I_r & 0 \\
            0 & 0
        \end{bmatrix} \begin{bmatrix}
            \vec{y}_1 \\
            \vec{y}_2
        \end{bmatrix} = \begin{bmatrix}
            0 \\
            0
        \end{bmatrix}
    \]
    where $\vec{y_1}$ and $\vec{y_2}$ are column vecotrs with size $r$ and $n - r$ respectively. Then $I_r \vec{y_1} = 0$, which implies $\vec{y_1} = 0$, while $\vec{y_2}$ can be anything. As $A$ is invertible, then $A\vec{x} = \vec{0}$ only has one trivial solution $\vec{x} = \vec{0}$. Also, $Q$ is invertible, hence $\vec{y}$ has only one trivial solution $\vec{0}$, i.e., $\vec{y}_2 = \vec{0}$. Hence we have $n - r = 0$ due to the size of $\vec{y}_2$ being 0. Hence the rank of $A$ is $n$.

    If $\rank{A} = n$, then the canonical form of $A$ is 
    \[
        \begin{bmatrix}
            I_{r \times r} & 0_{r \times (n - r)} \\
            0_{(m - r) \times r} & 0_{(m - r) \times (n - r)}
        \end{bmatrix} = \begin{bmatrix}
            I_{n \times n} & 0_{n \times (n - n)} \\
            0_{(m - n) \times n} & 0_{(m - n) \times (n - n)}
        \end{bmatrix} = \begin{bmatrix}
            I_{n \times n} \\
            0_{(m - n) \times n}
        \end{bmatrix} = \begin{bmatrix}
            I_n \\
            0
        \end{bmatrix} 
    \]

    If the canonical form of $A$ is $\begin{bmatrix}
        I_n \\
        0
    \end{bmatrix}$, then we consider $PAQ^{-1} = C$. Also, $A = P^{-1}CQ$. We construct a candidate for left inverse $D = [I_n \quad 0]$. Then we have $DC = [I_n \quad 0] \begin{bmatrix}
        I_n \\
        0
    \end{bmatrix} = I_n$. Then the left inverse of $A$ is $L = QDP^{-1}$. Then we check, $LA = QDP^{-1}A = QDP^{-1}PCQ^{-1} = I_n$. Hence, $A$ indeed has a left inverse.
\end{proof}

\begin{proposition}
    Let $A$ be an $m \times n$ matrix over a field $\F$. Then the following statements are equivalent:
    \[
        A \text{ has a right inverse } \iff A \text{ is surjective } \iff \rank{A} = m \iff \begin{bmatrix}
            I_r & 0 \\
            0 & 0
        \end{bmatrix} = \begin{bmatrix}
            I_m & 0
        \end{bmatrix}
    \]
\end{proposition}

\begin{proposition}
    For every $\vec{b}$, $\begin{bmatrix}
        I_r & 0 \\
        0 & 0
    \end{bmatrix} \vec{x} = \vec{b}$ has a unique solution.
\end{proposition}

Linear Algebra is the study of linear map between two finite dimensional vector spaces.

\begin{center}
    \begin{tikzcd}
        V \arrow[d, hook, two heads, "{[-]_{\B}}" swap] \arrow[dd, bend right=60, hook, two heads] \arrow[r, "T", hook, two heads] & W \arrow[d, hook, two heads, "{[-]_{\B'}}"] \arrow[dd, bend left=60, hook, two heads] \\
        \F^n \arrow[d, hook, two heads, "Q" swap] \arrow[r, "A", hook, two heads] & \F^m \arrow[d, hook, two heads, "P"] \\
        \F^n \arrow[r, "C", hook, two heads] & \F^m
    \end{tikzcd}
\end{center}
where $C = \begin{bmatrix}
    I_r & 0 \\
    0 & 0
\end{bmatrix}$, $\dim V = n$ and $\dim W = m$.

The coordinate maps $[-]_{\B}$ and $[-]_{\B'}$ are linear equivalences and they are the trivialisation of $V$ and $W$, respectively. The matrix $A$ is the standard matrix of the linear map $T: V \to W$ under the bases $\B$ and $\B'$. The matrix $C$ is the canonical form of $A$. The matrices $P$ and $Q$ are products of finite sequences of elementary row and column operation matrices, respectively. Both $P$ and $Q$ are elementary and invertible matrices.

\newpage

\section{Properties of Linear Maps}\index{Properties of Linear Maps}

Let $f : V \to W$ be a linear map between two finite dimensional vector spaces over $\F$. We have the following properties:
\begin{enumerate}
    \item $f$ is injective if and only if $\ker f = \{0_V\}$, i.e., the kernel is trivial.
    \item $f$ is surjective if and only if $\coker f = \{0_W\}$, i.e., the cokernel is trivial.
    \item $f$ is an isomorphism if and only if $\ker f = \{0_V\}$ and $\coker f = \{0_W\}$.
    \item $f$ is surjective if and only if for any linear map $g : W \to Z$, $g \circ f = 0$ implies $g = 0$.
    \item $f$ is injective if and only if for any linear map $h : U \to V$, $f \circ h = 0$ implies $h = 0$.
\end{enumerate}

Let $f : V \to W$ be a set map between linear spaces. Then the graph of $f$, $\Gamma_f := \{ (v, f(v)) \mid v \in V \}$ is a linear subspace of $V \oplus W$ if and only if $f$ is a linear map. Also, the domain of $f$ is isomorphic to $\Gamma_f$.

$f$ is injective if and only if $f$ is an imbedding, i.e., the map $\bar{f} : V \to \im{f}$ that sends $v$ to $f(v)$ is an isomorphism.

\chapter{Linear Spaces}

\epigraph{``Completion is one of the major great ideas in mathematics.''}{Guowu Meng}

\section{Linear Subspaces, Kernels and Images}\index{Linear Subspaces, Kernels and Images}

Here, we discuss linear spaces with more in depth terms.

\begin{definition}[Linear Subspaces]
    Let $W$ be a linear space over $\F$ and $V$ is a subset of $W$, denoted as $V \subset W$. $V$ is a \emph{linear subspace} of $W$ if $V$, with $+$ and $\cdot$ inherited from those of $W$, is a linear space.
\end{definition}

\begin{proposition}
    Let $V \subset W$. $V$ is a subspace of $W$ if and only if $V$ is not empty and $V$ is closed under $+$ and $\cdot$.
\end{proposition}

\begin{proof}
    If $V$ is a subspace of $W$, then $V$ is non-empty as a linear space must contain a zero vector by definition, as $V$ is also a linear space. Also, the other two are due to the axioms of linear space.

    If $V$ is not empty and closed under $+$ and $\cdot$, we just have to check the each axiom.
\end{proof}

\begin{definition}[Kernels]
    Let $f : V \to W$ be a linear map. The \emph{kernel} of $f$, denoted as $\ker f$, is defined as 
    \[
        \ker f \overset{\text{def}}{=\joinrel=} f^{-1}(0_W) = \{ v \in V \mid f(v) = 0_W \}
    \]
\end{definition}

\begin{example}
    Let $f : V \to W$ be a linear map. $\ker f$ is a subspace of domain of $f$, i.e., $V$.
    
    First, we have $0_V \in \ker f$, as $f(0_V) = 0_W$, so $\ker f$ is not empty.

    Then we consider $\alpha^1, \alpha^2 \in \F$ and $v_1, v_2 \in \ker f$, we have
    \[
        f(\alpha^1 v_1 + \alpha^2 v_2) = \alpha^1 f(v_1) + \alpha^2 f(v_2) = \alpha^1 (0_W) + \alpha^2 (0_W) = 0_W
    \]
    The first equality due to the linearity of $f$ and the second is due to $v_i \in \ker f$.
\end{example}

\begin{definition}[Images]
    Let $f : V \to W$ be a linear map. The \emph{image} of $f$, denoted by $\im f$, is defined as 
    \[
        \im f \overset{\text{def}}{=\joinrel=} \{ f(v) \mid v \in V \} \subset W
    \]
\end{definition}

\begin{example}
    Let $f : V \to W$ be a linear map. $\im f$ is a subspace of codomain of $f$, i.e., $W$.

    First, we have $f(0_V) = 0_W \in \im f$, so $\im f$ is not empty.

    Then we consider $\alpha^1, \alpha^2 \in \F$ and $f(v_1), f(v_2) \in \im f$. We have 
    \[
        \alpha^1 f(v_1) + \alpha^2 f(v_2) = f(\alpha^1 v_1 + \alpha^2 v_2) \in \im f
    \]
    The equality is due to the linearity of $f$.
\end{example}

\begin{example}
    Let $W$ be a linear space over a field $\F$ and $\{V_\alpha\}_{\alpha \in I}$ be the family of subspaces of $W$ indexed by the element in the index set $I$. Then $\bigcap_{\alpha \in I} V_\alpha$ is also a subspace of $W$.

    First, we have $0_W \in V_\alpha$ for all $\alpha \in I$, so $0_W \in \bigcap_{\alpha \in I} V_\alpha$. Thus, $\bigcap_{\alpha \in I} V_\alpha$ is not empty.

    Then we consider $\alpha^1, \alpha^2 \in \F$ and $v_1, v_2 \in \bigcap_{\alpha \in I} V_\alpha$. We have $v_1, v_2 \in V_\alpha$ for all $\alpha \in I$. Thus, $\alpha^1 v_1 + \alpha^2 v_2 \in V_\alpha$ for all $\alpha \in I$. This shows that $\alpha^1 v_1 + \alpha^2 v_2 \in \bigcap_{\alpha \in I} V_\alpha$.
\end{example}

Then we consider the duality of the intersection and union of subspaces. Whether the union of two subspaces is still a subspace? Unfortunately, the answer is no in general case. However, we have the following proposition.

\begin{proposition}
    Let $W$ be a linear space over a field $\F$ and consider the family of subspaces $\{V_\alpha\}_{\alpha \in I}$. Then $\bar{\bigcup_{\alpha \in I} V_\alpha}$ is a subspace of $W$ where $\bar{\bigcup_{\alpha \in I} V_\alpha}$ is the completion of $\bigcup_{\alpha \in I} V_\alpha$ under linear combinations. We call $\bar{\bigcup_{\alpha \in I} V_\alpha}$ the \emph{sum} of the subspaces $\{V_\alpha\}_{\alpha \in I}$, denoted by $\sum_{\alpha \in I} V_\alpha$.
\end{proposition}

\newpage

\section{Linear Span and Linear Independence}\index{Linear Span and Linear Independence}

\begin{definition}[Linear Span]
    Let $V$ be a linear space over a field $\F$ and $S \subset V$. The \emph{linear span} of $S$, denoted by $\Span_\F (S)$ or simply $\span S$ or $\bar{S}$ or $\langle S \rangle$, is defined as the completion of $S$ inside $V$ under linear combinations.
\end{definition}

\begin{corollary}
    The linear span of $S$ can also be defined as the intersection of all subspaces of $V$ containing $S$, which is the smallest linear subspace of $V$ containing $S$. It can be written as:
    \[
        \span S = \bigcap_{\alpha \in I} V_{\alpha} \subset V \quad \text{where } I = \{ V_{\alpha} \subset V \mid V_{\alpha} \text{ is a subspace of } V \text{ and } S \subset V_{\alpha} \}
    \]
\end{corollary}
\begin{remark}
    Note that $I$ is not empty as $V \in I$. Thus, $\span S$ is well-defined. $V$ is the largest subspace of itself and $\{0_V\}$ is the smallest subspace of $V$.
\end{remark}

\begin{proposition}
    Let $W$ be a linear space over a field $\F$ and $S \subset W$. Then 
    \[
        \span S = \left\{ \sum_{i=1}^{n} \alpha^i s_i \mid n \in \mathbb{N}, \alpha^i \in \F, s_i \in S \right\}
    \]
    Note that the summation is a finite summation.
\end{proposition}

\begin{definition}[Linear Independences] \label{def:linear_independence}
    Let $W$ be a linear space over a field $\F$ and $V_1, \cdots, V_k$ be subspaces of $W$. The subspaces $V_1, \cdots, V_k$ are said to be \emph{linearly independent} if $V_i \neq \{0_W\}$ for all $i$ and there is one and only one way to split $0_W \in W$ as a sum of vectors from each $V_i$, i.e., if $v_i \in V_i$ for all $i$ and $\sum_{i=1}^{k} v_i = 0_W$, then $v_i = 0_W$ for all $i$.
\end{definition}

Vectors $v_1, v_2, \cdots, v_k \in W$ are said to be independent if the subspaces $\span{v_1}$, $\span{v_2}$, $\cdots$, $\span{v_k}$ are linearly independent.

\begin{proposition}
    $v_1, v_2, \cdots, v_k \in W$ are linearly independent if and only if there is one and only one way to write $0_W \in W$ as the combination of $v_1, \cdots, v_k$ with coefficients in $\F$, i.e., the equation 
    \[
        \alpha^1 v_1 + \cdots + \alpha^k v_k = 0_W
    \]
    has only  the trivial solution, i.e., $\alpha^i = 0$ for all $i$.
\end{proposition}

\newpage

\section{Linearly Independent Sets and Spanning Sets}\index{Linearly Independent Sets and Spanning Sets}

If we consider a set, what does it mean by being linearly independent? Is there any properties for spanning if the set spans the whole codomain?

\begin{definition}[Linearly Independent Sets]
    Let $V$ be a linear space over a field $\F$. A subset $S \subseteq V$ is said to be a \emph{linearly independent set} of vectors in $V$ if no elements in $S$ can be expressed as a linear combination of the finitely many other elements in $S$.
\end{definition}

\begin{definition}[Spanning Sets]
    Let $V$ be a linear space over a field $\F$. A subset $S \subseteq V$ is said to be a \emph{spanning set} of $V$ if $\span{S} = V$.
\end{definition}

\begin{example}
    Let $V = \F^3$ and consider the three vectors $\vec{e}_1$, $\vec{e}_2$ and $\vec{e}_3$. 
    
    Then the set $S = \{\vec{e}_1, \vec{e}_2, \vec{e}_1 + \vec{e}_2\}$ is not a spanning set of $V$ as $\span{S} = \Span \{\vec{e}_1, \vec{e}_2\} \neq V$. If we consider the $\Span \{\vec{e}_1, \vec{e}_2\} = W$, then $\{\vec{e}_1, \vec{e}_2\}$ is a minimal spanning set of $W$.

    The set $S = \{\vec{e}_1, \vec{e}_1 + \vec{e}_2, \vec{e}_1 + \vec{e}_2 + \vec{e}_3\}$ is a spanning set of $V$.
\end{example}
\begin{remark}
    If we consider the matrix of $\{\vec{e}_1, \vec{e}_2, \vec{e}_1 + \vec{e}_2\}$ with respect to the standard basis of $\F^3$, we have:
    \[
        A = \begin{bmatrix}
            1 & 0 & 1 \\
            0 & 1 & 1 \\
            0 & 0 & 0
        \end{bmatrix}
    \]
    Then we have $\rank{A} = 2 < 3$. Thus, the set is not a spanning set of $\F^3$.
\end{remark}

\begin{example}
    Consider the subset $S = \{ 1, t, t^2, \cdots \} \subset \F[[t]]$. Then $\span{S} = \F[t]$ which is a proper subspace of $\F[[t]]$. As the linear combination of finitely many elements in $S$ is a polynomial, but an element in $\F[[t]]$ can be a power series.
\end{example}

\begin{definition}[Minimal Spanning Sets]
    Let $V$ be a linear space over a field $\F$. A spanning set $S \subseteq V$ is said to be a \emph{minimal spanning set} of $V$ if no proper subset of $S$ is a spanning set of $V$, i.e., $S' \subset S \implies \span{S'} \subset \span{S} = V$ where $\span{S'} \neq V$.
\end{definition}

The following is also the equivalence definition of linearly independent sets, spanning sets and minimal spanning sets.

Given a linear space $V$ over a field $\F$. We define the order set $S := \{\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n\} \subseteq V$. The order set $S$ forms a linear map $\phi_S: \F^n \to V$ defined by:
\[
    \phi_S(\vec{x}) = \phi_S\left(\begin{bmatrix}
        x^1 \\
        x^2 \\
        \vdots \\
        x^n
    \end{bmatrix}\right) = x^1 \vec{v}_1 + x^2 \vec{v}_2 + \cdots + x^n \vec{v}_n = \sum_{i=1}^{n} x^i \vec{v}_i
\]

\begin{proposition}
    The order set $S := \{\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n\} \subseteq V$ is said to be linearly independent if and only if the linear map $\phi_S: \F^n \to V$ defined above is injective.
\end{proposition}

\begin{proposition}
    The order set $S := \{\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n\} \subseteq V$ is said to be a spanning set of $V$ if and only if the linear map $\phi_S: \F^n \to V$ defined above is surjective.
\end{proposition}

\begin{proposition} \label{prop:minimal_spanning_set}
    The order set $S := \{\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n\} \subseteq V$ is said to be a minimal spanning set of $V$ if and only if the linear map $\phi_S: \F^n \to V$ defined above is bijective.
\end{proposition}
\begin{remark}
    A order minimal spanning set is regarded as \emph{basis}.
\end{remark}

\begin{example}
    Let $X$ be a set, $\F[[X]]$ be the set of all functions $f: X \to \F$ and $\F[X]$ be the set of all finite support functions $f: X \to \F$. For each $x \in X$, we define the Kronecker delta function $\delta_x : X \to \F$ at point $x$ by 
    \[
        \delta_x(y) = \begin{cases}
            1 & \text{if } y = x \\
            0 & \text{if } y \neq x
        \end{cases}
    \]
    Clearly, $\delta_x$ has finite support, thus $\delta_x \in \F[X]$.

    Then we have a set $\delta_X = \{\delta_x \mid x \in X\} \subset \F[X]$. We have $\span{\delta_X} = \F[X]$ as any finite support function $f: X \to \F$ can be written as a linear combination of finitely many delta functions. Thus, $\delta_X$ is a spanning set of $\F[X]$.

    Moreover, $\delta_X$ is a linearly independent set. Assume that there exists a finite linear combination of other delta functions such that $\delta_x = \sum \alpha^y \delta_{y}$. Then we have $\delta_x(x) = 1 = \sum \alpha^y \delta_y(x) = 0$. This is a contradiction. Thus, $\delta_X$ is a linearly independent set.
\end{example}

\newpage

\section{Group Actions}\index{Group Actions}

Next, we discuss quotient space. However, before introducing quotient space, we have to understand what group actions are. 

\begin{definition}[Group Actions]
    Let $G$ be a group and $X$ be a set. A \emph{left group action} of $G$ on $X$ is a map $\cdot : G \times X \to X$, $(g, x) \mapsto g \cdot x$, such that for all $g_1, g_2 \in G$ and $x \in X$, the following properties hold:
    \begin{enumerate}
        \item Compatibility: $(g_1 g_2) \cdot x = g_1 \cdot (g_2 \cdot x)$.
        \item Identity: $e \cdot x = x$ where $e$ is the identity element of $G$.
    \end{enumerate}
\end{definition}

Same for the right group action of $G$ on $X$, just think it dually.

Consider rotation on a plane. It is a group action of the group $SO(2)$ on the set $\R^2$.
\[
    g = \begin{pmatrix}
        \cos{\theta} & -\sin{\theta} \\
        \sin{\theta} & \cos{\theta}
    \end{pmatrix}
\]

Then we have the following group action:
\begin{center}
    \begin{tikzpicture}
        \draw[draw=none,fill=gray!15] (-2.5,-2.5) rectangle (2.5,2.5);
        \draw[red] (0,0) circle (1.5cm);
        \draw[red] (0,0) circle (1cm);
        \draw[red] (0,0) circle (0.5cm);
        \draw[red] (1.06066,-1.06066) -- (1.4,-1.4) node[below right]{Orbits};

        \filldraw[ocre] (1.414,1.414) circle (1pt) node[above right]{$g \cdot \vec{v}$};
        \draw[-Stealth] (-2.5,0) -- (2.5cm + 5pt,0) node[right]{$x$};
        \draw[-Stealth] (0,-2.5) -- (0,2.5cm + 5pt) node[above]{$y$};
        \filldraw[ocre] (2,0) circle (1pt) node[below right]{$\vec{v}$};
        \draw[-Stealth,ocre] (2,0) arc[start angle=0,end angle=45,radius=2];
        \draw[-Stealth,red] (0,2) arc[start angle=90,end angle=135,radius=2];
        \draw[-Stealth,red] (-1.414,-1.414) arc[start angle=225,end angle=270,radius=2];
    \end{tikzpicture}
\end{center}

\begin{definition}[Orbits]
    Let $G$ be a group acting on a set $X$. The \emph{orbit} of the action through a point $x \in X$, denoted as $G \cdot x$, is defined as the set of points in $X$ that can be reached from $x$ by the action of elements of $G$, i.e., 
    \[
        G \cdot x = \{g \cdot x \mid g \in G\}
    \]
\end{definition}

There is only two situation for the orbits, either the origin or a circle.

In the following section, we may regard the orbits $G \cdot x$ as a \emph{coset}.

\begin{definition}[Partition]
    A \emph{partition} of a set $X$ is a collection of non-empty, disjoint subsets of $X$ whose union is $X$. The partition of the set $X$ is the same as an equivalence relation on $X$.
\end{definition}

Orbits give a partition of the set $X$, i.e., $X$ can be expressed as the disjoint union of its orbits. The orbits of the action are the equivalence classes of the equivalence relation.

Let $f : X \to Y$ be a map between two sets $X$ and $Y$. Then $f$ defines a partition of $X$ by the equivalence relation. The equivalence classes are the preimages of points in $Y$, i.e., $f^{-1}(y)$ for each $y \in Y$.

\newpage

\section{Quotient Spaces}\index{Quotient Spaces}

Let $V$ be a subspace of a linear space $W$ over a field $\F$. We know $(V, +)$ is an abelian group. Then we have the group action of $V$ on $W$ defined by: $(v, w) \mapsto v \cdot w$ for all $v \in V, w \in W$. $v \cdot w$ is defined as $v + w$ where $+$ is the addition operation in $W$.
We know that $(v_1 + v_2) + w = v_1 + (v_2 + w)$ and $0_V + w = w$ for all $v_1, v_2 \in V$ and $w \in W$. Thus, it is a group action. 

The following commutative diagram illustrates the group action, where the associative and identity properties are inherited from the addition operation in $W$, i.e., we need not prove the group action as above.

\begin{center}
    \begin{tikzpicture}
        \matrix (m) [matrix of math nodes, row sep=3em, column sep=4em, minimum width=2em]
        { & W \times W & \\ V \times W & & W \\};
        \path[->]
        (m-1-2) edge (m-2-3)
        (m-2-1) edge (m-2-3)
                edge (m-1-2);
    \end{tikzpicture}
\end{center}

This group action defines the following equivalence relation on $W$, where $V$ is the acting group:
\[
    \begin{split}
        w_1 \sim w_2 & \implies \exists v \in V \text{ such that } w_2 = v + w_1 \\
        & \iff w_2 - w_1 \in V
    \end{split}
\]

\begin{definition}[Quotient Spaces]
    Let $W$ be a linear space over a field $\F$ and $V$ be a subspace of $W$. The \emph{quotient space} of $W$ by $V$, denoted by $\quotient{W}{V}$, is defined as the set of orbits of the group action of $V$ on $W$, or the set of $V$-equivalence classes in $W$ with the equivalence relation defined above, i.e.,
    \[
        \quotient{W}{V} = \{ V \cdot w \mid w \in W \} = \{ w + V \mid w \in W \}
    \]
    where $V \cdot w = w + V = \{ w + v \mid v \in V \}$ is called the \emph{coset} of $V$ in $W$ containing $w$.
\end{definition}

\begin{definition}[Quotient Map]
    The natural surjective map $\pi : W \to \quotient{W}{V}$ defined by $\pi(w) = w + V$ for all $w \in W$ is called the \emph{quotient map} or \emph{projection map}. Note that $w + V$ can be written as $\bar{w}$ or $[w]$.
\end{definition}

In general, if a group $G$ acts on a set $X$, then the quotient set $\quotient{X}{G}$ is defined as the set of orbits of the action, i.e.,
\[
    \quotient{X}{G} = \{ G \cdot x \mid x \in X \}
\]

Similarly, there is a natural surjective map $\pi : X \to G$ defined by $\pi(x) = G \cdot x$ for all $x \in X$.

The following is a graphical illustration of the quotient space.

\begin{center}
    \begin{tikzpicture}
        \draw[step=0.5cm,gray!20] (-2,-2) grid (2,2);
        \filldraw (0,0) circle (1pt) node [below] {$O$};
        \draw[thick,ocre] (-2,-2) -- (2,2) node [right] {$V = [0]$};
        \draw[thick,red] (-2,-1) node [left] {$[w'_1] = [w_1] = w_1 + V$} -- (1,2);
        \draw[thick,orange] (-2,0) node [left] {$[w_2] = w_2 + V$} -- (0,2);
        \draw[thick,magenta] (-2,1) node [left] {$[w_3] = w_3 + V$} -- (-1,2);
        \draw[thick,olive] (-1,-2) -- (2,1) node [right] {$w_4 + V = [w_4]$};
        \draw[thick,brown] (0,-2) -- (2,0) node [right] {$w_5 + V = [w_5]$};
        \draw[thick,purple] (1,-2) -- (2,-1) node [right] {$w_6 + V = [w_6]$};

        \draw[thick,-Stealth,violet] (0,0) -- (0,1) node [pos=0.38, xshift=-1ex] {\scriptsize $w_1$};
        \draw[thick,-Stealth,violet] (0,0) -- (0.5,0.5) node [midway, below] {\scriptsize $v$};
        \draw[thick,-Stealth,violet] (0,0) -- (0.5,1.5) node [right] {\scriptsize $w_1 + v$};
        
        \draw[thick,-Stealth,teal,dashed] (0,0) -- (-1,0) node [midway, yshift=1ex] {\scriptsize $w'_1$};
        \draw[thick,-Stealth,teal,dashed] (0,0) -- (-0.75,-0.75) node [pos=0.625, right] {\scriptsize $v'$};
        \draw[thick,-Stealth,teal,dashed] (0,0) -- (-1.75,-0.75) node [xshift=2.5ex, yshift=-1ex] {\scriptsize $w'_1 + v'$};
    \end{tikzpicture}
\end{center}

We can see that each line parallel to $V$ represents a coset of $V$ in $W$. The quotient space $\quotient{W}{V}$ is the set of all such lines. We may consider each line as an orbit of the group action of $V$ on $W$. Note that there is not only one unique way to represent the coset $w + V$. Just like the illustration above, $w_1$ and $w'_1$ are two different representatives of the same coset $w_1 + V = w'_1 + V$. Note that their difference is an element in $V$, i.e., $w_1 - w'_1 \in V$.

Note that we now do not know whether $\quotient{W}{V}$ is a linear space or not. We will show that it is indeed a linear space by using the following proposition.

\begin{proposition}
    There is a unique linear structure on $\quotient{W}{V}$ such that the quotient map $\pi : W \to \quotient{W}{V}$ is a linear map.
\end{proposition}

\begin{proof}
    Assume that such a linear structure exists. Then for all $w_1, w_2 \in W$ and $\alpha_1, \alpha_2 \in \F$, we have
    \[
        \pi(\alpha_1 w_1 + \alpha_2 w_2) = [\alpha_1 w_1 + \alpha_2 w_2] = \alpha_1 [w_1] + \alpha_2 [w_2] = \alpha_1 \pi(w_1) + \alpha_2 \pi(w_2)
    \]
    This suggests that $\alpha_1 [w_1] + \alpha_2 [w_2]$ should be defined as $[\alpha_1 w_1 + \alpha_2 w_2]$ if $\pi$ is linear. As there is only one formula, this proves the uniqueness of the linear structure on $\quotient{W}{V}$.

    Then we consider whether the linear combination on $\quotient{W}{V}$ is well-defined. Assume that $[w_1] = [w'_1]$ and $[w_2] = [w'_2]$, i.e., $w_1 - w'_1 \in V$ and $w_2 - w'_2 \in V$. Then we have
    \[
        (\alpha_1 w_1 + \alpha_2 w_2) - (\alpha_1 w'_1 + \alpha_2 w'_2) = \alpha_1 (w_1 - w'_1) + \alpha_2 (w_2 - w'_2) \in V
    \]
    which means $[\alpha_1 w_1 + \alpha_2 w_2] = [\alpha_1 w'_1 + \alpha_2 w'_2]$. This means that the linear combination is independent of the choice of representatives. Thus, the linear combination is well-defined.
\end{proof}

In normal procedure, we first define the operations and then check whether the set is closed under the operations and zero exists. Then we check whether the map preserves the structure and show the uniqueness of the structure. However, in this case, we first assume that such a structure exists and then derive the operations from the assumption. Then we check whether the operations are well-defined.

In the first part, we show that there is only one possible way to define the operations if the quotient map is linear. Also, during the definition, it ensures the preservation of the linear structure. In the second part, we show that the operations on the set $\quotient{W}{V}$ are well-defined.

If we consider the graphical representation of the quotient space $\quotient{W}{V}$ and the quotient map $\pi$, we may use the following diagram:

\begin{center}
    \begin{tikzpicture}
        \filldraw[gray!10] (-4,-2) rectangle (4,2);
        \draw[step=0.5cm,gray!20] (-4,-2) grid (4,2);
        \filldraw (0,0) circle (1pt) node [below left] {$0$};
        \draw[thick] (-2.5,-2) -- (-2.5,2) node [above] {\scriptsize $V + a$};

        \draw[thick] (0,-2)  -- (0,2) node [above] {\scriptsize $V$};

        \draw[thick] (2,-2) -- (2,2) node [above] {\scriptsize $b + V$};
        \draw[thick] (3.5,-2)  -- (3.5,2) node [above] {\scriptsize $(3.5, 0) + V$};
        
        \filldraw (-2.5, 1) circle (1pt) node [right] {$a$};
        \filldraw (2, -0.5) circle (1pt) node [right] {$b$};

        \draw[decoration={brace,raise=5pt},decorate]
            (-4,-2) -- node [left=6pt] {$W$} (-4,2);
        \draw[-Stealth] (-4 cm - 15 pt,-10 pt) -- (-4 cm - 15 pt, -4 cm + 10 pt) node [midway, left] {$\pi$};

        \draw[thick] (-4,-4) node [left] {$\quotient{W}{V}$} -- (4,-4);

        \draw[dashed] (-2.5,-2) -- (-2.5,-4);
        \draw[dashed] (0,-2) -- (0,-4);
        \draw[dashed] (2,-2) -- (2,-4);
        \draw[dashed] (3.5,-2) -- (3.5,-4);

        \filldraw (-2.5,-4) circle (1.5pt) node [below] {$[a]$};
        \filldraw (0,-4) circle (1.5pt) node [below] {$[0]$};
        \filldraw (2,-4) circle (1.5pt) node [below] {$[b]$};
        \filldraw (3.5,-4) circle (1.5pt) node [below] {$[(3.5, 0)]$};
    \end{tikzpicture}
\end{center}

\newpage

\section{Universal Properties}\index{Universal Properties}

\begin{proposition}
    Let $V$ be a linear space over a field $\F$ and $S$ be a minimal spanning set of $V$. Then for any set map $\phi : S \to Z$, where $Z$ is any linear space over $\F$, there is a unique linear map $\tilde{\phi} : V \to Z$ such that $\tilde{\phi}|_S = \phi$.

    In other words, the following diagram commutes:
    \begin{center}
        \begin{tikzcd}
            s \arrow[d, mapsto, xshift=-1.8ex] \in S \arrow[r, "\phi"] \arrow[swap, d, "\iota", hook, xshift=1.5ex] & Z \\
            s \in V \arrow[ru, "\tilde{\phi}"', dashed] &
        \end{tikzcd}
    \end{center}
\end{proposition}

\begin{proof}
    Assume the existence of such a linear map $\tilde{\phi}$. Then for all $s \in S$, we have $\tilde{\phi} \circ \iota (s) = \tilde{\phi}(s) = \phi(s)$.

    Since $S$ is a minimal spanning set of $V$, for any $v \in V$, we have a unique way to write $v$ as a linear combination of finitely many elements in $S$, i.e., $v = \sum_{i=1}^{n} \alpha_i s_i$ where $\alpha_i \in \F$ and $s_i \in S$ are distinct. Then we have 
    \[
        \tilde{\phi}(v) = \tilde{\phi}\left(\sum_{i=1}^{n} \alpha^i s_i\right) = \sum_{i=1}^{n} \alpha^i \tilde{\phi}(s_i) = \sum_{i=1}^{n} \alpha^i \phi(s_i) = \phi\left(\sum_{i=1}^{n} \alpha^i s_i\right) = \phi(v)
    \]
    This shows that $\tilde{\phi}$ agrees with $\phi$ on all of $V$, and thus $\tilde{\phi}$ is uniquely determined by $\phi$.
\end{proof}

Note that we first define the map on the spanning set and then extend it to the whole space. The uniqueness is due to the fact that there is only one way to write each element in $V$ as a linear combination of elements in $S$ and the existence is due to the fact that we can always define the map on $V$ by using the linear combination.

This proposition shows thats a linear space with a minimal spanning set has the following universal property: any set map from the minimal spanning set to another linear space can be uniquely extended to a linear map from the whole space to that linear space.

\begin{center}
    \begin{tikzcd}[row sep=tiny, column sep=tiny]
        \phi \arrow[r, mapsto] & \tilde{\phi} \\
        \Map(S, Z) \arrow[r, leftrightarrow, "\cong" description] & \Hom(V, Z) \\
        \tilde{\phi} \circ \iota \arrow[r, mapsto] & \tilde{\phi} \\
    \end{tikzcd}
\end{center}

\begin{proposition}
    Let $W$ be a linear space over a field $\F$ and $V$ be a subspace of $W$. Then we have the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            & V \arrow[d, "\iota", hook] \arrow[ddl, bend right, "0" swap] \arrow[ddr, bend left, "0"] \\
            & W \arrow[dl, "\forall \phi" swap] \arrow[dr, "\pi"] \\
            Z & & \quotient{W}{V} \arrow[ll, "\exists ! \tilde{\phi}" swap, dashed] 
        \end{tikzcd}
    \end{center}
    where $Z$ is any linear space over $\F$ and $\phi : W \to Z$ is any linear map such that $\phi(v) = 0_Z$ for all $v \in V$. Then there is a unique linear map $\tilde{\phi} : \quotient{W}{V} \to Z$ such that $\tilde{\phi} \circ \pi = \phi$.
\end{proposition}

\begin{proof}
    Assume the existence of such a linear map $\tilde{\phi}$. Then for all $w \in W$, we have $\tilde{\phi}([w]) = \phi(w)$. However, this may not be well-defined. Then, we check whether it is well-defined. Assume that $[w] = [w']$, then we have $\tilde{\phi}([w']) = \phi(w')$. Note that $w - w' \in V$. Thus, we have $\phi(w' - w) = 0_Z$. This means that $\phi(w') - \phi(w) = 0_Z$, i.e., $\phi(w') = \phi(w)$. This shows that $\tilde{\phi}([w']) = \tilde{\phi}([w])$. Thus, $\tilde{\phi}$ is well-defined.

    Then we consider the linearity of $\tilde{\phi}$. For all $[w_1], [w_2] \in \quotient{W}{V}$ and $\alpha^1, \alpha^2 \in \F$, we have
    \[
        \begin{split}
            \tilde{\phi}(\alpha^1 [w_1] + \alpha^2 [w_2]) &= \tilde{\phi}([\alpha^1 w_1 + \alpha^2 w_2]) \\
            &= \phi(\alpha^1 w_1 + \alpha^2 w_2) \\
            &= \alpha^1 \phi(w_1) + \alpha^2 \phi(w_2) \\
            &= \alpha^1 \tilde{\phi}([w_1]) + \alpha^2 \tilde{\phi}([w_2])
        \end{split}
    \]
    This shows that $\tilde{\phi}$ is linear.
\end{proof}
\begin{remark}
    Note that $[0] = V$. If $v \in V$, then $[v] = v + V = \{v + v' \mid v' \in V\} = \{v'' \mid v'' \in V\} = V = [0]$. Thus, $\pi(v) = [v] = [0]$ for all $v \in V$. So the map from $V \to \quotient{W}{V}$ is the zero map. Thus, the triangle commutes. Also, the map from $v$ to $Z$ is defined as the zero map, making the construction of $\tilde{\phi}$ is possible, as the key step is that $\phi(w' - w) = 0_Z$ for all $w' - w \in V$.
\end{remark}

Generally, we may consider the following commutative diagrams, where left is the general case and right is the dual case:

\begin{center}
    \begin{tikzcd}
        & \im{f} \arrow[d, "\iota", hook] \arrow[ddl, bend right, "0" swap] \arrow[ddr, bend left, "0"] \\
        & W \arrow[dl, "\forall \phi" swap] \arrow[dr, "\pi"] \\
        Z & & \coker{f} \arrow[ll, "\exists ! \tilde{\phi}" swap, dashed] 
    \end{tikzcd}
    \begin{tikzcd}[swap]
        & \coim{f} \arrow[from=d, "\iota", hook] \arrow[from=ddl, bend left, "0" swap] \arrow[from=ddr, bend right, "0"] \\
        & W \arrow[from=dl, "\forall \phi" swap] \arrow[from=dr, "\pi"] \\
        Z & & \ker{f} \arrow[from=ll, "\exists ! \tilde{\phi}" swap, dashed] 
    \end{tikzcd}
\end{center}

\begin{definition}[Cokernel]
    Let $f : V \to W$ be a linear map between two linear spaces over a field $\F$. The \emph{cokernel} of $f$, denoted by $\coker{f}$, is defined as the quotient space of $W$ by the image of $f$, i.e., 
    \[
        \coker{f} = \quotient{W}{\im{f}} = W / \im{f}
    \]
    where $\im{f} = \{ f(v) \mid v \in V \}$ is the image of $f$.
\end{definition}

\begin{definition}[Coimage]
    Let $f : W \to V$ be a linear map between two linear spaces over a field $\F$. The \emph{coimage} of $f$, denoted by $\coim{f}$, is defined as the quotient space of the domain $W$ by the kernel of $f$, i.e.,
    \[
        \coim{f} = \quotient{W}{\ker{f}} = W / \ker{f}
    \]
    where $\ker{f} = \{ w \in W \mid f(w) = 0_V \}$ is the kernel of $f$.
\end{definition}

\newpage

\section{Sum and Direct Sum}\index{Sum and Direct Sum}

\begin{definition}[Sum of Subspaces]
    Let $V_1$ and $V_2$ be two subspaces of a linear space $W$ over a field $\F$. The \emph{sum} of $V_1$ and $V_2$, denoted by $V_1 + V_2$, is defined as the set of all possible sums of elements from $V_1$ and $V_2$, i.e.,
    \[
        V_1 + V_2 = \{ v_1 + v_2 \mid v_1 \in V_1, v_2 \in V_2 \}
    \]
\end{definition}

\begin{proposition}
    The sum $V_1 + V_2$ of two subspaces $V_1$ and $V_2$ of a linear space $W$ over a field $\F$ is also a subspace of $W$.
\end{proposition}

\begin{proposition}
    $V_1 + V_2 = \span{V_1 \cup V_2}$.
\end{proposition}

Recall the definition of linear independence (Definition \ref{def:linear_independence}): $V_1$ and $V_2$ are said to be \emph{linearly independent} if $V_1$ and $V_2$ are non-trivial and $x_1 + x_2 = 0$ for $x_i \in V_i$ implies that $x_1 = x_2 = 0$.

We have the following definition for weakly linear independence.
\begin{definition}[Weak Linear Independence]
    Let $V_1$ and $V_2$ be two subspaces of a linear space $W$ over a field $\F$. $V_1$ and $V_2$ are said to be \emph{weakly linearly independent} if $x_1 + x_2 = 0$ for $x_1 \in V_1$ and $x_2 \in V_2$ implies that $x_1 = x_2 = 0$. Note that $V_1$ or $V_2$ can be trivial.
\end{definition}

Then the definition of direct sum is as follows.
\begin{definition}[Direct Sum of Subspaces]
    Let $V_1$ and $V_2$ be two subspaces of a linear space $W$ over a field $\F$. The \emph{direct sum} of $V_1$ and $V_2$, denoted by $V_1 \oplus V_2$, is defined as the sum $V_1 + V_2$ when $V_1$ and $V_2$ are weakly linearly independent, i.e.,
    \[
        V_1 \oplus V_2 = V_1 + V_2
    \]
    when $V_1$ and $V_2$ are weakly linearly independent.    
\end{definition}

Recall (Definition \ref{def:finite_dimensional_vector_space}) that $W$ is a finite dimensional if $W \cong \F^n$ for some positive integer $n$. It is equivalent to saying that $W$ is finitely spanned, i.e., having a finite spanning set.

\begin{proof}
    If we have a map $\phi : \F^n \to W$, then $W = \Span\{\phi(e_1), \phi(e_2), \cdots, \phi(e_n)\}$. However, the set $\{\phi(e_1), \phi(e_2), \cdots, \phi(e_n)\}$ may not be linearly independent. Thus, we can always find a minimal spanning set of $W$ from it. WLOG, we can say $W = \Span\{\phi(e_1), \phi(e_2), \cdots, \phi(e_k)\}$ for some $k \leq n$. Then using (Proposition \ref{prop:minimal_spanning_set}), we have a bijective map $\phi_{\{e_1, e_2, \cdots, e_k\}} : \F^k \to W = \Span\{\phi(e_1), \phi(e_2), \cdots, \phi(e_k)\}$.
\end{proof}

\begin{proposition} \label{prop:finite_dimensional_subspace_quotient_space}
    $W$ is finite dimensional if and only if all its subspaces and quotient spaces are finite dimensional.
\end{proposition}

\begin{proof}
    For subspace $U \subseteq W$ and $W$ is finite dimensional, we have:
    \begin{center}
        \begin{tikzcd}
            W \arrow[r, two heads] & U \\
            \F^n \arrow[u, hook, two heads] \arrow[ur, two heads, dashed, "\phi"]
        \end{tikzcd}
    \end{center}
    Then the map $\phi : \F^n \to U$ is defined by $x = \alpha^1 \vec{e}_1 + \cdots + \alpha^n \vec{e}_n \mapsto \phi(x) = \alpha^1 \phi(\vec{e}_1) + \cdots + \alpha^n \phi(\vec{e}_n)$. Thus, $U$ is finitely spanned, $U = \Span\{\phi(\vec{e}_1), \phi(\vec{e}_2), \cdots, \phi(\vec{e}_n)\}$. 

    For quotient space $\quotient{W}{V}$ and $W$ is finite dimensional, we have:
    \begin{center}
        \begin{tikzcd}
            V \arrow[r, hook, "\iota"] & W \arrow[r, two heads, "\pi"] & \quotient{W}{V}
        \end{tikzcd}
    \end{center}
    Then we know that $\pi(\vec{e}_1), \pi(\vec{e}_2), \cdots, \pi(\vec{e}_n)$ spans $\quotient{W}{V}$. Thus, $\quotient{W}{V}$ is finitely spanned.
\end{proof}

\begin{proposition} \label{prop:dimension_of_sum_of_subspaces}
    $\dim{(V_1 + V_2)} \leq \dim{V_1} + \dim{V_2}$. Equality holds if and only if the sum is direct.
\end{proposition}
\begin{proof}
    For $V_1$ and $V_2$, we can find the minimal spanning sets $S_1$ and $S_2$ respectively. Then we claim that $S_1 \cup S_2$ spans $V_1 + V_2$, i.e., $V_1 + V_2 = \Span\{S_1 \cup S_2\}$. 
    
    This is because for all $v \in V_1 + V_2$, we have $v = v_1 + v_2$ for some $v_i \in V_i$. Then we can write $v_i$ as a linear combination of finitely many elements in $S_i$, i.e., $v_i = \sum_{j=1}^{n_i} \alpha_i^j s_i^j$ where $\alpha_i^j \in \F$ and $s_i^j \in S_i$ are distinct. Thus, we have 
    \[
        v = v_1 + v_2 = \sum_{j=1}^{n_1} \alpha_1^j s_1^j + \sum_{j=1}^{n_2} \alpha_2^j s_2^j \in \Span\{S_1 \cup S_2\}
    \]
    This shows that $V_1 + V_2 \subseteq \Span\{S_1 \cup S_2\}$. The other direction is obvious. Thus, we have $V_1 + V_2 = \Span\{S_1 \cup S_2\}$.

    Then we have $\dim{(V_1 + V_2)} \leq |S_1| + |S_2| = \dim{V_1} + \dim{V_2}$, as $S_1 \cup S_2$ may not be a minimal spanning set. The equality holds if and only if $S_1 \cup S_2$ is a minimal spanning set of $V_1 + V_2$, which is equivalent to saying that $V_1$ and $V_2$ are weakly linearly independent. Thus, the equality holds if and only if the sum is direct.
\end{proof}

\newpage

\section{Exact Sequence}\index{Exact Sequence}

\begin{definition}[Exact and Exact Sequence]
    A sequence of linear maps between linear spaces over a field $\F$,
    \begin{center}
        \begin{tikzcd}
            \cdots \arrow[r, "f_{i-2}"] & V_{i-1} \arrow[r, "f_{i-1}"] & V_i \arrow[r, "f_i"] & V_{i+1} \arrow[r, "f_{i+1}"] & \cdots
        \end{tikzcd}
    \end{center}
    is said to be \emph{exact} at $V_i$ if 
    \[ \im{f_{i-1}} = \ker{f_i} \]
    i.e., the image of the map before $V_i$ is equal to the kernel of the map after $V_i$.

    The sequence is said to be an \emph{exact sequence} if it is exact at every $V_i$.
\end{definition}

\begin{example}
    For the following short exact sequence:
    \begin{center}
        \begin{tikzcd}
            0 \arrow[r] & V_1 \arrow[r, "i_1"] & V \arrow [r, "j_2"] & V_2 \arrow[r] & 0
        \end{tikzcd}
    \end{center}
    for which $V_2$ is assumed to have a minimal spanning set. Then 
    \begin{itemize}
        \item the exactness at $V_1$ implies that $\{0_{V_1}\} = \im{0} = \ker{i_1}$, thus $i_1$ is injective. 
        \item the exactness at $V$ implies that $\im{i_1} = \ker{j_2}$, thus $V_1 \cong \im{i_1} \subseteq V$. 
        \item the exactness at $V_2$ implies that $\im{j_2} = \ker{0} = V_2$, thus $j_2$ is surjective.
    \end{itemize}
    
    Somehow, we can draw a Euler diagram to illustrate the situation:
    \begin{center}
        \begin{tikzpicture}
            \filldraw[cyan!10] (0,0) ellipse (0.75cm and 1.5cm);
            \filldraw[cyan!10] (2.5,0) ellipse (0.5cm and 1cm);

            \filldraw[green!15] (-2.5,0) ellipse (0.375cm and 0.75cm);
            \draw[red, dashed, fill=green!15] (0,0) ellipse (0.5cm and 1cm);

            \draw (-2.5,0) ellipse (0.375cm and 0.75cm) node [below=1.75cm] {$V_1$};
            \draw (0,0) ellipse (0.75cm and 1.5cm) node [below=1.75cm] {$V$};
            \draw (2.5,0) ellipse (0.5cm and 1cm) node [below=1.75cm] {$V_2$};


            \draw[red, -Stealth] (-5,0) -- (-2.5,0) node [midway, above] {\scriptsize $\im{0}$};
            \draw[blue, -Stealth] (0,0) -- (-2.5,0) node [midway, above] {\scriptsize $\ker{i_1}$};
            \draw[red, -Stealth] (-2.5,0.75) -- (0,1) node [midway, above] {\scriptsize $\im{i_1}$};
            \draw[red, -Stealth] (-2.5,-0.75) -- (0,-1) node [midway, below] {\scriptsize $\im{i_1}$};
            \draw[blue, -Stealth] (2.5,0) -- (0,1) node [midway, above] {\scriptsize $\ker{j_2}$};
            \draw[blue, -Stealth] (2.5,0) -- (0,-1) node [midway, below] {\scriptsize $\ker{j_2}$};
            \draw[red, -Stealth] (0,1.5) -- (2.5,1) node [midway, above] {\scriptsize $\im{j_2}$};
            \draw[red, -Stealth] (0,-1.5) -- (2.5,-1) node [midway, below] {\scriptsize $\im{j_2}$};
            \draw[blue, -Stealth] (5,0) -- (2.5,1) node [midway, above] {\scriptsize $\ker{0}$};
            \draw[blue, -Stealth] (5,0) -- (2.5,-1) node [midway, below] {\scriptsize $\ker{0}$};


            \filldraw (-5,0) circle (1pt) node [below=1.75cm] {$0$};

            \filldraw (5,0) circle (1pt) node [below=1.75cm] {$0$};

            \filldraw (-2.5,0) circle (1pt) node [below] {\scriptsize $0_{V_1}$};
            \filldraw (0,0) circle (1pt) node [below] {\scriptsize $0_V$};
            \filldraw (2.5,0) circle (1pt) node [below] {\scriptsize $0_{V_2}$};
        \end{tikzpicture}
    \end{center}

    There are some facts about the short exact sequence:
    \begin{itemize}
        \item $j_2$ has a right inverse, i.e., there exists a linear map $i_2 : V_2 \to V$ such that $j_2 \circ i_2 = \id_{V_2}$.
        
        This is because $V_2$ has a minimal spanning set. Thus, for each element in the minimal spanning set of $V_2$, we can choose one representative in $V$ and define the map on the minimal spanning set. Then we can extend it to the whole space.
        \item $i_1$ has a left inverse, i.e., there exists a linear map $j_1 : V \to V_1$ such that $j_1 \circ i_1 = \id_{V_1}$. 
        
        This is because $i_1$ is injective. Thus, for each element in $V_1$, we can choose one representative in $V$ and define the map on the whole space by sending all other elements to zero.
    \end{itemize}

    The exact sequence becomes:
    \begin{center}
        \begin{tikzcd}
            0 \arrow[r] & V_1 \arrow[r, "i_1", bend left] \arrow[from=r, "j_1", dashed, bend left] & V \arrow [r, "j_2", bend left] \arrow[from=r, "i_2", dashed, bend left] & V_2 \arrow[r] & 0
        \end{tikzcd}
    \end{center}

\end{example}

There are some equalities about the composition of the maps in an exact sequence.
\begin{itemize}
    \item $j_1 \circ i_1 = \id_{V_1}$ because $j_1$ is a left inverse of $i_1$.
    \item $j_2 \circ i_2 = \id_{V_2}$ because $i_2$ is a right inverse of $j_2$.
    \item $j_2 \circ i_1 = 0$ because $\im{i_1} = \ker{j_2}$.
    \item $j_1 \circ i_2 = 0$ because $\im{i_2} = \ker{j_1}$.
    \item $i_1 \circ j_1 + i_2 \circ j_2 = \id_V$ because for all $v \in V$, we have $v = (v - i_2(j_2(v))) + i_2(j_2(v))$ where $v - i_2(j_2(v)) \in \im{i_1}$ and $i_2(j_2(v)) \in \im{i_2}$. Also, $\im{i_1} \cap \im{i_2} = \{0_V\}$.
\end{itemize}

There is actually one more fact about the short exact sequence.
\begin{proposition}
    $V \cong \im{i_1} \oplus \im{i_2}$.
\end{proposition}
\begin{proof}
    The meaning of $V \cong \im{i_1} \oplus \im{i_2}$ is that for any $x \in V$, it can be uniquely written as $x = x_1 + x_2$ where $x_i \in \im{i_i}$. Why? Suppose $x = x_1 + x_2 = x'_1 + x'_2$ where $x_i, x'_i \in \im{i_i}$. Then we have $(x_1 - x'_1) + (x_2 - x'_2) = 0$. Note that $x_1 - x'_1 \in \im{i_1}$ and $x_2 - x'_2 \in \im{i_2}$. Thus, we have $x_1 - x'_1 = 0$ and $x_2 - x'_2 = 0$. This shows the uniqueness.

    Note that all $V$, $V_1$ and $V_2$ are finite-dimensional. Then $V_2$ has a minimal spanning set, let say $S$. Then we construct $i_2 : s \mapsto i_2(s)$ where $i_2(s)$ is a choice of element from $j_2^{-1}(s) \neq \emptyset$ for each $s \in S$. Then we extend it to the whole space linearly. Thus, $i_2$ is injective. 

    Then we want to prove that $\im{i_1}$ and $\im{i_2}$ are weakly independent. Assume that $x_1 + x_2 = 0$ where $x_i \in \im{i_i}$. Then we have $j_2(x_1 + x_2) = j_2(x_1) + j_2(x_2) = 0$. Note that $j_2(x_1) = 0$ because $x_1 \in \im{i_1} = \ker{j_2}$, the exactness of $V$. Thus, we have $j_2(x_2) = 0$. However, $j_2$ is injective on $\im{i_2}$ because $j_2 \circ i_2 = \id_{V_2}$. Thus, we have $x_2 = 0$ and $x_1 = 0$. This shows that $\im{i_1}$ and $\im{i_2}$ are weakly independent.

    Finally, we want to prove that $\im{i_1} + \im{i_2} = V$. For all $x \in V$, we let $x_2 = i_2(j_2(x)) \in \im{i_2}$ and $x_1 = x - x_2$. Then we have to show that $x_1 \in \im{i_1} = \ker{j_2}$. Note that $j_2(x) = j_2(x_1) + j_2(x_2) = j_2(x_1) + j_2 \circ i_2(j_2(x)) = j_2(x_1) + j_2(x)$. This shows that $j_2(x_1) = 0$. Thus, $x_1 \in \ker{j_2} = \im{i_1}$. This shows that $\im{i_1} + \im{i_2} = V$.

    Actually $j_1$ is the projection from $\im{i_1} \oplus \im{i_2}$ to $\im{i_1}$ and it exists due to the uniqueness of the decomposition.
\end{proof}

The equalities can be summarized as follows:
\[
    j_m \circ i_n = \delta_{mn} \id_{V_n}, \quad \sum_{k=1}^{2} i_k \circ j_k = \id_V
\]

For the dimension of the spaces, we have:
\[
    \dim{V} = \dim{\im{i_1}} + \dim{\im{i_2}} = \dim{V_1} + \dim{V_2}
\]
As $V_1 \cong \im{i_1}$ and $V_2 \cong \im{i_2}$. $i_1$ and $i_2$ are injective and $V_k \to \im{i_k}$ are surjective. 

Also, we know that $\dim{V} \geq \dim{V_1}$ and $\dim{V} \geq \dim{V_2}$. Similarly, we have $\dim{W} \geq \dim{V}$ and $\dim{W} \geq \dim{\quotient{W}{V}}$, where $V$ is a subspace of $W$.

Consider Proposition \ref{prop:dimension_of_sum_of_subspaces}, more specifically, we have the following dimension formula:
\[
    \dim{(V_1 + V_2)} = \dim{V_1} + \dim{V_2} - \dim{(V_1 \cap V_2)}
\]

To proof the equality, we can consider the following short exact sequence:
\begin{center}
    \begin{tikzcd}
        0 \arrow[r] & V_1 \cap V_2 \arrow[r, "\iota"] & V_1 \arrow[r, "\pi"] & \quotient{(V_1 + V_2)}{V_2} \arrow[r] & 0
    \end{tikzcd}
\end{center}
Moreover, we have the isomorphism between $\quotient{(V_1 + V_2)}{V_2}$ and $\quotient{V_1}{(V_1 \cap V_2)}$.

\newpage

\epigraph{``No problem is difficult in linear algebra. All problems are trivial.''}{Guowu Meng}

\section{Fudan University Problems}

Students from Fudan University asked two hard problems but were completely cooked by Professor Guowu Meng

\subsection{The story behind the two problems}
``Well, linear algebra basically, no problem is difficult. All problems are trivial. 

``People don't believe me, because many years ago, more than 20 years ago, there were two exchange students from Fudan University, and when they came here, they carry solution manual with some sets of hard linear algebra problems. I told them `nothing is difficult'. 

``They don't believe me, so they dig out one hard problem from that solution book. Well, I told them I haven't seen this problem before, because when I was educated as a physicist engineer, I don't work on hard problems. I just deal with textbook. I don't read anything extract. I don't know but doesn't matter. Let me just write everything on board, and then pretty soon I figured out the answer. 

``Ok may be they say that I am lucky. Then the next day they came back with another problem. So again, I said I don't know how to do it but anyway doesn't matter. I put everything on board, then I draw some obvious facts in my mind about linear algebra. 

``I say no problems are difficult in linear algebra under the assumption that you know linear algebra inside-out, you know every facts about it. Usually you will say I have seen this type of problems before, and then step 1, step 2 step 3, but this is a very wrong way to do it. This is the way that AI does it, but we are human, we are smarter than machine. 

``When I do it, there are some keywords and each keywords remind me of some facts related to it, and keep doing this. Then I see a path from here to there.'' 

\begin{flushright}
    --- Guowu Meng on the lecture of September 19, 2025.
\end{flushright}

\subsection{Introduction to the two problems}

Later, we will get into the two problems that were asked by the students from Fudan University, but were completely cooked by Professor Guowu Meng. Before looking into the two problems, we need to introduce some basic terminologies in normal linear algebra.

Let $A$ be a $m \times n$ matrix. Then we consider the following diagram:
\begin{center}
    \begin{tikzcd}
        \ker{f} \subseteq \F^n \arrow[r, "f", "A" swap] & \F^m \supseteq \im{f}
    \end{tikzcd}
\end{center}

In normal linear algebra, we have four fundamental concepts: column space, null space, rank and nullity.
\begin{definition}[Column Space]
    The \emph{column space} of $A$, denoted by $\col{A}$, is defined as the image of the linear map $f : \F^n \to \F^m$ defined by $f(x) = Ax$, i.e.,
    \[
        \col{A} = \im{f} = \{ Ax \mid x \in \F^n \} \subseteq \F^m
    \]
\end{definition}

\newpage

\begin{definition}[Null Space]
    The \emph{null space} of $A$, denoted by $\nul{A}$, is defined as the kernel of the linear map $f : \F^n \to \F^m$ defined by $f(x) = Ax$, i.e.,
    \[
        \nul{A} = \ker{f} = \{ x \in \F^n \mid Ax = 0 \} \subseteq \F^n
    \]
\end{definition}

The alternative, or normal, definition of rank is as follows.
\begin{definition}[Rank]
    The \emph{rank} of $A$, denoted by $\rank{A}$, is defined as the dimension of the column space of $A$, i.e.,
    \[
        \rank{A} = \dim{\col{A}} = \dim{\im{f}} \leq m
    \]
\end{definition}

\begin{definition}[Nullity]
    The \emph{nullity} of $A$, denoted by $\nullity{A}$, is defined as the dimension of the null space of $A$, i.e.,
    \[
        \nullity{A} = \dim{\nul{A}} = \dim{\ker{f}} \leq n
    \]
\end{definition}

\subsection{Problem 1}

\begin{problem}
    Suppose we have three matrices $A$, $B$ and $C$. Then prove that 
    \[
        \rank{B} + \rank{ABC} \geq \rank{AB} + \rank{BC}
    \]
\end{problem}

\begin{proof}
    We consider the following diagram:
    \begin{center}
        \begin{tikzcd}[row sep=huge, column sep=large]
            0 \arrow[r] & 
            | [alias=D] | \col{BC} \arrow[r, hook, "C" description, red] \arrow[d, two heads, "A" description, blue] \arrow[rdr, start anchor=south, rounded corners, to path={
                -- (B.south)
                -- (\tikztotarget.north west) 
            }, orange, thick, -Stealth] \arrow[drr, start anchor=south, rounded corners, to path={
                -- (D.south west)
                |- (\tikztotarget.south west)
            }, violet, thick, -Stealth] & 
            | [alias=B] | \col{B} \arrow[r, two heads, "\pi_1" description, blue] \arrow[d, two heads, "A" description, blue] \arrow[dr, two heads, teal] & 
            \quotient{\col{B}}{\col{BC}} \arrow[r] \arrow[d, two heads, "\exists ! \phi" description, dashed, ocre] & 
            0 \\

            0 \arrow[r] & 
            | [alias=C] | \col{ABC} \arrow[r, hook, "C" description, red] & 
            | [alias=A] | \col{AB} \arrow[r, two heads, "\pi_2" description, blue] & 
            \quotient{\col{AB}}{\col{ABC}} \arrow[r] & 
            0
        \end{tikzcd}
    \end{center}

    We denote the injective map with red color and the surjective map with blue color. Notice that there is a surjective map from $\col{B}$ to $\quotient{\col{AB}}{\col{ABC}}$ due to the surjectivity of $A$ and $\pi_2$. Then we denote this surjective map with teal color.

    Then we have to consider whether the map from $\col{BC}$ to $\quotient{\col{AB}}{\col{ABC}}$ is zero. If the map is zero, then we can construct a unique surjective map $\phi$ from $\quotient{\col{B}}{\col{BC}}$ to $\quotient{\col{AB}}{\col{ABC}}$ due to the universal property of quotient space.

    Note that the map from $\col{BC}$ to $\quotient{\col{AB}}{\col{ABC}}$ is a zero map. As both upper and lower sequences are exact, we have the exactness at $\col{AB}$, i.e., $\im{C} = \ker{\pi_2}$. Thus the composite map $\pi_2 \circ C$ is a zero map. This shows that the map from $\col{BC}$ to $\quotient{\col{AB}}{\col{ABC}}$ is a zero map.

    Then we can construct a unique surjective map $\phi$ from $\quotient{\col{B}}{\col{BC}}$ to $\quotient{\col{AB}}{\col{ABC}}$ due to the universal property of quotient space.

    Finally, we consider the dimensions of the spaces. Note that $\phi$ is surjective, thus we have
    \[
        \begin{split}
            \dim{\quotient{\col{B}}{\col{BC}}} & \geq \dim{\quotient{\col{AB}}{\col{ABC}}} \\
            \dim{\col{B}} - \dim{\col{BC}} & \geq \dim{\col{AB}} - \dim{\col{ABC}} \\
            \dim{\col{B}} + \dim{\col{ABC}} & \geq \dim{\col{AB}} + \dim{\col{BC}} \\
            \rank{B} + \rank{ABC} & \geq \rank{AB} + \rank{BC}
        \end{split}
    \]
\end{proof}

\subsection{Problem 2}

\begin{problem}
    If $A$ is a $n \times n$ matrix then prove that
    \[
        \rank{A^n} = \rank{A^{n+1}}
    \]
\end{problem}
\begin{proof}
    We consider the following diagram:
    \begin{center}
        \begin{tikzcd}[column sep=normal]
            I_n \arrow[r, "A"] & \im{A} \arrow[r, "A"] & \im{A^2} \arrow[r, "A"] & \cdots \arrow[r, "A"] & \im{A^n} \arrow[r, "A"] & \cdots
        \end{tikzcd}
    \end{center}
    As $I_n \supseteq \im{A} \supseteq \im{A^2} \supseteq \cdots$, we know that 
    \[
        n = \dim{I_n} \geq r(A) \geq r(A^2) \geq \cdots
    \]

    As the space is finite-dimensional, the sequence will eventually become constant. That means there exists a $k$ such that for all $j \geq k$, we have $r(A^j) = r(A^{j + 1})$. 

    There are two possibilities: either $k \leq n$ or $k > n$. If $k \leq n$, the equality works properly, as for every $j \geq k$, including $j = n$, such that $r(A^j) = r(A^{j + 1})$ implies $r(A^n) = r(A^{n + 1})$. 

    For $k > n$, consider the strict inequality, we know that each time the dimension must drop at least 1. Without the loss of generality, we may consider the sequence of dimension as $n, n - 1, n - 2, \cdots, 1, 0$. This involves $n$ times. So it is impossible to have $k > n$. 
\end{proof}

\newpage

\section{Rank-Nullity Theorem}

Actually, using short exact sequence, we can easily prove the rank-nullity theorem.
\begin{theorem}[Rank-Nullity Theorem]
    For a linear map $f : V \to W$ between finite dimensional linear spaces over $\F$, we have
    \[
        \rank{f} + \nullity{f} = \dim{V}
    \]
\end{theorem}
\begin{proof}
    Consider the following short exact sequence:
    \begin{center}
        \begin{tikzcd}[column sep=huge]
            0 \arrow[r] & \ker{f} \arrow[r, "\iota", hook] & V \arrow[r, "f", two heads] & \im{f} \arrow[r] & 0
        \end{tikzcd}
    \end{center}
    Then we have $V \cong \ker{f} \oplus \im{f}$. Thus, we have $\dim{V} = \dim{\ker{f}} + \dim{\im{f}}$. This shows that $\rank{f} + \nullity{f} = \dim{V}$.
\end{proof}

Moreover, we have the following corollary.
\begin{corollary}
    For a linear map $f : V \to W$ between finite dimensional linear spaces over $\F$, we have
    \[
        \dim{W} = \rank{f} + \dim{\coker{f}}
    \]
\end{corollary}
\begin{proof}
    Consider the following short exact sequence:
    \begin{center}
        \begin{tikzcd}[column sep=huge]
            0 \arrow[r] & \im{f} \arrow[r, "\iota", hook] & W \arrow[r, "\pi", two heads] & \coker{f} \arrow[r] & 0
        \end{tikzcd}
    \end{center}
    Then we have $W \cong \im{f} \oplus \coker{f}$. Thus, we have $\dim{W} = \dim{\im{f}} + \dim{\coker{f}}$. This shows that $\dim{W} = \rank{f} + \dim{\coker{f}}$.
\end{proof}

\begin{corollary}
    For a linear map $f : V \to W$ between finite dimensional linear spaces over $\F$, we have
    \[
        \dim{V} = \nullity{f} + \dim{\coim{f}}
    \]
\end{corollary}
\begin{proof}
    Consider the following short exact sequence:
    \begin{center}
        \begin{tikzcd}[column sep=huge]
            0 \arrow[r] & \ker{f} \arrow[r, "\iota", hook] & V \arrow[r, "\pi", two heads] & \coim{f} \arrow[r] & 0
        \end{tikzcd}
    \end{center}
    Then we have $V \cong \ker{f} \oplus \coim{f}$. Thus, we have $\dim{V} = \dim{\ker{f}} + \dim{\coim{f}}$. This shows that $\dim{V} = \nullity{f} + \dim{\coim{f}}$.
\end{proof}

Moreover, we have the following properties for rank:
\begin{enumerate}
    \item The rank of a matrix is invariant under elementary row and column operations.
    \item $\rank{A + B} \leq \rank{A} + \rank{B}$
    \item $\rank{AB} \leq \rank{A}$ and $\rank{AB} \leq \rank{B}$
\end{enumerate}

\newpage

\section{Canonical Form of Linear Map}

First, let $f : V_1 \to V_2$ be a linear map between finite dimensional linear spaces over $\F$. Recall that $\ker{f} = f^{-1} (0)$, $\im{f} = \{f(v_1) \mid v_1 \in V_1\}$, $\coim{f} = \quotient{V_1}{\ker{f}}$ and $\coker{f} = \quotient{V_2}{\im{f}}$. We have the following commutative diagram:

\begin{center}
    \begin{tikzcd}
        0 \arrow[d] & 0 \\
        \ker{f} \arrow[d, hook] & \coker{f} \arrow[u] \arrow[d, bend left, "s_2", red] \\
        V_1 \arrow[d, two heads] \arrow[r, "f"] \arrow[dr, ocre, "\bar{f}", two heads] & V_2 \arrow[u, two heads] \\
        \coim{f} \arrow[d] \arrow[r, "\exists ! f'" swap, hook, two heads] \arrow[u, bend left, "s_1", red] & \im{f} \arrow[u, hook] \\
        0 & 0 \arrow[u]
    \end{tikzcd}
\end{center}

Here, each column is an exact sequence, and the square in the middle is commutative, as the lower left triangle and upper right triangle are commutative.

Moreover, the $f'$, the universal property for quotient map, is a linear equivalence. It is injective due to the trivial $\ker{f'}$. 

$s_1$ and $s_2$ are the \emph{right inverses} or called \emph{sections}.

With respect to the decomposition of $V_1$ and $V_2$ into subspaces, i.e., $V_1 = \im{s_1} \oplus \ker{f}$ and $V_2 = \im{f} \oplus \im{s_2}$, the linear map $f$ is decomposed as follows:
\begin{center}
    \begin{tikzpicture}
        \matrix (m) [matrix of math nodes, column sep=4.8em, minimum width=2em]
        {\im{s_1} \oplus \ker{f} & \im{f} \oplus \im{s_2} \\};
        \path[->] (m-1-1) edge node[above] {$f = \begin{bmatrix}
            \tilde{f} & 0 \\
            0 & 0
        \end{bmatrix}$} (m-1-2);
    \end{tikzpicture}
\end{center}
where $\tilde{f} : \im{s_1} \to \im{f}$ is a linear equivalence, as there are linear equivalences $f' : \coim{f} \to \im{f}$ and $s_1 : \coim{f} \to \im{s_1}$. Then the graph below commutes:

\begin{center}
    \begin{tikzcd}
        \im{s_1} \arrow[r, hook, two heads] & \im{f} \\
        \coim{f} \arrow[u, hook, two heads, "s_1"] \arrow[ur, hook, two heads, "f'" swap]
    \end{tikzcd}
\end{center}
\begin{remark}
    The choice of $s_1$ and $s_2$ is not unique, so the decomposition of $V_1$ and $V_2$, and hence $f$, is not unique.
\end{remark}

The matrix $\begin{bmatrix}
    \tilde{f} & 0 \\
    0 & 0
\end{bmatrix}$ is the canonical form of the linear map. Just as the canonical form of a matrix, it reveals the essential structure of the linear map. However, the rank of $\tilde{f}$ is unique, which is equal to $\rank{f} = \dim{\im{f}}$.

\begin{center}
    \begin{tikzpicture}
        \matrix (m) [matrix of math nodes, column sep=4.8em, minimum width=2em]
        {\F^r \oplus \F^{n - r} & \F^r \oplus \F^{n - r} \\};
        \path[->] (m-1-1) edge node[above] {$\begin{bmatrix}
            I_r & 0 \\
            0 & 0
        \end{bmatrix}$} (m-1-2);
    \end{tikzpicture}
\end{center}

Moreover, from the diagram of two exact sequences, we can see that $f$ can be decomposed into two linear maps: $f = \iota \circ \bar{f}$, where $\bar{f} : V_1 \to \coim{f}$ is a surjective map and $\iota : \coim{f} \to V_2$ is an injective map. Note that the decomposition is not unique, as we can choose the path from $V_1$ to $\coim{f}$ then to $V_2$.

\newpage

\section{Free Vector Space}

Let $X$ be a set and $\delta_X = \{ \delta_x \mid x \in X \}$. Here $\delta_x : X \to \F$ is the $\delta$-function at $x$. 

\begin{proposition}
    $\delta_X$ is a linearly independent set of $\F[[X]] =$ the linear space of $\F$-valued functions on $X$.
\end{proposition}

\begin{proposition}
    $\span{\delta_X} = \F[X]$
\end{proposition}

Then $\delta_X$ is a minimal spanning set for $\F[X]$.

\begin{proposition}
    There is a natural set isomorphism $X \to \delta_X$ which maps $x$ to $\delta_x$.
\end{proposition}

Then we have an injective set map $\iota : X \equiv \delta_X \to \F[X]$ which maps $x$ to $\delta_x$. This is a set mapping to a linear space.

Among all set maps from $X$ to a linear space over $\F$, the set map $\iota : X \to \F[X]$ is universal in the following sense:
\begin{center}
    \begin{tikzcd}
        X \arrow[r, "\forall \phi"] \arrow[d, "\iota"] & Z \\
        \F[X] \arrow[ur, "\exists ! \tilde{\phi}" swap]
    \end{tikzcd}
\end{center}
For any set map $\phi : X \to Z$, there exists a unique linear map $\tilde{\phi} : \F[X] \to Z$ such that $\tilde{\phi} \circ \iota = \phi$.

\begin{proof}
    Assume the existence of such $\tilde{\phi}$, then $\tilde{\phi} \circ \iota (x) = \phi(x)$ for all $x \in X$, i.e., $\tilde{\phi} (\delta_x) = \phi(x)$ for all $x \in X$. As $\{ \delta_x \mid x \in X \}$ is a minimal spanning set for $\F[X]$, $\tilde{\phi}$ must be the linear map such that $\tilde{\phi} (\delta_x) = \phi(x)$, thus unique. Existence of $\tilde{\phi}$ is also proved.
\end{proof}

Via the natural identification of $\delta_X \equiv X$ ($\delta_x \equiv x$), an element $\sum \alpha_x \delta_x \in F[X]$, where the sum is finite and $\alpha_x \in \F$, is naturally identified with $\sum \alpha_x x$, which is called a \emph{formal linear combination} of elements in $X$. Hereafter, we always use this natural identification, so $\F[X]$ is now defined as \emph{the set of formal linear combinations of elements in the set $X$}. Then $\iota : X \to \F[X]$ is just the inclusion map : $x \mapsto x$.

The universal map is unique in the following sense: suppose that $\iota' : X \to \F[X]'$ is another inclusion map, then there is a unique linear equivalence $\lambda$ in the commutative triangle:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        & X \arrow[dl, "\iota" swap] \arrow[dr, "\iota'"] \\
        \F[X] \arrow[rr, "\lambda" swap, hook, two heads] & & \F[X]'
    \end{tikzcd}
\end{center}

This can be seen from the following diagram:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        & & X \arrow[dll, "\iota'" swap] \arrow[dl, "\iota"] \arrow[dr, "\iota'" swap] \arrow[drr, "\iota"] \\
        \F[X]' \arrow[r, "\mu" swap, dashed] \arrow[rrr, bend right, "1"] & \F[X] \arrow[rr, "\lambda" swap, dashed] \arrow[rrr, bend right, "1"] & & \F[X]' \arrow[r, "\mu" swap, dashed] & \F[X]
    \end{tikzcd}
\end{center}
$\lambda$ exists because $\iota$ is universal, and $\mu$ exists because $\iota'$ is universal. $\lambda \mu = 1$ because $\iota'$ is universal, same for $\mu \lambda = 1$. Then $\lambda$ is isomorphism.

The universal property implies an assignment of a linear map $\F[f] : \F[X] \to \F[Y]$ to any set map $f : X \to Y$. Indeed,
\begin{center}
    \begin{tikzcd}
        X \arrow[r, "f"] \arrow[d, "\iota" swap, hook] \arrow[dr, "\iota f"] & Y \arrow[d, "\iota", hook] \\
        \F[X] \arrow[r, dashed, "{\exists ! \F[f]}" swap] & \F[Y]
    \end{tikzcd}
\end{center}

Moreover, $\F[1_X] = 1_{\F[X]}$ or simply $\F[1] = 1$ for all $X$, and $\F[fg] = \F[f] \F[g]$ for all $f : Y \to Z$ and $g : X \to Y$.

%----------------------------------------------------------------------------------------
\chapter{Introduction to Category Theory}

\epigraph{``In linear algebra, all the proofs should be straight-forward. There is no trick. If you think it's very hard, there is something wrong''}{Guowu Meng}

\section{Categories and Functors}

The collection of set maps is denoted as $\Set$ and the collection of linear maps over $\F$ is denoted as $\Vect_{\F}$. There is a diagram below:
\begin{center}
    \begin{tikzcd}
        \Set \arrow[d, "{\F[-]}"] \\
        \Vect_{\F}
    \end{tikzcd}
\end{center}
where $\F[-]$ sends set map $f : X \to Y$ to a linear map $\F[f] : \F[X] \to \F[Y]$. 

$\F[-]$ is an example of functors.

Monoid homomorphisms are another example of functors: in particular group homomorphisms
\begin{center}
    \begin{tikzcd}
        M_1 \arrow[d, "\phi"] \\
        M_2
    \end{tikzcd}
\end{center}

An element $a \in M_1$ is viewed as an arrow, or morphism, that sends $*$ to $*$, i.e., $a : * \to *$. Then $ab$ is viewed as the composition of arrows:
\begin{center}
    \begin{tikzcd}
        * \arrow[r, "b"] \arrow[rr, "ab", bend right] & * \arrow[r, "a"] & *
    \end{tikzcd}
\end{center}

Recall that a monoid $M$ is a set, which is called a small collection of objects, together with a binary operation, which is also called composition, on $M$ with both the associtivity law and identity law satisfied. 

By relaxing the condition on binary operation, allowing the composition being partially defined, we end up with the notion of \emph{small category}.

Being partially defined means that the composition may not be always defined. For example, take $f : X \to Y$ and $g : W \to Z$, then $gf$ is not defined. But for normal, $f : X \to Y$ and $g : Y \to Z$, then $gf$ is defined. In monoid, as we may suggest there is only one element $*$, then the composition is always defined.

An example of small category: The collection of all matrices over $\F$. We may consider any $m \times n$ matrix as an arrow that sends $n$ to $m$: $A : n \to m$. If we have a $k \times m$ matrix $B$ that sends $m$ to $k$, then we have the composition $BA : n \to k$. Note that $I_n : n \to n$ is the identity, which is not unique, there can be $I_m$ and $I_k$. We have 
\begin{center}
    \begin{tikzcd}
        n \arrow[loop left, "1_n"] \arrow[r, "A"] & m \arrow[loop right, "1_m"] \arrow[d, "B"] \\
        & k
    \end{tikzcd}
\end{center}
Note that $A 1_n = A = 1_m A$ and $B 1_m = B$.
\begin{remark}
    The identity elements are not unique unlike the case of monoid.
\end{remark}

The following shows the associativity law:
\begin{center}
    \begin{tikzcd}
        n \arrow[r, "C"] \arrow[rr, bend right, "BC"] \arrow[rrr, bend right, "A(BC)" swap] \arrow[rrr, bend left, "(AB)C"] & m \arrow[r, "B"] \arrow[rr, bend left, "AB" swap] & k \arrow[r, "A"] & l
    \end{tikzcd}
\end{center}

Hence, the set of all matrices form a small category.

Consider the set of all invertible matrices over $\F$, it is also a small category, in fact, it is a \emph{groupoid}. Groupoid is defined as a small category such that every morphism is invertible.

\begin{center}
    \begin{tikzcd}[column sep=3.5em, row sep=huge, math mode=false]
        & Categories \\
        Monoids \arrow[r] & Small Categories \arrow[u] \\
        Groups \arrow[r] \arrow[u] & Groupoids \arrow[u]
    \end{tikzcd}
\end{center}
The graph above shows the relation, the arrows show the subsets relation. The arrow head is the larger set and arrow tail is the subset.

\newpage

\section{Small Categories}

\begin{definition}[Small Categories]
    A small category is a set $\C$ together with a subset $\C_0$ of $\C$, two surjective maps $s, t : \C \to \C_0$ and a composition map $\C \times_{(s, t)} \C \to \C$ that sends $(f, g)$ to $fg$ which satisfies the identity law and associativity law.
\end{definition}

Here $\C \times_{s, t} \C$ is defined as the pullback of the diagram below:
\begin{center}
    \begin{tikzcd}
        \C \times_{s, t} \C \arrow[r, "p_1"] \arrow[d, "p_2"] \arrow[dr, phantom, "\ulcorner", very near start] & \C \arrow[d, "t"] \\
        \C \arrow[r, "s"] & \C_0
    \end{tikzcd}
\end{center}
where the set $\C \times_{s, t} \C = \{ (x, y) \in \C \times \C \mid s(x) = t(y) \}$. Intuitively, the pullback is to filter out the mappings that can do composition, such as $f, g \in \C \times_{(s, t)} \C$ where \begin{tikzcd}[cramped, column sep=normal] A \arrow[r, "f"] & B \arrow[r, "g"] & C \end{tikzcd}.

The $s$ and $t$ are called the \emph{source map} and \emph{target map} respectively. We can picture the composition graphically as follows:
\begin{center}
    \begin{tikzcd}[row sep=subtext, column sep=tiny]
        * & * \arrow[l, "f" swap] & * & * \arrow[l, "g" swap] \\
        t(f) & s(f) \arrow[r, equal] & t(g) & s(g)
    \end{tikzcd}
    \qquad
    \begin{tikzcd}[row sep=subtext, column sep=tiny]
        * & * \arrow[l, "fg" swap] \\
        t(f) & s(g)
    \end{tikzcd}
\end{center}
The left diagram is the equivalent to the right one.

We may draw the identity law this way:
\begin{center}
    \begin{tikzcd}[row sep=subtext, column sep=tiny]
        * \arrow[loop left, "1_{t(f)}"] & * \arrow[l, "f" swap] \\
        t(f) & s(f)
    \end{tikzcd}
    \qquad
    \begin{tikzcd}[row sep=subtext, column sep=tiny]
        * & * \arrow[l, "f" swap] \\
        t(f) & s(f)
    \end{tikzcd}
    \qquad
    \begin{tikzcd}[row sep=subtext, column sep=tiny]
        * & * \arrow[l, "f" swap] \arrow[loop right, "1_{s(f)}"] \\
        t(f) & s(f)
    \end{tikzcd}
\end{center}
The three diagrams are equivalent.

We may draw the associativity law this way:
\begin{center}
    \begin{tikzcd}[swap]
        * & * \arrow[l, "f"] & * \arrow[l, "g"] \arrow[ll, bend left, "fg", blue] & * \arrow[l, "h"] \arrow[lll, bend left, "(fg)h", blue] \arrow[ll, bend right, "gh" swap, red] \arrow[lll, bend right, "f(gh)" swap, red]
    \end{tikzcd}
\end{center}

\begin{example}
    In the small category of matrices over $\F$, we have 
    \[
        \begin{split}
            \C &= \{ \M{m \times n}{\F} \mid m, n \in \mathbb{N} \} \\
            \C_0 &= \{ I_n \mid n \in \mathbb{N} \} \equiv \mathbb{N}
        \end{split}
    \]
    If $A \in \C$ is an $m \times n$ matrix, then $s(A) = I_n \equiv n$ and $t(A) = I_m \equiv m$. We can draw $A$ as follows:
    \begin{center}
        \begin{tikzcd}[row sep=subtext, swap]
            * & * \arrow[l, "A"] \\
            m & n
        \end{tikzcd}
    \end{center}
    Note that $(A, B) \in \C \times_{s, t} \C$, where the composition of $A$ and $B$ defined as the matrix multiplication $AB$, means for some positive integer $m, n$ and $k$:
    \begin{center}
        \begin{tikzcd}[row sep=subtext, swap]
            * & * \arrow[l, "A"] & * \arrow[l, "B"] \\
            m & n & k
        \end{tikzcd}
    \end{center}
\end{example}
\begin{remark}
    Elements in $\C$ are \emph{morphisms} or \emph{arrows}, and elements in $\C_0$ are \emph{identity morphisms}. A morphism $f$ is viewed as an arrow from $s(f) \in C_0$ to $t(f) \in C_0$, i.e., $f : s(f) \to t(f)$. 
    An identity morphism is drawn in the following way with $X$ being called the \emph{object}:
    \begin{center}
        \begin{tikzcd}[row sep=subtext, swap]
            * & * \arrow[l, "1_X"] \\
            X & X
        \end{tikzcd}
    \end{center}
    In the last example, $I_n$ is the identity morphism at $n$. So $\C_0$ is also called the set of objects. Then a morphism $f$ is viewed as an arrow from object $X \equiv 1_X = s(f)$ to object $Y \equiv 1_Y = t(f)$, i.e., $f : X \to Y$.
\end{remark}

So, normally, we denote a small category as $\C$ and its set of objects as $\C_0$.
\begin{remark}
    The set of morphisms from object $X$ to object $Y$ is denoted by $\Mor(X, Y)$. In the last example, $\Mor(m, n) = \M{m \times n}{\F}$, the set of all $m \times n$ matrices over $\F$. Note that $1_X \in \Mor(X, X)$, so $\Mor(X, X) \neq \emptyset$ for all $X \in \C_0$.
\end{remark}

Then $\C$ is the disjoint union of all $\Mor(X, Y)$ for all pairs of objects $(X, Y)$:
\[
    \C = \bigsqcup_{X, Y \in \C_0} \Mor(X, Y)
\]
\begin{remark}
    The composition can be written as follows:
    \begin{center}
        \begin{tikzcd}[row sep=subtext]
            \Mor(Y, Z) \times \Mor(X, Y) \arrow[r] & \Mor(X, Z) \\
            (Z \xlongleftarrow{f} Y, X \xlongleftarrow{g} Y) \arrow[r, mapsto] & X \xlongleftarrow{fg} Z
        \end{tikzcd}
    \end{center}
\end{remark}

Then the following is the second definition of small category, which is also the normal definition of a small category.

\begin{definition}[Small Categories]
    A small category $\C$ is a collection of the following data:
    \begin{enumerate}
        \item A set of objects $\C_0$;
        \item A set of morphisms $\Mor(X, Y)$ for each pair of objects $(X, Y)$;
        \item A composition map $\Mor(Y, Z) \times \Mor(X, Y) \to \Mor(X, Z)$ that sends $(f, g)$ to $fg$ for each triple of objects $(X, Y, Z)$;
        \item An identity morphism $1_X \in \Mor(X, X)$ for each object $X$;
    \end{enumerate}
    Moreover, these data satisfies the following conditions:
    \begin{enumerate}[label=(\alph*)]
        \item (Identity Law) For all $f \in \Mor(X, Y)$, we have $f 1_X = f = 1_Y f$;
        \item (Associativity Law) For all appropriate morphisms $f, g, h$, we have $(fg)h = f(gh)$.
    \end{enumerate}
\end{definition}

For a small category $\C$, the set of objects is denoted by $\Ob{\C}$ and the set of morphisms for any pair of objects $(X, Y)$ is denoted by $\Mor(X, Y)$, $\Mor_{\C} (X, Y)$, Hom$_{\C}(X, Y)$ or simply $\C(X, Y)$.

If we allow $\Ob{\C}$ and $\Mor_{\C}(X, Y)$ for any pair of objects $(X, Y)$ being a \emph{class}, (a larger collection than set), we end up with the definition of \emph{category}.

We say a morphism is \emph{isomorphic} or \emph{invertible} if it has a two-sided inverse. A category such that every morphism is isomorphic is called a \emph{groupoid}.

\begin{example}
    The collection of all sets and set maps, denoted by $\Set$, is a category.
\end{example}

\begin{example}
    The collection of all linear spaces over $\F$ and linear maps, denoted by $\Vect_{\F}$, is a category.
\end{example}

\begin{example}
    If $\C$ and $\D$ are two categories, then we have the product category $\C \times \D$ with objects $(X, Y)$ and morphisms $(f, g)$, where $X \in \Ob{\C}$, $Y \in \Ob{\D}$, $f \in \Mor_{\C}(X, X')$ and $g \in \Mor_{\D}(Y, Y')$.
\end{example}

\begin{example}
    The category of set maps between finite sets, denoted by $\boldsymbol{\mathsf{FinSet}}$, is a subcategory of $\Set$.
\end{example}

\begin{example}
    Fix an object $X$ in a category $\C$. Then the collection of all morphisms with source $X$, denoted by $\C(X, -)$, is a new category:
    \begin{itemize}
        \item Objects: all morphisms $f : X \to Y$ in $\C$ for all $Y \in \Ob{\C}$;
        \item Morphisms: commutative triangles in $\C$:
        \begin{center}
            \begin{tikzcd}
                & X \arrow[dl, "f" swap] \arrow[dr, "f'"] \\
                Y \arrow[rr, "g"] & & Y'
            \end{tikzcd}
        \end{center}
        \item The identity morphism at object $f : X \to Y$ is the commutative triangle in $\C$:
        \begin{center}
            \begin{tikzcd}
                & X \arrow[dl, "f" swap] \arrow[dr, "f"] \\
                Y \arrow[rr, "1_Y"] & & Y
            \end{tikzcd}
        \end{center}
    \end{itemize} 
\end{example}

\begin{example}
    Let $V$ be a subspace of the linear space $W$ over $\F$. Then we have a category:
    \begin{itemize}
        \item Objects: all morphisms $f : W \to Z$ in $\Vect_{\F}$ such that $f\mid_V = 0$;
        \item Morphisms: commutative triangles in $\Vect_{\F}$:
        \begin{center}
            \begin{tikzcd}
                & W \arrow[dl, "f_1" swap] \arrow[dr, "f_2"] \\
                Z_1 \arrow[rr, "g"] & & Z_2
            \end{tikzcd}
        \end{center}
    \end{itemize}
\end{example}

\begin{definition}[Terminal Object and Initial Object]
    Let $\C$ be a category. An object $T \in \Ob{\C}$ is called a \emph{terminal object} if for all object $X$, there exists a unique morphism from $X$ to $T$, i.e., $|\C(X, T)| = 1$. An object $I \in \Ob{\C}$ is called an \emph{initial object} if for all object $X$, there exists a unique morphism from $I$ to $X$, i.e., $|\C(I, X)| = 1$.
\end{definition}

\begin{corollary}
    A terminal object or an initial object is unique up to isomorphism.
\end{corollary}

\begin{example}
    In the last example of category, the quotient map $\pi : W \to \quotient{W}{V}$ is an initial object and the zero map $0 : W \to 0$ is a terminal object.
\end{example}

\begin{example}
    In $\Set$, any singleton set is a terminal object, and the empty set is an initial object.
\end{example}

\begin{example}
    In $\Vect_{\F}$, the zero vector space is both a terminal object and an initial object.
\end{example}

\newpage

\section{Products and Coproducts}

\subsection{Products}

\begin{definition}[Products]
    Let $\C$ be a category and $X, Y \in \Ob{\C}$. The \emph{product} of $X$ and $Y$ is an object $X \prod Y$ together with two morphisms $\pi_X : X \prod Y \to X$ and $\pi_Y : X \prod Y \to Y$ such that for any object $Z$ and any two morphisms $f_X : Z \to X$ and $f_Y : Z \to Y$, there exists a unique morphism $f : Z \to X \prod Y$ such that the following diagram commutes:
    \begin{center}
        \begin{tikzcd}
            & Z \arrow[dl, "f_X" swap] \arrow[d, dashed, "\exists ! f"] \arrow[dr, "f_Y"] \\
            X & X \prod Y \arrow[l, "\pi_X"] \arrow[r, "\pi_Y" swap] & Y
        \end{tikzcd}
    \end{center}
\end{definition}
\begin{remark}
    The product is unique up to isomorphism if it exists.
\end{remark}

\begin{corollary}
    Let $\C$ be a category and $X, Y \in \Ob{\C}$. Consider the following new category:
    \begin{itemize}
        \item Objects: all morphisms \begin{tikzcd}[cramped, swap, column sep=normal] X \arrow[from=r, "f_X"] & Z & Y \arrow[from=l, "f_Y" swap] \end{tikzcd} in $\C$ for all $Z \in \Ob{\C}$;
        \item Morphisms: commutative diagrams in $\C$:
    \end{itemize}
    \begin{center}
        \begin{tikzcd}[row sep=normal]
            & Z \arrow[dd, "f"] \arrow[dl, "f_X" swap] \arrow[dr, "f_Y"] \\
            X & & Y \\
            & Z' \arrow[ul, "f_X'" swap] \arrow[ur, "f_Y'"]
        \end{tikzcd}
    \end{center}
    Then the product of $X$ and $Y$ is a terminal object in this new category.
\end{corollary}

\begin{example}
    In $\Set$, the product of two sets $X$ and $Y$ is the Cartesian product $X \times Y = \{ (x, y) \mid x \in X, y \in Y \}$ with the projection maps $\pi_X(x, y) = x$ and $\pi_Y(x, y) = y$. Then with $f(z) = (f_X(z), f_Y(z))$ for all $z \in Z$, we have the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            & Z \arrow[dl, "f_X" swap] \arrow[d, dashed, "\exists ! f"] \arrow[dr, "f_Y"] \\
            X & X \times Y \arrow[l, "\pi_X"] \arrow[r, "\pi_Y" swap] & Y
        \end{tikzcd}
    \end{center}
    
\end{example}

\begin{example}
    In $\Vect_{\F}$, the product of two linear spaces $V_1$ and $V_2$ over $\F$ is the direct product $V_1 \times V_2 = \{ (v_1, v_2) \mid v_1 \in V_1, v_2 \in V_2 \}$ with the projection maps $\pi_{V_1}(v_1, v_2) = v_1$ and $\pi_{V_2}(v_1, v_2) = v_2$. Then with $f(z) = (f_{V_1}(z), f_{V_2}(z))$ for all $z \in Z$, we have the following commutative diagram:
    \begin{center}
        \begin{tikzcd}
            & Z \arrow[dl, "f_{V_1}" swap] \arrow[d, dashed, "\exists ! f"] \arrow[dr, "f_{V_2}"] \\
            V_1 & V_1 \times V_2 \arrow[l, "\pi_{V_1}"] \arrow[r, "\pi_{V_2}" swap] & V_2
        \end{tikzcd}
    \end{center}
\end{example}

\subsection{Coproducts}

\begin{definition}[Coproducts]
    Let $\C$ be a category and $X, Y \in \Ob{\C}$. The \emph{coproduct} of $X$ and $Y$ is an object $X \coprod Y$ together with two morphisms $\iota_X : X \to X \coprod Y$ and $\iota_Y : Y \to X \coprod Y$ such that for any object $Z$ and any two morphisms $f_X : X \to Z$ and $f_Y : Y \to Z$, there exists a unique morphism $f : X \coprod Y \to Z$ such that the following diagram commutes:
    \begin{center}
        \begin{tikzcd}
            X \arrow[r, "\iota_X"] \arrow[dr, "f_X" swap] & X \coprod Y \arrow[d, dashed, "\exists ! f"] & Y \arrow[l, "\iota_Y" swap] \arrow[dl, "f_Y"] \\
            & Z
        \end{tikzcd}
    \end{center}
\end{definition}
\begin{remark}
    The coproduct is unique up to isomorphism if it exists.
\end{remark}

\begin{corollary}
    Let $\C$ be a category and $X, Y \in \Ob{\C}$. The \emph{coproduct} of $X$ and $Y$ is the initial object in the new category:
    \begin{itemize}
        \item Objects: all morphisms \begin{tikzcd}[cramped, column sep=normal] X \arrow[r, "f_X"] & Z & Y \arrow[l, "f_Y" swap] \end{tikzcd} in $\C$ for all $Z \in \Ob{\C}$;
        \item Morphisms: commutative diagrams in $\C$:
    \end{itemize}
    \begin{center}
        \begin{tikzcd}[row sep=normal]
            & Z \\
            X \arrow[ur, "f_X"] \arrow[dr, "f_X'" swap] & & Y \arrow[ul, "f_Y" swap] \arrow[dl, "f_Y'"] \\
            & Z' \arrow[uu, "f"]
        \end{tikzcd}
    \end{center}
\end{corollary}

\begin{example}
    In $\Set$, the coproduct of two sets $X$ and $Y$ is the disjoint union $X \sqcup Y = \{ (x, 1) \mid x \in X \} \cup \{ (y, 2) \mid y \in Y \}$.
\end{example}

\begin{example}
    In $\Vect_{\F}$, the coproduct of two linear spaces $V_1$ and $V_2$ over $\F$ is the direct sum $V_1 \oplus V_2 = \{ (v_1, v_2) \mid v_1 \in V_1, v_2 \in V_2 \}$.
\end{example}

\subsection{Biproducts}

In $\Vect_{\F}$, the product and coproduct are the same, i.e., $V_1 \times V_2 \cong V_1 \oplus V_2$. Then we will say the \emph{biproduct} of $V_1$ and $V_2$ and denote it by $V_1 \oplus V_2$. The following diagram commutes:
\begin{center}
    \begin{tikzcd}[row sep=normal]
        & V_1 \times V_2 \arrow[dl, "\pi_{V_1}" swap] \arrow[dr, "\pi_{V_2}"] \\
        V_1 \arrow[dr, "\iota_{V_1}"] && V_2 \arrow[dl, "\iota_{V_2}" swap] \\
        & V_1 \oplus V_2 \arrow[uu, equal]
    \end{tikzcd}
\end{center}

\begin{definition}[Biproducts]
    The \emph{biproduct} of two objects $X$ and $Y$ in a category $\C$ is an object $X \oplus Y$ that is both the product and coproduct of $X$ and $Y$.
\end{definition}
\begin{remark}
    The biproduct exists if and only if the product and coproduct exist and are isomorphic, or if the initial object and the terminal object exist and are isomorphic.
\end{remark}

\begin{example}
    In $\Vect_{\F}$, the zero vector space is both a terminal object and an initial object, so the biproduct exists.
\end{example}

However, in $\Set$, the empty set is an initial object but the terminal object is any singleton set, so the biproduct does not exist.

\subsection{Products and Coproducts of a Family of Objects}

In general, we may have the product or coproduct of a family of objects.

Let $\C$ be a category and $\{ X_{\alpha} \}_{\alpha \in I}$ be a collection of objects in $\C$ indexed by a set $I$, called the \emph{indexing set}. The \emph{product} of $\{ X_{\alpha} \}_{\alpha \in I}$ is the terminal object in the new category:
\begin{itemize}
    \item Objects: all collections of morphisms $\{ f_{\alpha} : Z \to X_{\alpha} \}_{\alpha \in I}$ in $\C$ for all $Z \in \Ob{\C}$;
    \item Morphisms: for all $\alpha \in I$, commutative diagrams in $\C$:
\end{itemize}
\begin{center}
    \begin{tikzcd}[column sep=normal]
        & X_\alpha \\
        Z \arrow[ur, "f_\alpha"] \arrow[rr, "f" swap, dashed] & & Z' \arrow[ul, "f_\alpha'" swap]
    \end{tikzcd}
\end{center}

The \emph{coproduct} of $\{ X_{\alpha} \}_{\alpha \in I}$ is the initial object in the new category:
\begin{itemize}
    \item Objects: all collections of morphisms $\{ f_{\alpha} : X_{\alpha} \to Z \}_{\alpha \in I}$ in $\C$ for all $Z \in \Ob{\C}$;
    \item Morphisms: for all $\alpha \in I$, commutative diagrams in $\C$:
\end{itemize}
\begin{center}
    \begin{tikzcd}[column sep=normal]
        & Z \arrow[dr, "f_\alpha"] \arrow[dl, "f_\alpha'" swap] \\
        X_\alpha & & Z' \arrow[ll, "f", dashed]
    \end{tikzcd}
\end{center}

Then the product and coproduct have the following universal properties respectively:
\begin{center}
    \begin{tikzcd}
        X_\alpha & Z \arrow[dl, "\exists ! f"] \arrow[l, "\forall f_\alpha" swap, dashed] \\
        \prod X_\alpha \arrow[u, "\pi_\alpha"]
    \end{tikzcd}
    \qquad
    \begin{tikzcd}[swap]
        X_\alpha \arrow[d, "\iota_\alpha"] \arrow[r, "\forall f_\alpha" swap] & Z \\
        \coprod X_\alpha \arrow[ur, "\exists ! f", dashed]
    \end{tikzcd}
\end{center}

The elements in the product of a family of objects in $\Vect_{\F}$ can be written as ordered tuples: $(v_\alpha)_{\alpha \in I}$. Then the product can be defined as follows:
\[
    \prod_{\alpha \in I} V_\alpha = \{ (v_\alpha)_{\alpha \in I} \mid v_\alpha \in V_\alpha \}
\]
Then the coproduct can be defined as follows:
\[
    \bigoplus_{\alpha \in I} V_\alpha = \{ (v_\alpha) \in \prod_{\alpha \in I} V_\alpha \mid v_\alpha \text{ is finitely supported} \} \subseteq \prod_{\alpha \in I} V_\alpha
\]
\begin{remark}
    In general, the product is not equal to the coproduct. They are equal if and only if the indexing set $I$ is a finite set.
\end{remark}

Consider the following diagram:
\begin{center}
    \begin{tikzpicture}
        \filldraw[gray!10] (-4,-2) rectangle (4,2);
        \draw[step=0.5cm,gray!20] (-4,-2) grid (4,2);

        \draw[red] plot [smooth] coordinates { (-4, 1.5) (-2.5, 1) (-1, -1) (0.5, -1) (2, -1.5) (4, -2) };
        \draw[ocre] plot [smooth] coordinates { (-4, -0.5) (-2.5, 0) (-1, 0) (0.5, 1.5) (2, 0) (4, 0) };

        \draw[thick] (-2.5,-2) -- (-2.5,2) node [above] {\scriptsize $V_{\alpha_1}$};
        \draw[thick] (-1,-2)  -- (-1,2) node [above] {\scriptsize $V_{\alpha_2}$};
        \draw[thick] (0.5,-2) -- (0.5,2) node [above] {\scriptsize $V_{\alpha_3}$};
        \draw[thick] (2,-2)  -- (2,2) node [above] {\scriptsize $V_{\alpha_4}$};

        \path (3.5, 2) node [above] {\scriptsize $\cdots$};
        
        \filldraw (-2.5, 1) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_1)$};
        \filldraw (-1, -1) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_2)$};
        \filldraw (0.5, -1) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_3)$};
        \filldraw (2, -1.5) circle (1pt) node [above right] {\scriptsize $s_1(\alpha_4)$};

        \filldraw (0.5, 1.5) circle (1pt) node [above right] {\scriptsize $s_2(\alpha_3)$};

        \filldraw (-2.5, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_1}$};
        \filldraw (-1, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_2}$};
        \filldraw (0.5, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_3}$};
        \filldraw (2, 0) circle (1pt) node [below right] {\scriptsize $0_{\alpha_4}$};

        \draw[decoration={brace,raise=5pt},decorate]
            (-4,-2) -- node [left=6pt] {$\displaystyle \bigcup_{\alpha \in I} V_\alpha$} (-4,2);
        \draw[-Stealth] (-4 cm - 22 pt, -4 cm + 10 pt) -- (-4 cm - 22 pt,-15 pt) node [midway, left] {{\color{red} $s_1$}, {\color{ocre} $s_2$}};

        \draw[thick] (-4,-4) node [left=16pt] {$I$} -- (4,-4);

        \draw[dashed] (-2.5,-2) -- (-2.5,-4);
        \draw[dashed] (-1,-2) -- (-1,-4);
        \draw[dashed] (0.5,-2) -- (0.5,-4);
        \draw[dashed] (2,-2) -- (2,-4);

        \filldraw (-2.5,-4) circle (1.5pt) node [below] {$\alpha_1$};
        \filldraw (-1,-4) circle (1.5pt) node [below] {$\alpha_2$};
        \filldraw (0.5,-4) circle (1.5pt) node [below] {$\alpha_3$};
        \filldraw (2,-4) circle (1.5pt) node [below] {$\alpha_4$};
    \end{tikzpicture}
\end{center}
\begin{remark}
    The right sections $s_1$ and $s_2$ are two elements in the product $\prod V_\alpha$. Note that $s_2$ is likely to be ``finitely supported'' since it is zero in almost all components shown in the diagram. However, if $I$ is an infinite set, then $s_2$ may not be finitely supported since there may be infinitely many non-zero components not shown in the diagram. So $s_2$ may not be an element in the coproduct $\bigoplus V_\alpha$ if $I$ is an infinite set, but most likely to be.
\end{remark}

So the product $\prod V_\alpha$ contains all possible sections $s : I \to \bigcup V_\alpha$, so it is called the \emph{space of sections}. The coproduct $\bigoplus V_\alpha$ contains all finitely supported sections, so it is called the \emph{space of sections with finite support}. The elements in the coproduct $\bigoplus V_\alpha$ written as ordered tuples $(v_\alpha)_{\alpha \in I}$ can also be written as finite sums $\sum_{\alpha \in I} v_\alpha$ since only finitely many $v_\alpha$ are non-zero.

Actually, the product and coproduct are the generalisation of the polynomial ring and the formal power series ring respectively. We can consider the following diagrams:
\begin{center}
    \begin{tikzpicture}
        \filldraw[gray!10] (-4,-2) rectangle (4,2);
        \draw[step=0.5cm,gray!20] (-4,-2) grid (4,2);
        
        \draw[red] plot [smooth] coordinates { (-4, 1.5) (-2.5, 1) (-1, -1) (0.5, 1.5) (2, -1.5) (4, -0.5) };

        \filldraw (-2.5, 1) circle (1pt) node [above right] {\scriptsize $s(\alpha_1)$};
        \filldraw (-1, -1) circle (1pt) node [right] {\scriptsize $s(\alpha_2)$};
        \filldraw (0.5, 1.5) circle (1pt) node [right] {\scriptsize $s(\alpha_3)$};
        \filldraw (2, -1.5) circle (1pt) node [below right] {\scriptsize $s(\alpha_4)$};

        \draw[thick] (-2.5,-2) -- (-2.5,2) node [above] {\scriptsize $\F$};
        \draw[thick] (-1,-2)  -- (-1,2) node [above] {\scriptsize $\F$};
        \draw[thick] (0.5,-2) -- (0.5,2) node [above] {\scriptsize $\F$};
        \draw[thick] (0.5,-2) -- (0.5,2) node [above] {\scriptsize $\F$};
        \draw[thick] (2,-2)  -- (2,2) node [above] {\scriptsize $\F$};

        \path (3.5, 2) node [above] {\scriptsize $\cdots$};

        \draw[decoration={brace,raise=5pt},decorate]
            (-4,-2) -- node [left=6pt] {$\F[S]$} (-4,2);
        \draw[-Stealth] (-4 cm - 20 pt, -4 cm + 10 pt) -- (-4 cm - 20 pt,-10 pt) node [midway, left] {$s$};

        \draw[thick] (-4,-4) node [left=13pt] {$S$} -- (4,-4);

        \draw[dashed] (-2.5,-2) -- (-2.5,-4);
        \draw[dashed] (-1,-2) -- (-1,-4);
        \draw[dashed] (0.5,-2) -- (0.5,-4);
        \draw[dashed] (2,-2) -- (2,-4);

        \filldraw (-2.5,-4) circle (1.5pt) node [below] {$\alpha_1$};
        \filldraw (-1,-4) circle (1.5pt) node [below] {$\alpha_2$};
        \filldraw (0.5,-4) circle (1.5pt) node [below] {$\alpha_3$};
        \filldraw (2,-4) circle (1.5pt) node [below] {$\alpha_4$};

        \draw[thick] (6, -2) -- (6, 2) node [above] {\scriptsize $\F$};

        \filldraw (6, 1) circle (1pt) node [right] {\scriptsize $s(\alpha_1)$};
        \filldraw (6, -1) circle (1pt) node [right] {\scriptsize $s(\alpha_2)$};
        \filldraw (6, 1.5) circle (1pt) node [right] {\scriptsize $s(\alpha_3)$};
        \filldraw (6, -1.5) circle (1pt) node [right] {\scriptsize $s(\alpha_4)$};
    \end{tikzpicture}
\end{center}

The left shows the diagram in generalised version, but it can be squeezed to the right since all fibres are the same. So we can consider the set map as $s : S \to \F$ as shown on the right.

\newpage

\section{Functors}

\begin{definition}[Functors]
    Let $\C$ and $\D$ be two categories. A \emph{functor} $F : \C \to \D$ consists of the following data:
    \begin{itemize}
        \item A map $F : \Ob{\C} \to \Ob{\D}$;
        \item A map $F : \Mor_{\C}(X, Y) \to \Mor_{\D}(F(X), F(Y))$ for all $X, Y \in \Ob{\C}$;
    \end{itemize}
    such that the following conditions are satisfied:
    \begin{enumerate}[label=(\alph*)]
        \item For all $X \in \Ob{\C}$, we have $F(1_X) = 1_{F(X)}$;
        \item For all appropriate morphisms $f, g$ in $\C$, we have $F(fg) = F(f) F(g)$.
    \end{enumerate}
\end{definition}

\begin{example}
    There are two functors from $\Set$ to $\Vect_{\F}$:
    \begin{center}
        \begin{tikzcd}
            \Set \arrow[r, "{\F[-]}", yshift=0.5ex] & \Vect_{\F} \arrow[l, "|-|", yshift=-0.5ex]
        \end{tikzcd}
    \end{center}
    where $\F[-]$ sends set $X$ to the free vector space $\F[X]$ generated by $X$, and a set map $f : X \to Y$ to the linear map $\F[f] : \F[X] \to \F[Y]$ induced by $f$. The functor $|-|$ sends a vector space $V$ to its underlying set $|V|$, and a linear map $\phi : V \to W$ to the set map $|\phi| : |V| \to |W|$ induced by $\phi$.

    The $\F[-]$ is called the \emph{free functor}, specifically it is the \emph{free vector space functor}. The $|-|$ is called the \emph{underlying functor} or \emph{forgetful functor}.
\end{example}

For some set $X$ and any vector space $V$, we can consider the following diagram:
\begin{center}
    \begin{tikzcd}
        X \arrow[r, "\forall \phi"] \arrow[d, "\iota", hook] & V \\
        \F[X] \arrow[ur, dashed, "\exists ! \bar{\phi}" swap]
    \end{tikzcd}
\end{center}
This is called the \emph{universal property of free vector space over a set}. Here $\iota : X \to \F[X]$ is the inclusion map, $\phi : X \to V$ is any set map, and $\bar{\phi} : \F[X] \to V$ is the unique linear map induced by $\phi$.
\begin{remark}
    The universal property of free vector space over a set can be rephrased as follows: for any set $X$ and any vector space $V$, there is a natural identification:
    \[
        \Set(X, |V|) \equiv \Vect_{\F}(\F[X], V)
    \]
    where $\Set(X, |V|)$ is the set of all set maps from $X$ to the underlying set of $V$, and $\Vect_{\F}(\F[X], V)$ is the set of all linear maps from the free vector space $\F[X]$ to $V$.
    
    If we consider $\phi : X \to |V|$ as an element in $\Set(X, |V|)$, then the corresponding element in $\Vect_{\F}(\F[X], V)$ is the unique linear map $\bar{\phi} : \F[X] \to V$ induced by $\phi$.

    Note that $\iota \equiv 1_{\F[X]}$ is the identity element in $\Vect_{\F}(\F[X], \F[X])$, so it corresponds to an element in $\Set(X, |\F[X]|)$, which is exactly the inclusion map $\iota : X \to |\F[X]|$.
\end{remark}

\begin{definition}[Adjoint Functors]
    Let $\C$ and $\D$ be two categories. A functor $F : \C \to \D$ is called a \emph{left adjoint} of a functor $G : \D \to \C$, and $G$ is called a \emph{right adjoint} of $F$, if there is a natural identification:
    \[
        \D(F(X), Y) \equiv \C(X, G(Y))
    \]
    for all $X \in \Ob{\C}$ and $Y \in \Ob{\D}$.
\end{definition}

\begin{example}
    The free functor $\F[-] : \Set \to \Vect_{\F}$ is a left adjoint of the underlying functor $|-| : \Vect_{\F} \to \Set$. This is exactly the universal property of free vector space over a set.
\end{example}

\begin{definition}[Endofunctors]
    An \emph{endofunctor} is a functor $F : \C \to \C$ that maps a category to itself.
\end{definition}

\begin{example}
    Let $X$ be a set. Then we have an adjoint pair of functors:
    \begin{center}
        \begin{tikzcd}
            \Set \arrow[r, "- \times X", yshift=0.5ex] & \Set \arrow[l, "{\Set(X, -)}", yshift=-0.5ex]
        \end{tikzcd}
    \end{center}

    On the left is the endofunctor $- \times X$ and on the right is the endofunctor $\Set(X, -)$.
    \begin{center}
        \begin{tikzcd}[row sep=normal]
            \Set \arrow[r, "- \times X"] & \Set \\[-1.8em]
            Y \arrow[dd, "f" swap] & Y \times X \arrow[dd, "f \times 1_X"] \\
            \arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
            Z & Z \times X
        \end{tikzcd}
        \qquad\qquad
        \begin{tikzcd}[row sep=normal]
            \Set & \Set \arrow[l, "{\Set(X, -)}" swap] \\[-1.8em]
            \Set(X, Y) \arrow[dd, "{\Set(X, f)}" swap] & Y \arrow[dd, "f"] \\
            \phantom{*} & \arrow[l, mapsto, shorten <= 2ex, shorten >= 2ex] \\
            \Set(X, Z) & Z 
        \end{tikzcd}
    \end{center}

    Consider an element $g \in \Set(X, Y)$, which is a set map $g : X \to Y$. Then the corresponding element in $\Set(X, Z)$ is $\Set(X, f)(g) = fg : X \to Z$.

    Then we can write the natural identification as follows:
    \[
        \Set(Y \times X, Z) \equiv \Set(Y, \Set(X, Z))
    \]
    for all sets $Y$ and $Z$. This means that a set map $F : Y \times X \to Z$ corresponds to a set map $F_{\musNatural} : Y \to \Set(X, Z)$ such that a $y \in Y$ is mapped to a set map $F_{\musNatural}(y) : X \to Z$ defined by $F_{\musNatural}(y)(x) = F(y, x)$ for all $x \in X$.
\end{example}

Consider the following two diagrams:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        X_1 & X_1 \times X_2 \arrow[l] \arrow[r] \arrow[d, "{\F[-]}", Rightarrow] & X_2 \\
        \F[X_1] & \F[X_1 \times X_2] \arrow[l] \arrow[r] & \F[X_2] \\[-3.6em]
        & {\scriptstyle \equiv \F[X_1] \otimes \F[X_2]}
    \end{tikzcd}
    \qquad
    \begin{tikzcd}[column sep=normal]
        X_1 \arrow[r] & X_1 \sqcup X_2 \arrow[d, "{\F[-]}", Rightarrow] & X_2 \arrow[l] \\
        \F[X_1] \arrow[r] & \F[X_1 \sqcup X_2] & \F[X_2] \arrow[l] \\[-3.6em]
        & {\scriptstyle \equiv \F[X_1] \oplus \F[X_2]}
    \end{tikzcd}
\end{center}
The left diagram shows that the free functor sends the product of two sets to the tensor product of two vector spaces. The right diagram shows that the free functor sends the coproduct of two sets to the direct sum of two vector spaces, i.e., the coproduct of two vector spaces. Note that the tensor product of two vector spaces is \emph{not} the product of two vector spaces, as the dimension of the tensor product is $\dim(V_1 \otimes V_2) = \dim(V_1) \cdot \dim(V_2)$ while the dimension of the product is $\dim(V_1 \oplus V_2) = \dim(V_1) + \dim(V_2)$. There is a unique but not isomorphic linear map $\phi : V_1 \otimes V_2 \to V_1 \oplus V_2$.
\begin{remark}
    The left adjoint functor preserves coproducts, and the right adjoint functor preserves products. This is the consequences of the \emph{adjoint functor theorem}. 
\end{remark}

Similarly, we have the following natural identifications:
\[
    \Vect_{\F}(X \otimes Y, Z) \equiv \Vect_{\F}(Y, \Vect_{\F}(X, Z))
\]
Note that $\Vect_{\F}(X, Z)$ is a vector space over $\F$, as $\Vect_{\F} \equiv \Hom_{\F}$. Then, we have the following adjoint pair of endofunctors on $\Vect_{\F}$:
\begin{center}
    \begin{tikzcd}
        \Vect_{\F} \arrow[r, "- \otimes X", yshift=0.5ex] & \Vect_{\F} \arrow[l, "{\Hom_{\F}(X, -)}", yshift=-0.5ex]
    \end{tikzcd}
\end{center}

\newpage

\section{Dual Spaces and Dual Bases}

Let $V$ be a finite dimensional linear space over $\F$. The \emph{dual space} of $V$ is the vector space $V^* = \Hom_{\F}(V, \F)$, the set of all linear functionals from $V$ to $\F$, or \emph{covectors}. 

\begin{proposition}
    Let $V$ be a finite dimensional linear space over $\F$. Then $\dim(V^*) = \dim(V)$. So, $V^*$ is isomorphic to $V$ but not naturally isomorphic to $V$.
\end{proposition}
\begin{proof}
    Without the loss of generality, we may assume $\dim{V} = n$ and $V = \F^n$. Then $V^* = \Hom_{\F}(\F^n, \F) \equiv \M{1 \times n}{\F}$, the linear space of row matrices with $n$ entries. The linear space is the span of $n$ standard basis row matrices: $\hat{e}^1, \hat{e}^2, \cdots, \hat{e}^n$. So $\dim(V^*) = n = \dim(V)$. We can say $V^* \cong V$.
\end{proof}

We have a map $\phi_s : \F^n \to (\F^n)^* \supset S = \{\hat{e}^1, \hat{e}^2, \cdots, \hat{e}^n\}$ defined by $\phi_s(\vec{x}) = \sum_{i=1}^n x_i \hat{e}^i$. This is a vector space isomorphism but not a natural isomorphism, as it depends on the choice of $S$.

\begin{definition}[Bases]
    A \emph{basis} of a linear space $V$ over $\F$ is the minimal spanning set of $V$ with an order. The set of all bases of $V$ is denoted by $\B_V$.
\end{definition}

\begin{proposition}
    $\B_V$ and $\B_{V^*}$ are naturally isomorphic in $\Set$, i.e., the following natural identification holds:
    \[
        \begin{split}
            \B_V &\equiv \B_{V^*} \\
            v = (\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n) &\equiv (\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n) = v^* \\
        \end{split}
    \]
    where $\hat{v}^i \in V^*$ is defined by $\hat{v}^i(\vec{v}_j) = \delta^i_j$ for all $1 \leq i, j \leq n$.
\end{proposition}

\begin{proof}
Consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        V \arrow[d, "{[-]_V}" swap, hook, two heads] \arrow[dr, dashed, "\hat{v_i}"] \\
        \F^n \arrow[r, "\pi_i"] & \F
    \end{tikzcd}
\end{center}
The projection map $\pi_i$ is a linear functional in $\F^n$ that sends $\vec{x} = (x_1, x_2, \cdots, x_n)$ to $x_i$. It is actually $\hat{e}^i$. Note that $[-]_V : V \to \F^n$ is a coordinate map defined by a basis $v = (\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n) \in \B_V$ such that $[\vec{v}_j]_V = \vec{e}_j$ for all $1 \leq j \leq n$. It is a unique linear map which identify $\vec{v}_i$ with $\vec{e}_i$. It can be done by trivialisation of $V$ with respect to the basis $v$. Then we define $\hat{v}^i(\vec{v}_j) = \delta^i_j$ for all $1 \leq i, j \leq n$.

Then we have to consider whether $(\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$ is a basis of $V^*$. As $\dim{V^*} = n$, we only need to show that $(\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$ is a spanning set of $V^*$ or linearly independent. We have to check whether the equation $\sum_{i=1}^n x_i \hat{v}^i = 0$ for some $x_i \in \F$ has only the trivial solution. Applying it to $\vec{v}_j$ for all $1 \leq j \leq n$, we have $0 = \sum_{i=1}^n x_i \hat{v}^i(\vec{v}_j) = \sum_{i=1}^n x_i \delta_j^i = x_j$. So $x_j = 0$ for all $1 \leq j \leq n$. This means that $(\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$ is linearly independent, and hence it is a basis of $V^*$. We call it the \emph{dual basis} of the basis $\vec{v} = (v_1, v_2, \cdots, v_n)$ and denote it by $\vec{v}^* = (\hat{v}^1, \hat{v}^2, \cdots, \hat{v}^n)$. 

Then we have to show that there is a unique basis in $V^*$ that satisfies $\hat{v}^i(\vec{v}_j) = \delta^i_j$. Let $V = \F^n$ and $v = (\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n)$ be a basis of $V$. Then $A = [\vec{v}_1 \quad \vec{v}_2 \quad \cdots \quad \vec{v}_n]$ is an invertible matrix. Let $(\alpha^1, \alpha^2, \cdots, \alpha^n)$ be a basis of $V^*$. Then we have the following equations:
\[
    [ \delta^i_j ] = \begin{bmatrix}
        \hdash & \alpha^1 & \hdash \\
        & \vdots \\
        \hdash & \alpha^n & \hdash
    \end{bmatrix} \begin{bmatrix}
        | & & | \\
        \vec{v_1} & \cdots & \vec{v_n} \\
        | & & |
    \end{bmatrix} = I_n
\]
Then $(\alpha_1, \alpha_2, \cdots, \alpha_n) = A^{-1}$. So the dual basis is unique.

Finally, we have the natural identification:
\end{proof}
\begin{remark}
    $V \cong V^*$ but $\B_V \equiv \B_{V^*}$. The isomorphism $V \cong V^*$ depends on the choice of a basis in $\B_V$, while the natural isomorphism $\B_V \equiv \B_{V^*}$ does not depend on any choice.
\end{remark}

\begin{example}
    Consider the following open subset $U$ of $\R^2$:
    \begin{center}
		\begin{tikzpicture}
			\draw[gray,-Stealth,thin] (-1,0) -- (5,0) node[right] {$x$};
			\draw[gray,-Stealth,thin] (0,-1) -- (0,4) node[above] {$y$};

            \filldraw (3.5,1) circle (1pt) node[below right] {$p$};

			\draw[dashed] plot[smooth cycle, tension=1] coordinates{(4.5,1.5) (2.5,2.91) (0.5,1.5) (1.25,0.6) (2.5,1) (3.75,0.4)};
			\path (2.5,1.75) node {$U$};

            \draw[-Stealth, ocre] (3.5,1) -- (4,2) node[midway, right] {$\vec{u}$};
            \draw[-Stealth, ocre] (0,0) -- (0.5,1) node[midway, right] {$\vec{u}$};
		\end{tikzpicture}
	\end{center}
    Consider the cotangent vector $df_p$ at point $p$ for some smooth function $f : U \to \R$. It is a linear functional d$f_p : \text{T}_p U \to \R$ defined by d$f_p(\vec{u}) = \nabla f(p) \cdot \vec{u}$ for all $\vec{u} \in \text{T}_p U$. Here T$_p U$ is the tangent space of $U$ at point $p$, which is a vector space over $\R$. Note that both $\vec{u}$ and $\nabla f(p)$ are depending on the choice of a coordinate system. However, d$f_p$ is independent of any choice of coordinate system. In normal calculus, d$f_p$ is called the \emph{first partial derivative} of $f$ at point $p$, and normally we write it as $\frac{\partial f}{\partial x}(p)$ and $\frac{\partial f}{\partial y}(p)$.
\end{example}

The dual functor is not naturally isomorphic to the identity functor on $\Vect_{\F}$, as $(-)^*$ is a contravariant functor, while the identity is a contravariant functor, so there is no natural transformation from $\id_{\Vect_{\F}}$ to $(-)^*$. 

\begin{center}
    \begin{tikzcd}[row sep=normal]
        \Vect_{\F} \arrow[r, "\id_{\Vect_{\F}}"] & \Vect_{\F} \\[-1.8em]
        Y \arrow[dd, "f" swap] & Y \arrow[dd, "f"] \\
        \arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
        Z & Z
    \end{tikzcd}
    \qquad\qquad
    \begin{tikzcd}[row sep=normal]
        \Vect_{\F} \arrow[r, "(-)^*"] & \Vect_{\F} \\[-1.8em]
        Y \arrow[dd, "f" swap] & Y^* \\
        \arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
        Z & Z^* \arrow[uu, "f^*" swap]
    \end{tikzcd}
\end{center}

\newpage

\section{Double Dual Spaces and Doubles}

Consider the endofunctors on $\Vect_{\F}$:
\begin{center}
    \begin{tikzcd}
        \Vect_{\F} \arrow[r, "(-)^{**}", yshift=0.5ex] \arrow[r, "\id_{\Vect_{\F}}" swap, yshift=-0.5ex] & \Vect_{\F} 
    \end{tikzcd}
\end{center}
There is a natural transformation from $\id_{\Vect_{\F}}$ to $(-)^{**}$ defined by the natural identification: $V \equiv V^{**}$. As $\id_{\Vect_{\F}}$ and $(-)^{**}$ are covariant functors, there is a natural transformation between them.

\begin{center}
    \begin{tikzcd}[row sep=normal]
        \Vect_{\F} \arrow[r, "\id_{\Vect_{\F}}"] & \Vect_{\F} \\[-1.8em]
        Y \arrow[dd, "f" swap] & Y \arrow[dd, "f"] \\
        \arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
        Z & Z
    \end{tikzcd}
    \qquad\qquad
    \begin{tikzcd}[row sep=normal]
        \Vect_{\F} \arrow[r, "(-)^{**}"] & \Vect_{\F} \\[-1.8em]
        Y \arrow[dd, "f" swap] & Y^{**} \arrow[dd, "f^{**}" swap] \\
        \arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
        Z & Z^{**}
    \end{tikzcd}
\end{center}

Let $\langle-,-\rangle : V^* \times V \to \F$ be the natural pairing defined by $\langle\alpha, u\rangle = \alpha(u)$ where $\alpha : V \to \F$ that sends $u \to \alpha{u}$. It is the pairing of a covector with a vector and the map is bilinear.

\begin{definition}[Bilinear Maps]
    A map $B : U \times V \to W$ is called \emph{bilinear} if for all $u \in U$, the map $B(u, -) : V \to W$ is linear, and for all $v \in V$, the map $B(-, v) : U \to W$ is linear.
\end{definition}

We have the following natural identification:
\begin{center}
    \begin{tikzcd}[row sep=normal, column sep=normal]
        V^* \times V \arrow[rr, "{\langle-,-\rangle}"] & \arrow[d, "\vequiv" description, phantom] & \F \arrow[r, "\equiv" description, phantom] 
        & V^* \arrow[rr, "1_{V^*}"] & \arrow[d, "\vequiv" description, phantom] & \Hom_{\F}(V, \F) \\


        V \times V^* \arrow[d, hook, two heads] \arrow[rr] & \phantom{*} & \F \arrow[r, "\equiv" description, phantom] 
        & V \arrow[rr, "\iota_V"] & \phantom{*} & \Hom_{\F}(V^*, \F) \\[1.8em]


        V^* \times V \arrow[urr, "{\langle-,-\rangle}" swap]
    \end{tikzcd}
\end{center}
where $\iota_V : V \to V^{**}$ is defined by $\iota_V(u) = \check{u}$ such that $\check{u}(\alpha) = \alpha(u)$. Then $V^{**} = \Hom_{\F}(V^*, \F) \equiv V$.

\begin{definition}[Doubles]
    Let $V$ be a linear space over $\F$. The \emph{double} of $V$, denoted by $D(V)$, is defined as follows:
    \[
        D(V) = V \oplus V^*
    \]
\end{definition}

As $V$ is naturally isomorphic to $V^{**}$, we have the following natural identification:
\[
    D(V) = V \oplus V^* \equiv V^* \oplus V^{**} = D(V^*)
\]

The matrix representation of the isomorphism between $D(V)$ and $D(V^*)$ is
\[
    \begin{bmatrix}
        0 & -\iota_V \\
        1 & 0
    \end{bmatrix}
\]
where $\iota_V : V \to V^{**}$ is the natural isomorphism defined above. The negative sign is used to make the isomorphism a symplectic isomorphism, which will be discussed in the later chapters.

\newpage

\section{Natural Transformation and Natural Equivalences}

\begin{definition}[Natural Transformations]
    Let $F, G : \C \to \D$ be two functors. A \emph{natural transformation} $\eta : F \to G$ is a collection of morphisms $\eta_X : F(X) \to G(X)$ in $\D$ for all objects $X$ in $\C$, such that for all morphisms $f : X \to Y$ in $\C$, the following diagram commutes:
    \begin{center}
        \begin{tikzcd}
            F(X) \arrow[r, "F(f)"] \arrow[d, "\eta_X" swap] & F(Y) \arrow[d, "\eta_Y"] \\
            G(X) \arrow[r, "G(f)"] & G(Y)
        \end{tikzcd}
    \end{center}
\end{definition}

\begin{definition}[Natural Equivalences]
    A \emph{natural equivalence} from functor $F$ to functor $G$ is a natural transformation $\eta : F \to G$ which has a two-sided inverse natural transformation $\eta^{-1} : G \to F$ such that $\eta \eta^{-1} = 1_G$ and $\eta^{-1} \eta = 1_F$. In this case, we say that $F$ and $G$ are \emph{naturally equivalent}, denoted by $F \equiv G$.
\end{definition}

\begin{example}
    Consider the endofunctors on $\Vect_{\F}$:
    \begin{center}
        \begin{tikzcd}
            \Vect_{\F} \arrow[r, "(-)^{**}", yshift=0.5ex] \arrow[r, "\id_{\Vect_{\F}}" swap, yshift=-0.5ex] & \Vect_{\F} 
        \end{tikzcd}
    \end{center}
    We have the following natural transformation:
    \begin{center}
        \begin{tikzcd}
            (-)^{**} \arrow[d, "\vequiv", phantom] & V_1 \arrow[r, "f"] & V_2 \arrow[r, mapsto] & V_1^{**} \arrow[r, "f^{**}"] \arrow[d, "\eta_{V_1}" swap, ocre, "\cong"] & V_2^{**} \arrow[d, "\eta_{V_2}", ocre, "\cong" swap] \\
            \id_{\Vect_{\F}} & V_1 \arrow[r, "f"] & V_2 \arrow[r, mapsto] & V_1 \arrow[r, "f"] & V_2
        \end{tikzcd}
    \end{center}
    Then we have the natural equivalence: $(-)^{**} \equiv \id_{\Vect_{\F}}$.
\end{example}

\begin{example}
    We have the following natural equivalence:
    \[
        \Map^{\BL}(U \times V, -) \equiv \Hom_{\F}(U, \Hom_{\F}(V, -))
    \]
    where both are endofunctors on $\Vect_{\F}$. For any linear space $Z$ over $\F$, we have the natural isomorphism:
    \[
        \musNatural_Z : \Map^{\BL}(U \times V, Z) \to \Hom_{\F}(U, \Hom_{\F}(V, Z))
    \]
\end{example}

\begin{example}
    We have the following natural equivalence:
    \[
        \F \otimes - \equiv \id_{\Vect_{\F}} \equiv - \otimes \F \equiv \Hom_{\F}(\F, -) \equiv (-)^{**}
    \]
\end{example}

\newpage

\section{Exact Functors}

\begin{definition}[Covariant Exact Functors]
    Let $\C$ and $\D$ be two abelian categories. A covariant functor $F : \C \to \D$ is called:
    \begin{itemize}
        \item \emph{left exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $0 \to F(A) \to F(B) \to F(C)$ is exact in $\D$, i.e., it preserves all finite limits;
        \item \emph{right exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $F(A) \to F(B) \to F(C) \to 0$ is exact in $\D$, i.e., it preserves all finite colimits;
        \item \emph{exact} if it is both left exact and right exact.
    \end{itemize}
\end{definition}

\begin{definition}[Contravariant Exact Functors]
    Let $\C$ and $\D$ be two abelian categories. A contravariant functor $G : \C \to \D$, it is called:
    \begin{itemize}
        \item \emph{contravariant left exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $0 \to G(C) \to G(B) \to G(A)$ is exact in $\D$;
        \item \emph{contravariant right exact} if whenever $0 \to A \to B \to C \to 0$ is exact then $G(C) \to G(B) \to G(A) \to 0$ is exact in $\D$;
        \item \emph{contravariant exact} if it is both contravariant left exact and contravariant right exact.
    \end{itemize}
\end{definition}

\begin{example}
    The dual functor $(-)^* : \Vect_{\F} \to \Vect_{\F}$ is a contravariant left exact functor, as it sends a short exact sequence $0 \to U \to V \to W \to 0$ to a left exact sequence $0 \to W^* \to V^* \to U^*$. Moreover, $U \to V \to W$ is exact if and only if $W^* \to V^* \to U^*$ is exact. Also, the map $U \to V$ is injective if and only if the map $V^* \to U^*$ is surjective; the map $U \to V$ is surjective if and only if the map $V^* \to U^*$ is injective. This can be shown by considering the following two exact sequences: $0 \to U \to V$ and $U \to V \to 0$.
\end{example}

In general, the hom-set functor $\Hom_{\C}(X, -) : \C \to \Set$ is a covariant left exact functor for any object $X$ in an abelian category $\C$, and the hom-set functor $\Hom_{\C}(-, X) : \C \to \Set$ is a contravariant left exact functor for any object $X$ in an abelian category $\C$.

\begin{example}
    The tensor product functor $- \otimes V$ is a covariant right exact functor, as it sends a short exact sequence $0 \to U \to V \to W \to 0$ to a right exact sequence $U \otimes V \to V \otimes V \to W \otimes V \to 0$. 
\end{example}

Note that the tensor product functor is a left adjoint functor, and left adjoint functors are right exact in general, while the $\Vect_{\F}(V, -)$ functor is a right adjoint functor, and right adjoint functors are left exact in general.




\chapter{Tensor Algebra}

\epigraph{In high level universities, students will blame themselves if they don't understand the content, but in low level universities, students will blame the professors.}{Guowu Meng}

\section{Tensor Products}

Let $U$ and $V$ be two fixed linear spaces over $\F$ and $Z$ be any linear space over $\F$. Consider the set of all bilinear maps from $U \times V$ to $Z$, denoted by $\Map^{\BL}(U \times V, Z)$. It is a vector space over $\F$ as it is a subset of $\Map(U \times V, Z)$, the set of all maps from $U \times V$ to $Z$.

By the universal property of tensor product, we have a natural identification:
\[
    \Map^{\BL}(U \times V, Z) \equiv \Hom_{\F}(U \otimes V, Z)
\]
Note that both are naturally identical to $\Hom_{\F}(U, \Hom_{\F}(V, Z))$. Also note that $\Hom(- \otimes V, Z) \equiv \Hom(-, \Hom(V, Z))$ is a \emph{tensor-hom adjunction}.

The natural identification is the universal property of tensor product. Consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        U \times V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & Z \\
        U \otimes V \arrow[ur, dashed, "\exists ! \bar{\phi}" swap]
    \end{tikzcd}
\end{center}
Note that the map $\iota$ and $\phi$ are bilinear maps, and the existence of the unique linear map $\bar{\phi}$ follows from the universal property of the tensor product. We can also consider it as the initial object in a new category:
\begin{itemize}
    \item Objects: all bilinear maps $\phi : U \times V \to Z$ for all $Z \in \Ob{\Vect_{\F}}$;
    \item Morphisms: commutative diagrams in $\Vect_{\F}$:
\end{itemize}
\begin{center}
    \begin{tikzcd}[row sep=normal]
        & Z \arrow[dd, "f"] \\
        U \times V \arrow[ur, "\phi"] \arrow[dr, "\phi'" swap] & \\
        & Z'
    \end{tikzcd}
\end{center}

The existence of tensor product follows from the existence of free vector space over a set and the existence of quotient spaces. 

Consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        & U \times V \arrow[dr, "\forall \phi"] \arrow[d, "\iota'" swap, hook] \\
        \ideal_{U, V} \arrow[r, hook] & \F[U \times V] \arrow[r, dashed, "\exists ! \phi'"] \arrow[d, "\pi" swap, two heads] & Z \\
        & \quotient{\F[U \times V]}{\ideal_{U, V}} \arrow[ur, dashed, "\exists ! \bar{\phi}" swap] \arrow[from=uu, bend right=60, "\iota" near start, swap, crossing over]
    \end{tikzcd}
\end{center}
where $\ideal_{U, V}$ is the subspace of $\F[U \times V]$ generated by the following elements for all $u, u_1, u_2 \in U$, $v, v_1, v_2 \in V$ and $\alpha, \beta \in \F$:
\begin{itemize}
    \item $(\alpha u_1 + \beta u_2, v) - \alpha (u_1, v) - \beta (u_2, v)$;
    \item $(u, \alpha v_1 + \beta v_2) - \alpha (u, v_1) - \beta (u, v_2)$;
\end{itemize}
Why the construction of $\ideal_{U, V}$ is like this? This is because we want $\iota$ to be a bilinear map. Then $\iota(\alpha u_1 + \beta u_2, v) = \alpha \iota(u_1, v) + \beta \iota(u_2, v)$ and $\iota(u, \alpha v_1 + \beta v_2) = \alpha \iota(u, v_1) + \beta \iota(u, v_2)$. This means that the elements in $\ideal_{U, V}$ should be mapped to $0$ by $\iota$. So we have to quotient $\F[U \times V]$ by $\ideal_{U, V}$ to make $\iota$ a bilinear map.

We define $U \otimes V = \quotient{\F[U \times V]}{\ideal_{U, V}}$ and this shows the existence of tensor product.
\begin{remark}
    The inclusion map $\iota : U \times V \to U \otimes V$ is `surjective' in the sense that the image of $\iota$ spans $U \otimes V$, i.e. $\span{\im{\iota}} = U \otimes V$. To know $\bar{\phi}$, it suffices to know $\bar{\phi}(u \otimes v) = \phi(u, v)$ for all $u \in U$ and $v \in V$.
\end{remark}

We can talk about the tensor product of $k$ linear spaces with $k \geq 2$. Moreover, the tensor product is associative and commutative up to isomorphism, i.e., $V_1 \otimes V_2 \otimes V_3 \cong (V_1 \otimes V_2) \otimes V_3 \cong V_1 \otimes (V_2 \otimes V_3)$ and $V_1 \otimes V_2 \cong V_2 \otimes V_1$. Both of them are natural isomorphisms.

\begin{center}
    \begin{tikzcd}[row sep=normal]
        V_1 \times V_2 \times V_3 \arrow[r] \arrow[d] & V_1 \otimes V_2 \otimes V_3 \\
        (V_1 \otimes V_2) \times V_3 \arrow[r] & (V_1 \otimes V_2) \otimes V_3 \arrow[u, "\vequiv" description, phantom]
    \end{tikzcd}
    \qquad
    \begin{tikzcd}[row sep=normal]
        V_1 \times V_2 \arrow[r] \arrow[d, leftrightarrow] & V_1 \otimes V_2 \\
        V_2 \times V_1 \arrow[r] & V_2 \otimes V_1 \arrow[u, "\vequiv" description, phantom]
    \end{tikzcd}
\end{center}

We have a natural equivalence:
\[
    \Hom(U, V \otimes W) \equiv \Hom(U, V) \otimes W
\]
Then we can prove that $\Hom(V_1, V_2) \equiv V_1^* \otimes V_2$ and $(V_1 \otimes V_2)^* \equiv V_1^* \otimes V_2^*$. Also, we have the following equation, by considering $V_1 \otimes V_2 \equiv \Hom(V_1^*, V_2)$:
\[
    \dim(V \otimes W) = \dim(V) \cdot \dim(W)
\]
If $e$ is a minimal spanning set of $V_1$ and $f$ is a minimal spanning set of $V_2$, then $e \otimes f$ is a minimal spanning set of $V_1 \otimes V_2$. Moreover, we have $\End{V} \equiv (\End{V})^*$ and the identity map $1_V$ corresponds to the \emph{trace} map $\tr : \End{V} \to \F$ under this identification.

We also have the distribution of tensor product over direct sum: $V_1 \otimes (V_2 \oplus V_3) \equiv (V_1 \otimes V_2) \oplus (V_1 \otimes V_3)$. Moreover, $\Hom(V_1, V_2 \oplus V_3) \equiv \Hom(V_1, V_2) \oplus \Hom(V_1, V_3)$ and $\Hom(V_1 \oplus V_2, V_3) \equiv \Hom(V_1, V_3) \times \Hom(V_2, V_3)$.

\newpage

\section{Algebras}

\begin{definition}[Algebras]
    An \emph{algebra} over a field $\F$ is a linear space $A$ over $\F$ equipped with a bilinear product map $A \times A \to A$, or equivalently a linear map $A \otimes A \to A$.
\end{definition}

\begin{example}
    The set of all polynomials in $t$ with coefficients in $\F$, denoted $\F[t]$, is an algebra over $\F$. As $\F[t] \times \F[t] \to \F[t]$ defined by $(f, g) \mapsto fg$ is a bilinear map. Moreover, $\F[t]$ has a multiplicative identity $1 \in \F[t]$, $fg = gf$ for all $f, g \in \F[t]$, and $(fg)h = f(gh)$ for all $f, g, h \in \F[t]$. So $\F[t]$ is a unital commutative associative algebra over $\F$.
\end{example}

\begin{example}
    The set of all square matrices with order $n$ over $\F$, denoted by $\M{n \times n}{\F}$, is an algebra over $\F$. As $\M{n \times n}{\F} \times \M{n \times n}{\F} \to \M{n \times n}{\F}$ defined by $(A, B) \mapsto AB$ is a bilinear map. Moreover, $\M{n \times n}{\F}$ has a multiplicative identity $I_n \in \M{n \times n}{\F}$, $(AB)C = A(BC)$ for all $A, B, C \in \M{n \times n}{\F}$. However, in general $AB \neq BA$ for some $A, B \in \M{n \times n}{\F}$. So $\M{n \times n}{\F}$ is a unital associative algebra but it is a non-commutative algebra over $\F$.
\end{example}

\begin{example}
    The 3-dimensional Euclidean space $\R^3$ with the cross product $\times : \R^3 \times \R^3 \to \R^3$ is an algebra over $\R$. As the cross product is bilinear. However, it does not have a multiplicative identity, not associative and not commutative. So $\R^3$ with the cross product is a non-unital non-associative non-commutative algebra over $\R$.
\end{example}
\begin{remark}
    $(\R^3, \times)$ is an example of a simple real lie algebra. It is the lie algebra of the lie group SO(3), the special orthogonal group in dimension 3, i.e., the 3-dimensional rotations. $(\R^3, \times)$ is denoted by $\mathfrak{so}(3)$. Also, it is the lie algebra of the infinitesimal symmetries of a pointed 3-dimensional Euclidean space. 
\end{remark}

\begin{definition}[Lie Algebras]
    An algebra is $\mathfrak{g}$ over a field $\F$ is called a \emph{lie algebra} if the \emph{lie bracket} or lie product $[-, -] : \mathfrak{g} \times \mathfrak{g} \to \mathfrak{g}$ satisfies the following two conditions:
    \begin{itemize}
        \item Skew-symmetry: $[x, x] = 0$ for all $x \in \mathfrak{g}$, i.e., $[x, y] = -[y, x]$ for all $x, y \in \mathfrak{g}$ if $\chart(\F) \neq 2$;
        \item Jacobi Identity: $[x, [y, z]] + [y, [z, x]] + [z, [x, y]] = 0$ for all $x, y, z \in \mathfrak{g}$.
    \end{itemize}
\end{definition}

\begin{definition}[Graded Linear Space]
    A linear space $V_{\smallbullet}$ over $\F$ is called a \emph{$\Z_{\geq 0}$-graded linear space} or \emph{graded vector space} if it is a direct sum of linear subspaces $V_n$ for all $n \in \Z_{\geq 0}$:
    \[
        V_{\smallbullet} = \bigoplus_{n = 0}^{\infty} V_n
    \]
    The elements in $V_n$ are called \emph{homogeneous elements} of degree $n$. If $v \in V_n$ is a homogeneous element, we write $\deg(v) = n$.
\end{definition}

\begin{definition}[Graded Linear Maps]
    A linear map $\phi : V_{\smallbullet} \to W_{\smallbullet}$ is called a \emph{graded linear map} with graded degree $k \geq 0$ if $\phi(V_n) \subseteq W_{n + k}$ for all $n \in \Z_{\geq 0}$.
\end{definition}

\newpage

\section{Tensor Algebras}

Let $V$ be a finite dimensional linear space over $\F$. We define a new notation:
\[
    V^{\otimes k} = \underbrace{V \otimes V \otimes \cdots \otimes V}_{k \text{ times}}
\]
for all $k \geq 0$. Note that $V^{\otimes 0} = \F$. Also, $\dim(V^{\otimes k}) = (\dim{V})^k$ for all $k \geq 0$.

We define the \emph{tensor algebra} of $V$ over $\F$, denoted by $\T V$, as follows:
\[
    \T V = \bigoplus_{k = 0}^{\infty} V^{\otimes k} = \F \oplus V \oplus (V \otimes V) \oplus (V \otimes V \otimes V) \oplus \cdots
\]

The tensor algebra $\T V$ is an algebra over $\F$ with the bilinear product map defined by the tensor product:
\[
    \otimes : \T V \times \T V \to \T V
\]
which sends $(\sum_n u_n, \sum_m v_m)$ to $\sum_{n, m} (u_n \otimes v_m)$.
\begin{remark}
    As the algebra product is bilinear, it suffices to know the product of two homogeneous elements, i.e., $V^{\otimes n} \times V^{\otimes m} \to \T V$ for all $n, m \geq 0$. So $\T V$ is a $\Z_{\geq 0}$-graded algebra over $\F$. As the tensor algebra is bi-additive, we have the following equality:
    \[
        \sum_n u_n \otimes \sum_m v_m = \sum_n (u_n \otimes \sum_m v_m) = \sum_n \sum_m (u_n \otimes v_m) = \sum_{n, m} (u_n \otimes v_m)
    \]
\end{remark}

Then to define the bilinear product above, we have to define the tensor product of two homogeneous elements:
\begin{center}
    \begin{tikzcd}[row sep=normal, column sep=normal]
        V^{\otimes n} \times V^{\otimes m} \arrow[rr] \arrow[dd] & & \T V \\
        & V^{\otimes (n + m)} \arrow[ur, hook] & \\
        V^{\otimes n} \otimes V^{\otimes m} \arrow[ur, dashed]
    \end{tikzcd}
\end{center}

We have to prove the existence of the bilinear map $V^{\otimes n} \times V^{\otimes m} \to V^{\otimes (n + m)}$ for all $n, m \geq 0$. We can prove it by the following commutative diagram:
\begin{center}
    \begin{tikzcd}[row sep=normal, column sep=normal]
        V^{\otimes n} \times V^{\otimes m} \arrow[r] & \overbrace{V \otimes \cdots \otimes V}^{n \text{ times}} \otimes \overbrace{V \otimes \cdots \otimes V}^{m \text{ times}} \\
        \underbrace{(V \times \cdots \times V)}_{n \text{ times}} \times \underbrace{(V \times \cdots \times V)}_{m \text{ times}} \arrow[u] & \\
        \underbrace{V \times \cdots \times V}_{n + m \text{ times}} \arrow[u] \arrow[uur, "\phi" swap] \arrow[r] & \underbrace{V \otimes \cdots \otimes V}_{n + m \text{ times}} \arrow[uu, "\exists ! \bar{\phi}" swap, dashed]
    \end{tikzcd}
\end{center}
The proof used a lot of universal properties of tensor products. Note that the map $\phi$ is a multilinear map and $\bar{\phi}$ is a linear equivalence.

So we have proved the existence of the bilinear product map $\otimes : \T V \times \T V \to \T V$. Then $\T V$ is an algebra over $\F$.
\begin{remark}
    The tensor algebra $(\T V, \otimes)$ is a graded unital associative algebra over $\F$. It is graded, as it is degree additive, i.e., $V^{\otimes n} \times V^{\otimes m} \to V^{\otimes (n + m)}$ for all $n, m \geq 0$. It is unital, as $V^{\otimes 0} - \F \times V^{\otimes m} \to V^{\otimes m}$ and the reverse. The multiplicative identity is $1 \in \F$. It is associative, as $(u \otimes v) \otimes w = u \otimes (v \otimes w)$ for all $u, v, w \in V$ and the associativity can be extended to all homogeneous elements by bi-additivity. However, in general it is not commutative, as $u \otimes v \neq v \otimes u$ for some $u, v \in V$.
\end{remark}

There is a universal property of tensor algebras. Consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & A^{\smallbullet} \\
        \T V \arrow[ur, dashed, "\exists ! \bar{\phi}" swap]
    \end{tikzcd}
\end{center}
Note that $V = V^{\otimes 1} = 0 \oplus V \oplus 0 \oplus \cdots \subseteq \T V$ and $\iota$ is the inclusion map. Here $A^{\smallbullet}$ is any graded unital associative algebra over $\F$ and $\phi : V \to A^{\smallbullet}$ is a graded linear map with graded degree $0$. Then there exists a unique graded algebra homomorphism with graded degree $0$ $\bar{\phi} : \T V \to A^{\smallbullet}$ such that $\bar{\phi} \circ \iota = \phi$. This shows the universal property of tensor algebras. More specifically, the map $\phi$ is a map from $V$ to the degree $1$ part of $A^{\smallbullet}$, i.e., $\phi : V \to A_1$, then with an inclusion map.

The tensor algebra construction is actually a functor from $\Vect_{\F}$ to the category of graded unital associative algebras over $\F$, denoted by $\Z_{\geq 0} - \Alg_{\F}$:
\begin{center}
    \begin{tikzcd}[row sep=normal]
        \Vect_{\F} \arrow[r, "\T"] & \Z_{\geq 0} - \Alg_{\F} \\[-1.8em]
        V \arrow[dd, "f" swap] & \T V \arrow[dd, "\T f"] \\
        \arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
        W & \T W
    \end{tikzcd}
\end{center}
where $\T f : \T V \to \T W$ is the unique graded algebra homomorphism with graded degree $0$ such that $\T f \circ \iota_V = \iota_W \circ f$. Here $\iota_V : V \to \T V$ and $\iota_W : W \to \T W$ are the inclusion maps. 

The existence of the functor $\T$ follows from the universal property of tensor algebras. It is called the \emph{free graded algebra functor}, normally the ``unital'' and ``associative'' will be omitted.

\newpage

\section{Quotient Algebras}

In this section, we will discuss three quotient algebras associated with a vector space $V$: the symmetric algebra, exterior algebra, and universal enveloping algebra.

Before that we need to introduce the concept of ideals in algebras.

\begin{definition}[Ideals of Algebras]
    An \emph{ideal} of an algebra $A$ over a field $\F$ is a non-empty subset $I$ of $A$ which is closed under linear combinations and algebra multiplications by elements in $A$. That is, for all $x, y \in I$, $\alpha, \beta \in \F$ and $a \in A$, we have:
    \begin{itemize}
        \item $\alpha x + \beta y \in I$;
        \item $ax \in I$ and $xa \in I$.
    \end{itemize}
\end{definition}

Simply speaking, an ideal is a generalisation of a rule to an algebra.

The following is an example of an ideal in a ring, which is an example of ideal in a more general concept.
\begin{example}
    Consider the ring of integers, $\Z$. The set of all $n$-multiples, denoted by $n\Z$, is an ideal of $\Z$ for all $n \in \Z$. As it is closed under addition and multiplication by any integer.
\end{example}

\subsection{Symmetric Algebras}

The \emph{symmetric algebra} of a vector space $V$ over a field $\F$, denoted by $\Sym V$, is defined as the quotient algebra of the tensor algebra $\T V$ by the ideal of $\T V$ generated by elements of the form $u \otimes v - v \otimes u$ for all $u, v \in V$:
\[
    \Sym V = \quotient{\T V}{\ideal_{\Sym}} = \quotient{\T V}{\langle u \otimes v - v \otimes u \mid u, v \in V \rangle}
\]
The $\ideal_{\Sym}$ is called the \emph{symmetrising ideal} of $\T V$. It is actually the \emph{ideal completion} of the relation $u \otimes v = v \otimes u$ for all $u, v \in V$. We use $\langle - \rangle$ to denote the ideal generated by a set.

Then the elements in $\Sym V$ are equivalence classes of elements in $\T V$. We have $uv \in \Sym V$ as the equivalence class of $u \otimes v \in \T V$ denoted by $[u \otimes v]$. Note that $uv = [u \otimes v] = [v \otimes u] = vu$ in $\Sym V$, as $[u \otimes v - v \otimes u] = 0$. So the product in $\Sym V$ is commutative. 
\begin{remark}
    Symmetric algebra is still a graded algebra. As the ideal $\ideal_{\Sym}$ is a graded ideal, i.e., $\ideal_{\Sym} = \bigoplus_{k = 0}^{\infty} (\ideal_{\Sym} \cap V^{\otimes k})$.
\end{remark}

Similar to tensor algebras, we have the following expression:
\[
    \Sym V = \bigoplus_{k = 0}^{\infty} \mathcal{S}^k V
\]
where $\mathcal{S}^k V$ is the $k$-th symmetric power of $V$.

We also have the following universal property of symmetric algebras. Consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}[row sep = normal]
        V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & A^{\smallbullet} \\
        \T V \arrow[d, "\pi" swap, two heads] \arrow[ur, dashed] & \\
        \Sym V \arrow[uur, dashed, "\exists ! \bar{\phi}" swap]
    \end{tikzcd}
\end{center}
Here $A^{\smallbullet}$ is any graded unital commutative associative algebra over $\F$.

Similarly, $\Sym V$ is the \emph{free graded commutative algebra functor} from $\Vect_{\F}$ to the category of graded unital commutative associative algebras over $\F$, denoted by $\Z_{\geq 0} - \CAlg_{\F}$:

\subsection{Exterior Algebras}

The \emph{exterior algebra} of a vector space $V$ over a field $\F$, denoted by $\Ext V$, is defined as the quotient algebra of the tensor algebra $\T V$ by the ideal of $\T V$ generated by elements of the form $v \otimes v$ for all $v \in V$:
\[
    \Ext V = \quotient{\T V}{\ideal_{\Ext}} = \quotient{\T V}{\langle v \otimes v \mid v \in V \rangle} = \quotient{\T V}{\langle v \otimes w + w \otimes v \mid v, w \in V \rangle}
\]
The $\ideal_{\Ext}$ is called the \emph{alternating ideal} of $\T V$. It is actually the \emph{ideal completion} of the relation $v \otimes v = 0$ for all $v \in V$, or equivalently $v \otimes w = - w \otimes v$ for all $v, w \in V$. Sometimes the exterior algebra is also called the \emph{skew-symmetric algebra}. Note that the characteristic of the field $\F$ should not be $2$, i.e., $\chart(\F) \neq 2$, otherwise $v \otimes w = - w \otimes v$ implies that $v \otimes w = w \otimes v$.

Then the product in $\Ext V$ is called the \emph{exterior product} or \emph{wedge product}, denoted by $\wedge$. We have $u \wedge v = - v \wedge u$ in $\Ext V$ for all $u, v \in V$. So the product in $\Ext V$ is skew-commutative.
\begin{remark}
    Exterior algebra is still a graded algebra. As the ideal $\ideal_{\Ext}$ is a graded ideal, i.e., $\ideal_{\Ext} = \bigoplus_{k = 0}^{\infty} (\ideal_{\Ext} \cap V^{\otimes k})$.
\end{remark}

Then we have the following expression:
\[
    \Ext V = \bigoplus_{k = 0}^{\infty} {\bigwedge}^k V
\]
where ${\bigwedge}^k V$ is the $k$-th exterior power of $V$.

We also have the following universal property of exterior algebras. Consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}[row sep = normal]
        V \arrow[r, "\forall \phi"] \arrow[d, "\iota" swap, hook] & A^{\smallbullet} \\
        \T V \arrow[d, "\pi" swap, two heads] \arrow[ur, dashed] & \\
        \Ext V \arrow[uur, dashed, "\exists ! \bar{\phi}" swap]
    \end{tikzcd}
\end{center}
Here $A^{\smallbullet}$ is any graded unital associative skew-commutative algebra over $\F$.

Similarly, $\Ext V$ is the \emph{free graded skew-commutative algebra functor} from $\Vect_{\F}$ to the category of graded unital associative skew-commutative algebras over $\F$, denoted by $\Z_{\geq 0} - \SAlg_{\F}$:

\subsection{Universal Enveloping Algebras}

Let $\mathfrak{g}$ be a lie algebra over a field $\F$. The \emph{universal enveloping algebra} of $\mathfrak{g}$ over $\F$, denoted by $\Env \mathfrak{g}$, is defined as the quotient algebra of the tensor algebra $\T \mathfrak{g}$ by the ideal of $\T \mathfrak{g}$ generated by elements of the form $x \otimes y - y \otimes x - [x, y]$ for all $x, y \in \mathfrak{g}$:
\[
    \Env \mathfrak{g} = \quotient{\T \mathfrak{g}}{\ideal_{\Env}} = \quotient{\T \mathfrak{g}}{\langle x \otimes y - y \otimes x - [x, y] \mid x, y \in \mathfrak{g} \rangle}
\]
The $\ideal_{\Env}$ is called the \emph{lie ideal} of $\T \mathfrak{g}$. It is actually the \emph{ideal completion} of the relation $xy - yx = [x, y]$ for all $x, y \in \mathfrak{g}$.
\begin{remark}
    However, the universal enveloping algebra is not a graded algebra. As the ideal $\ideal_{\Env}$ is not a graded ideal. The $x \otimes y - y \otimes x$ is in $\mathfrak{g}^{\otimes 2}$ but $[x, y]$ is in $\mathfrak{g}^{\otimes 1}$.
\end{remark}

\newpage

\section{Hilbert-Poincar Series}
Let $V_{\smallbullet} = \bigoplus_{i \geq 0} V_i$ be a $\Z_{\geq 0}$-graded finite dimensional linear space over a field $\F$. The \emph{Hilbert-Poincar series} of $V_{\smallbullet}$ is defined as the following formal power series:
\[
    P_{V_{\smallbullet}}(t) = \sum_{i = 0}^{\infty} \dim(V_i)\ t^i
\]

\begin{example}
    The Hilbert-Poincar series of the tensor algebra $\T V$ is:
    \[
        P_{\T V}(t) = \sum_{i = 0}^{\infty} \dim(V^{\otimes i})\ t^i = \sum_{i = 0}^{\infty} (\dim{V})^i\ t^i = \frac{1}{1 - \dim{V}\ t}
    \]
\end{example}

\begin{example}
    The Hilbert-Poincar series of the symmetric algebra $\Sym V$ is:
    \[
        P_{\Sym V}(t) = \sum_{i = 0}^{\infty} \dim(\mathcal{S}^i V)\ t^i = \frac{1}{(1 - t)^{\dim{V}}}
    \]
\end{example}

\begin{example}
    The Hilbert-Poincar series of the exterior algebra $\Ext V$ is:
    \[
        P_{\Ext V}(t) = \sum_{i = 0}^{\infty} \dim({\bigwedge}^i V)\ t^i = (1 + t)^{\dim{V}}
    \]
\end{example}

As the Hilbert-Poincar series of the exterior algebra $\Ext V$ is a polynomial of degree $\dim{V}$, we have ${\bigwedge}^k V = 0$ for all $k > \dim{V}$. Especially, if $\dim{V} = n$, then ${\bigwedge}^n V$ is 1-dimensional and ${\bigwedge}^{n + 1} V = 0$. This is because any $(n + 1)$ vectors in an $n$-dimensional vector space are linearly dependent, so the exterior product of them is $0$. Moreover, $\dim({\bigwedge}^k V) = \dim({\bigwedge}^{n - k} V)$ for all $0 \leq k \leq n$.

Any one-dimensional linear space is called a \emph{line}.
\chapter{Determinants}

\epigraph{``If you are willing to prove it, you can prove it. There is no trick.''}{Guowu Meng}

\section{Determinant Lines}

We have known that the top exterior power ${\bigwedge}^n V$ of an $n$-dimensional vector space $V$ over a field $\F$ is 1-dimensional. So we can define the following:

\begin{definition}[Determinant Lines]
    The \emph{determinant line} of an $n$-dimensional vector space $V$ over a field $\F$ is defined as the top exterior power of $V$:
    \[
        \det{V} = {\bigwedge}^n V = {\bigwedge}^{\dim{V}} V
    \]
\end{definition}

Note that the $\det = {\bigwedge}^k$ is a functor from the category of vector spaces with $n$-dimensions $\Vect_{\F}^n$ to the category of vector spaces with 1-dimensional, i.e., the category of lines, $\Vect_{\F}^1$ for all $k \geq 0$:
\begin{center}
    \begin{tikzcd}[row sep=normal]
        \Vect_{\F}^n \arrow[r, "{\bigwedge}^n = \det"] & \Vect_{\F}^1 \\[-1.8em]
        V_1 \arrow[dd, "f" swap] & {\bigwedge}^n V_1 = \det{V_1} \arrow[dd, "{\bigwedge}^n f = \det{f}"] \\
        \arrow[r, mapsto, shorten <= 2ex, shorten >= 2ex] & \phantom{*} \\
        V_2 & {\bigwedge}^n V_2 = \det{V_2}
    \end{tikzcd}
\end{center}

As $\det$ is a functor, we have the following two properties:
\[
    \det \id_V = \id_{\det V}, \qquad \det fg = \det f \cdot \det g
\]

In particular, if $f \in \End{V}$, then $\det f : \det V \to \det V$ is a multiplication by a scalar in $\F$. So we can identify $\det f$ with a scalar in $\F$. This scalar is called the \emph{determinant} of $f$ and is denoted by $\det f$.

Consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        \F^n \arrow[r, "A'", red] \arrow[dd, bend right=60, "P" swap, ocre] & \F^n \arrow[dd, bend left=60, "P", red] \\
        V \arrow[r, "f"] \arrow[u, "{[-]_{\B'}}"] \arrow[d, "{[-]_{\B}}" swap] & V \arrow[u, "{[-]_{\B'}}" swap] \arrow[d, "{[-]_{\B}}"] \\
        \F^n \arrow[r, "A", ocre] & \F^n
    \end{tikzcd}
\end{center}
where $V$ is an $n$-dimensional vector space over $\F$, $\B$ and $\B'$ are two bases of $V$, $f \in \End{V}$, $A$ and $A'$ are the matrix representations of $f$ under the bases $\B$ and $\B'$ respectively, and $P$ is the change of basis matrix from $\B$ to $\B'$. Then by focusing the red and blue commutative square, we have:
\[
    AP = PA', \qquad A = P A' P^{-1}
\]

Then we have:
\[
    \det A \overset{\text{def}}{=\joinrel=} \det f \overset{\text{def}}{=\joinrel=} \det A'
\]

In ordinary linear algebra, $A$ and $A'$ are called \emph{similar matrices}, i.e., $A \sim A'$. This means they represent the same endomorphism, so they have the same determinant.

\newpage

\section{Permutation Groups}

Before we derive the explicit formula of determinants, we need to introduce the concept of permutation groups.

\begin{definition}[Automorphisms]
    An \emph{automorphism} is an isomorphism from a mathematical object to itself.
\end{definition}

\begin{definition}[Automorphism Groups]
    The set of all automorphisms on a mathematical object $X$ forms a group under the composition of functions, denoted by $\Aut(X)$.
\end{definition}

\begin{example}
    The general linear group $\GL(V)$ of a vector space $V$ over $\F$ is the group of all invertible linear maps from $V$ to $V$, i.e., $\GL(V) = \Aut(V)$. The group operation is the composition of functions.
\end{example}

\begin{example}
    The general linear group $\GL_n(\F)$ of degree $n$ over $\F$ is the group of all invertible $n \times n$ matrices over $\F$, i.e., $\GL_n(\F) = \Aut(\F^n)$. The group operation is the matrix multiplication. Note that $\GL_n(\F) \cong \GL(\F^n)$. Also note that the group is not abelian if $n \geq 2$.
\end{example}

\begin{definition}[Permutation Groups]
    A \emph{permutation group} $S_n$ on a set $\underline{n} := \{ 1, 2, \cdots, n \}$ is the group of all bijections from $\underline{n}$ to itself, i.e., $S_n = \Aut(\underline{n})$. It is called the \emph{symmetric group} on $n$ elements. The group operation is the composition of functions.
\end{definition}

Then the order of $S_n$, denoted by $|S_n|$, is $n!$.

\begin{example}
    The permutation group $S_2$ has two elements: the identity permutation $1$ and the transposition $\sigma_1$ defined by $\sigma_1(1) = 2$ and $\sigma_1(2) = 1$. 
\end{example}

Instead of writing $S_2 = \{ 1, \sigma_1 \}$, we can write $S_2 = \langle \sigma_1 \mid \sigma_1^2 = 1 \rangle$, where $\sigma_1$ is called the \emph{generator} of $S_2$ and $\sigma_1^2 = 1$ is called the \emph{relation} of $S_2$. This is called the \emph{presentation} of $S_2$.

In general, the generator $\sigma_i$ of $S_n$ is defined by:
\[
    \sigma_i(j) = \begin{cases}
        j + 1, & j = i \\
        j - 1, & j = i + 1 \\
        j, & \text{otherwise}
    \end{cases} = (i \quad i + 1)
\]

\begin{example}
    The generator $\sigma_1$ of $S_3$ can be represented by the following diagram:
    \begin{center}
        \begin{tikzcd}
            1 \arrow[dr] & 2 \arrow[dl, crossing over] & 3 \arrow[d] \\
            2 & 1 & 3
        \end{tikzcd}
    \end{center}
    It can also be written as $\sigma_1 = (1 \quad 2)$ or $(1 \quad 2)(3)$ or $\begin{pmatrix}
        1 & 2 & 3 \\
        2 & 1 & 3
    \end{pmatrix}$.
\end{example}

Moreover, we have a cycle with 3 elements denoted as $(1 \quad 2 \quad 3)$ defined by the $\begin{pmatrix}
    1 & 2 & 3 \\
    2 & 3 & 1
\end{pmatrix}$.

Then the presentation of $S_3$ is:
\[
    S_3 = \langle \sigma_1, \sigma_2 \mid \sigma_1^2 = 1, \sigma_2^2 = 1, \sigma_1 \sigma_2 \sigma_1 = \sigma_2 \sigma_1 \sigma_2 \rangle
\]

In general, the presentation of $S_n$ is:
\[
    S_n = \langle \sigma_1, \sigma_2, \cdots, \sigma_{n - 1} \mid \sigma_i^2 = 1, \sigma_i \sigma_j = \sigma_j \sigma_i\ (|i - j| > 1), \sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1} \rangle
\]

The last two relations are called the \emph{braid relations}:
\begin{itemize}
    \item \emph{Far commutativity:} $\sigma_i \sigma_j = \sigma_j \sigma_i$ for all $|i - j| > 1$;
    \item \emph{Braid relation:} $\sigma_i \sigma_{i + 1} \sigma_i = \sigma_{i + 1} \sigma_i \sigma_{i + 1}$.
\end{itemize}

The permutation group $S_n$ is generated by quotienting the braid group $B_n$ by the relations $\sigma_i^2 = 1$ for all $1 \leq i \leq n - 1$. We call $B_n$ the \emph{braid group} on $n$ strands. A simple way to visualise the braid group is to think about braiding $n$ strands of hair. The braid group $B_n$ has the same presentation as $S_n$ except that there is no relation $\sigma_i^2 = 1$ for all $1 \leq i \leq n - 1$. Consider the following diagrams:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        1 \arrow[d, dash] & 2 \arrow[d, dash] \\
        1 & 2
    \end{tikzcd}
    \qquad
    $\xLongrightarrow{\sigma_1}$
    \qquad
    \begin{tikzcd}[column sep=normal]
        1 \arrow[dr, dash, start anchor=south, end anchor=north] & 2 \arrow[dl, crossing over, dash, start anchor=south, end anchor=north] \\
        2 & 1
    \end{tikzcd}
    \qquad
    $\xLongrightarrow{\sigma_1}$
    \qquad
    \begin{tikzcd}[column sep=normal, row sep=normal]
        1 \arrow[dr, dash, start anchor=south, end anchor=center] & 2 \arrow[dl, crossing over, dash, start anchor=south, end anchor=center] \\
        \phantom{2} \arrow[dr, dash, start anchor=center, end anchor=north] & \phantom{1} \arrow[dl, crossing over, dash, start anchor=center, end anchor=north, shorten=1cm] \arrow[dl, dash, start anchor=center, end anchor=north] \\
        1 & 2
    \end{tikzcd}
\end{center}

Consider the following exact sequence:
\begin{center}
    \begin{tikzcd}
        1 \arrow[r] & A_n \arrow[r, hook] & S_n \arrow[r, "\sgn", two heads] & \{ \pm 1 \} \arrow[r] & 1
    \end{tikzcd}
\end{center}
where $A_n$ is the \emph{alternating group} on $n$ elements, i.e., the subgroup of $S_n$ consisting of all even permutations, and $\sgn : S_n \to \{ \pm 1 \}$, the \emph{sign homomorphism}, is the unique group homomorphism such that $\sgn(\sigma_i) = -1$ for all $1 \leq i \leq n - 1$. Note that $\ker{\sgn} = A_n$ and $\im{\sgn} = \{ \pm 1 \}$. 
\begin{remark}
    $A_n$ is simple for all $n \geq 5$. This means that $A_n$ has no non-trivial normal subgroups for all $n \geq 5$.
\end{remark}

Then we have two properties of the sign homomorphism:
\begin{itemize}
    \item $\sgn(1) = 1$;
    \item $\sgn(\sigma \tau) = \sgn(\sigma) \cdot \sgn(\tau)$ for all $\sigma, \tau \in S_n$.
\end{itemize}

\newpage

\section{Universal Property of Exterior Powers}

We have known that the $k$-th exterior power ${\bigwedge}^k V$ of a vector space $V$ over a field $\F$ is the quotient of the $k$-th tensor power $V^{\otimes k}$ by the alternating ideal. So, consider $\dim{V} = n$, we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}[row sep = normal]
        \overbrace{V \times V \times \cdots \times V}^{n \text{ times}} \arrow[r, "\forall \phi"] \arrow[d, "\iota"', hook] & Z \\
        V^{\otimes n} \arrow[d, "\pi"', two heads] \arrow[ur, dashed] \\
        {\bigwedge}^n V \arrow[uur, dashed, "\exists ! \bar{\phi}" swap]
    \end{tikzcd}
\end{center}
Here $Z$ is any vector space over $\F$ and $\phi : V \times V \times \cdots \times V \to Z$ is an alternating (skew-symmetric) multilinear map, i.e., $\phi(v_1, v_2, \cdots, v_n) = 0$ if $v_i = v_j$ for some $i \neq j$. Then there exists a unique linear map $\bar{\phi} : {\bigwedge}^n V \to Z$ such that $\bar{\phi} \circ \pi \circ \iota = \phi$. This shows the universal property of exterior powers.

Also, we can consider the ${\bigwedge}^k$ as a functor applied to the map $f : V \to W$. Then we have ${\bigwedge}^k f : {\bigwedge}^k V \to {\bigwedge}^k W$. Then the following diagram commutes:
\begin{center}
    \begin{tikzcd}
        \overbrace{V \times V \times \cdots \times V}^{k \text{ times}} \arrow[r, "f \times f \times \cdots \times f"] \arrow[d, hook] & \overbrace{W \times W \times \cdots \times W}^{k \text{ times}} \arrow[d, hook] \\
        {\bigwedge}^k V \arrow[r, "{\bigwedge}^k f", dashed] & {\bigwedge}^k W \\[-3.6em]
        {\scriptscriptstyle \vec{v}_1 \wedge \vec{v}_2 \wedge \cdots \wedge \vec{v}_k} \arrow[r, mapsto] & {\scriptscriptstyle f(\vec{v}_1) \wedge f(\vec{v}_2) \wedge \cdots \wedge f(\vec{v}_k)}
    \end{tikzcd}
\end{center}

Note that the permutation group $S_n$ acts on $\overbrace{V \times V \times \cdots \times V}^{n \text{ times}}$ by:
\[
    \sigma_i : (v_1, v_2, \cdots, v_n) \mapsto (v_1, v_2, \cdots, v_{i - 1}, v_{i + 1}, v_i, v_{i + 2}, \cdots, v_n)
\]
By the universal property of exterior powers, we have:
\begin{center}
    \begin{tikzcd}
        \overbrace{V \times V \times \cdots \times V}^{n \text{ times}} \arrow[r, "\sigma_i"] \arrow[d] & \overbrace{V \times V \times \cdots \times V}^{n \text{ times}} \arrow[d] \\
        {\bigwedge}^n V \arrow[r, "(\sigma_i)_*", dashed] & {\bigwedge}^n V
    \end{tikzcd}
\end{center}

Consider that $a \wedge b = - b \wedge a$. Then in general, we have:
\[
    P \wedge Q = (-1)^{pq} Q \wedge P
\]
where $P \in {\bigwedge}^p V$ and $Q \in {\bigwedge}^q V$. This is called the \emph{graded commutativity} of exterior algebras.

\newpage

\section{Determinants and Duals}

Let $V$ be an $n$-dimensional vector spaces over $\F$ and $\B_V = \{ v_1, v_2, \cdots, v_n \}$ be a basis of $V$. 

As $\det V$ is a 1-dimensional vector space, so there is a basis. So the basis of $\det V$ is actually equivalent to $\det V \setminus \{ 0 \}$ Then we have a map from $\B_V$ to $\B_{\det V}$ defined by:
\[
    \vec{v} = (v_1, v_2, \cdots, v_n) \mapsto v_1 \wedge v_2 \wedge \cdots \wedge v_n = \det \vec{v} \in \det V
\]

Then we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        \B_{V^*} \arrow[r, "\equiv" description, phantom] \arrow[d] & \B_V \arrow[d] \\
        \B_{\det V^*} \arrow[r, "\equiv" description, phantom] & \B_{\det V}
    \end{tikzcd}
\end{center}

Note that $(\det v)^* \equiv \det v$ where $v \in \B_V$. So we have the following equivalence:
\[
    \det v^* \equiv \det v \equiv (\det v)^*
\]
The first equivalence is because of the commutative diagram above, and the second equivalence is because of the definition of dual basis.

Consider $L$ be a line over $\F$ and $L^n$ defined as $\overbrace{L \otimes L \otimes \cdots \otimes L}^{n \text{ times}}$. Also, $L^0$ is defined as $\F$. Normally, we have $L^* \otimes L \to \F$. However, as $L$ is 1-dimensional, we have the following isomorphism:
\[
    L^* \otimes L \equiv \F
\]
Then $L^*$ is regarded as $L^{-1}$, and they from a group under the tensor product operation, $(\{ L^k \}, \otimes)$ where $k \in \Z$.

Consider $V_1$ and $V_2$ are two $n$-dimensional vector spaces over $\F$. Then we have the following diagram:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        V_1 \arrow[rr, "f"] & \arrow[d, Rightarrow, shorten >= 1ex, "\det"'] & V_2 \arrow[r, Rightarrow, "(-)^*"] & V_1^* & \arrow[d, Rightarrow, shorten >= 1ex, "\det"] & V_2^* \arrow[ll, "f^*"'] \\
        \det V_1 \arrow[rr, "\det f"] & \phantom{*} & \det V_2 \arrow[r, dashed, Rightarrow] & \det V_1^* & \phantom{*} & \det V_2^* \arrow[ll, "\det f^*"']
    \end{tikzcd}
\end{center}

Then we consider the left part, we have:
\[
    \det f \in \Hom(\det V_1, \det V_2) \equiv (\det V_1)^* \otimes \det V_2
\]
Similarly, for the right part, we have:
\[
    \det f^* \in (\det V_2^*)^* \otimes \det V_1^* \equiv (\det V_2)^** \otimes \det V_1^* \equiv \det V_2 \otimes (\det V_1)^*
\]
Note that the first equivalence is due to $\det V^* \equiv (\det V)^*$. As the tensor product is commutative, we have:
\[
    \det f^* \equiv \det f
\]

\newpage

\section{Determinant Formula}

Consider the following diagram:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        \F^n \arrow[rr, "A"] & \arrow[d, Rightarrow, shorten >= 1ex, "\det"'] & \F^n \\
        \det \F^n \arrow[rr, "\det A"] & \phantom{*} & \det \F^n 
    \end{tikzcd}
\end{center}
Given the standard basis $\B = \{ \vec{e}_1, \vec{e}_2, \cdots, \vec{e}_n \}$ of $\F^n$, we have:
\[
    \det \B = \vec{e}_1 \wedge \vec{e}_2 \wedge \cdots \wedge \vec{e}_n
\]
Note that 
\[
    A = \begin{bmatrix}
        | & | & & | \\
        \vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_n \\
        | & | & & |
    \end{bmatrix}
\]
Consider the map $\det A : \det \B \mapsto \det A \cdot \det \B$ where $\det A \in \F$ is a scalar, we have
\[
    \det A \cdot \det \B = A\vec{e}_1 \wedge A\vec{e}_2 \wedge \cdots \wedge A\vec{e}_n = \vec{a}_1 \wedge \vec{a}_2 \wedge \cdots \wedge \vec{a}_n
\]
So, we know that $\det A$ is multilinear and alternating in the columns of $A$. Also, $\det I = 1$. 

Consider the elements of $A$ as $\vec{a}_j = \sum_{i_j = 1}^n a_j^{i_j} \vec{e}_{i_j}$ for all $1 \leq j \leq n$. Then we have:
\[
    \vec{a}_1 \wedge \cdots \wedge \vec{a}_n = \sum_{i_1 = 1}^n a_1^{i_1} \vec{e}_{i_1} \wedge \cdots \wedge \sum_{i_n = 1}^n a_n^{i_n} \vec{e}_{i_n} = \sum_{i_1, \cdots, i_n = 1}^n a_1^{i_1} \cdots a_n^{i_n} \ (\vec{e}_{i_1} \wedge \cdots \wedge \vec{e}_{i_n})
\]
We assume that $\vec{e}_{i_k}$ are mutually distinct for all $1 \leq k \leq n$. Otherwise, the term is $0$ because of the alternating property of exterior products. So there exists a unique permutation $\sigma \in S_n$ such that $i_k = \sigma(k)$ for all $1 \leq k \leq n$. Then we have:
\[
    \vec{a}_1 \wedge \cdots \wedge \vec{a}_n = \sum_{\sigma \in S_n} a_1^{\sigma(1)} \cdots a_n^{\sigma(n)} \ (\vec{e}_{\sigma(1)} \wedge \cdots \wedge \vec{e}_{\sigma(n)}) = \sum_{\sigma \in S_n} a_1^{\sigma(1)} \cdots a_n^{\sigma(n)} \ \sgn(\sigma) \ (\vec{e}_1 \wedge \cdots \wedge \vec{e}_n)
\]

Hence, we have the formula of determinants:
\[
    \det A = \sum_{\sigma \in S_n} \sgn(\sigma) \ a_1^{\sigma(1)} a_2^{\sigma(2)} \cdots a_n^{\sigma(n)}
\]
\begin{remark}
    For the magnitude part in the formula, $a_1^{\sigma(1)} a_2^{\sigma(2)} \cdots a_n^{\sigma(n)}$, they are in distinct rows and in distinct columns. They are in distinct columns because of the subscript of $a_j^{\sigma(j)}$ is $j$ for all $1 \leq j \leq n$. They are in distinct rows due to the $\sigma$, otherwise it will be zero because of the alternating property of exterior products.
\end{remark}

\newpage

\section{Properties of Determinants}

The $\det A$ has the following properties:
\begin{itemize}
    \item Linear in each column: for all $1 \leq j \leq n$;
    \item Alternating (skew-symmetric): $\cdots \vec{a}_i \cdots \vec{a}_j \cdots = - \cdots \vec{a}_j \cdots \vec{a}_i \cdots$ for all $i < j$;
    \item $\det I = 1$;
\end{itemize}
For the alternating property, we have the following evaluation from the original definition of wedge products (we assumed that $\chart(\F) \neq 2$):
\[
    \begin{split}
        \cdots \vec{a}_i \overbrace{\color{red} \cdots}^{k \text{ times}} \vec{a}_j \cdots & = (-1)^k \cdots {\color{red} \cdots} \vec{a}_i \vec{a}_j \cdots \\
        & = (-1)^{k + 1} \cdots {\color{red} \cdots} \vec{a}_j \vec{a}_i \cdots \\
        & = - \cdots \vec{a}_j {\color{red} \cdots} \vec{a}_i \cdots
    \end{split}
\]
Moreover, the three properties above uniquely determine the determinant function.
\begin{remark}
    The first two properties can be defined on the rows of $A$ as well and they still hold. This is because the determinant of a matrix is equal to the determinant of its transpose, which is the matrix part of $\det f^* \equiv \det f$ shown in the previous section.
\end{remark}

If we drop the last property, then the function is called the \emph{alternating multilinear form}. Suppose that $\phi : \M{n \times n}{\F} \to \F$ is an alternating multilinear form, then we have:
\[
    \phi(A) = \det A \phi(I_n)
\]

\begin{proposition}
    The following equality holds:
    \[
        \det \begin{bmatrix}
            A_1 & * \\
            0 & A_2
        \end{bmatrix} = \det A_1 \cdot \det A_2
    \]
\end{proposition}
\begin{proof}
    Consider the part on the left-hand side, we know that it is multilinear in the columns and alternating. Then we have the following evaluation:
    \[
        \begin{split}
            \det \begin{bmatrix}
                A_1 & * \\
                0 & A_2
            \end{bmatrix} & = \det A_1 \cdot \det \begin{bmatrix}
                I_{n_1} & * \\
                0 & A_2
            \end{bmatrix} \\
            & = \det A_1 \cdot \det A_2 \cdot \det \begin{bmatrix}
                I_{n_1} & * \\
                0 & I_{n_2}
            \end{bmatrix} \\
            & = \det A_1 \cdot \det A_2 \cdot \det \begin{bmatrix}
                I_{n_1} & 0 \\
                0 & I_{n_2}
            \end{bmatrix} \\
            & = \det A_1 \cdot \det A_2 \cdot \det I_{n_1 + n_2} = \det A_1 \cdot \det A_2
        \end{split}
    \]
    For the last equality, as we know the following property:
    \[
        \cdots \vec{a}_i \cdots (k\vec{a}_i + \vec{a}_j) \cdots = k \cdots \vec{a}_i \cdots \vec{a}_j \cdots + \cdots \vec{a}_i \cdots \vec{a}_j \cdots = \cdots \vec{a}_i \cdots \vec{a}_j \cdots
    \]
    Note that $k$ can be 0 as well. Therefore, we can eliminate all the $*$ in the matrix by using the above property without changing the determinant value.
\end{proof}

Instead of writing $\det$, we can use two pipes to denote the determinant. Concretely, we have the following determinants:
\[
    \left|
        \begin{array}{cc|ccc}
            1 & & * & * & * \\
            & 1 & * & * & * \\
            \hline
            & & 1 & & \\
            & & & 1 & \\
            & & & & 1
        \end{array}
    \right| = \left|
        \begin{array}{cc|ccc}
            1 & & 0 & 0 & 0 \\
            & 1 & * & * & * \\
            \hline
            & & 1 & & \\
            & & & 1 & \\
            & & & & 1
        \end{array}
    \right| = \left|
        \begin{array}{cc|ccc}
            1 & & 0 & 0 & 0 \\
            & 1 & 0 & 0 & 0 \\
            \hline
            & & 1 & & \\
            & & & 1 & \\
            & & & & 1
        \end{array}
    \right| = |I_5| = 1
\]
For the first equality, we eliminated the first row's $*$ by using the first row. For the second equality, we eliminated the second row's $*$ by using the second row.

So, for block upper-triangular matrices, its determinant is equal to the product of the determinants of the diagonal blocks. Same for the block lower-triangular matrices.

In particular, we have the following equation:
\[
    \begin{vmatrix}
        a_{11} & \cdots & * \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & a_{nn}
    \end{vmatrix} = a_{11} \cdots a_{nn}
\]
Also, $\det [a] = a \det[1] = a$.
\begin{remark}
    In determinant, we prefer to use $a_{ij}$ to denote the element in the $i$-th row and $j$-th column instead of using superscript and subscript like $a_j^{i}$. This is because in determinants, we usually consider the rows and columns instead of vectors.
\end{remark}

Consider the following determinant:
\vspace{6ex}
\[
    \begin{split}
        \begin{vmatrix}
            & & & \mypoint{jcol}{\phantom{*}} \\
            & * & & 0 & & * & \\ 
            \\
            \mypoint{irow}{a_{i,1}} & \cdots & a_{i,j-1} & 1 & a_{i,j+1} & \cdots & a_{i,n} \\
            \\
            & * & & 0 & & * & \\
            \\
        \end{vmatrix} & = (-1)^{i - 1} \begin{vmatrix}
            a_{i,1} & \cdots & a_{i,j-1} & 1 & a_{i,j+1} & \cdots & a_{i,n} \\
            \\ \\
            & * & & 0 & & * & \\
            \\ \\
        \end{vmatrix} \\
        & = (-1)^{i - 1 + j - 1} \begin{vmatrix}
            1 & a_{i,1} & \cdots & \widehat{a_{i,j}} & \cdots & a_{i,n} \\
            \\ \\
            0 & & & A^i_j \\
            \\ \\
        \end{vmatrix} \\
        & = (-1)^{i + j} \det A^i_j
    \end{split}
\]
\begin{tikzpicture}[remember picture, overlay]
    \node[above=20pt of jcol](textofhere1){the $j$-th column};
    \draw[myarrow] (textofhere1) -- (jcol);
    \node[left=20pt of irow](textofhere2){the $i$-th row};
    \draw[myarrow] (textofhere2) -- (irow);
\end{tikzpicture}
Here $\widehat{a_{i,j}}$ means that the element $a_{i,j}$ is omitted, and $A^i_j$ is the submatrix obtained by deleting the $i$-th row and $j$-th column of $A$.

Then we can consider general matrix $A$, for any $j$, we have:
\[
    \begin{split}
        \det A & = \det [\cdots \quad \vec{a}_j \quad \cdots] \\
        & = \sum_{i = 1}^n a_j^i \det [\cdots \quad \vec{e}_i \quad \cdots] \\
        & = \sum_{i = 1}^n a_j^i (-1)^{i + j} \det A^i_j
    \end{split}
\]
This is called the \emph{cofactor expansion} or \emph{Laplace expansion} along the $j$-th column. Similarly, we can have the cofactor expansion along the $i$-th row.

Then we have the definition of \emph{adjoint} of a matrix.
\begin{definition}[Adjoint Matrices]
    An \emph{adjoint matrix} of $A$, denoted by $\adj A$, is defined as the matrix whose $(i,j)$-th entry is $(-1)^{i + j} \det A_i^j$.
\end{definition}
\begin{remark}
    Beaware of the notation difference between $A_i^j$ and $A^i_j$. The former means deleting the $j$-th row and $i$-th column, while the latter means deleting the $i$-th row and $j$-th column. Also note that the notation of $\vec{e}_i$ means that the $i$-th row is 1 and other rows are 0 (standard basis vector), which is different from the notation in $A_i^j$ and $A^i_j$. To conclude, the subscript is for columns and the superscript is for rows, except they are in the notation of standard basis vectors.
\end{remark}

\begin{proposition}
    The following equality holds:
    \[
        A \cdot \adj A = \adj A \cdot A = \det A I_n
    \]
    In particular, if $\det A \neq 0$, then $A^{-1} = \frac{1}{\det A} \adj A$.
\end{proposition}
\begin{proof}
    In particular, we just have to show 
    \[
        \sum_{k = 1}^n a^k_j (\adj A)^i_k = \det A \delta^i_j
    \]

    From the previous Laplace expansion, we know:
    \[
        \det A = \sum_{i = 1}^n a_j^i (-1)^{i + j} \det A^i_j = \sum_{i = 1}^n a_j^i (\adj A)_i^j = (A \cdot \adj A)_j^j
    \]
    Then we know that for $i = j$, the equality holds. If $i \neq j$, then we can consider the following determinant:
    \vspace{4ex}
    \[
        \det \begin{vmatrix}
            \\
            \cdots & \mypoint{icol}{\vec{a}_j} & \cdots & \mypoint{jcol}{\vec{a}_j} & \cdots \\
            \\
        \end{vmatrix} = 0
    \]
    \vspace{2ex}

    This means that originally, there are two same columns in the determinant, so its value is zero. Then by the Laplace expansion along the $j$-th column, we have:
    \[
        0 = \sum_{k = 1}^n a_j^k (-1)^{k + j} \det A^k_i = \sum_{k = 1}^n a_j^k (\adj A)_k^i = (A \cdot \adj A)_j^i
    \]
\end{proof}
\begin{tikzpicture}[remember picture, overlay]
    \node[below=20pt of icol](textofhere1){the $i$-th column};
    \draw[myarrow] (textofhere1) -- (icol);
    \node[above=20pt of jcol](textofhere2){the $j$-th column};
    \draw[myarrow] (textofhere2) -- (jcol);
\end{tikzpicture}

To better understand the reason why the equality holds when $i \neq j$, we can consider the following explanation \cite{1404250}. Consider the $3 \times 3$ case:
\[
    \underbrace{\begin{bmatrix}
        A_1^1 & -A_1^2 & A_1^3 \\
        -A_2^1 & A_2^2 & -A_2^3 \\
        A_3^1 & -A_3^2 & A_3^3
    \end{bmatrix}}_{\adj A} \cdot \underbrace{\begin{bmatrix}
        {\color{red} a_1^1} & {\color{ocre} a_2^1} & a_3^1 \\
        {\color{red} a_1^2} & {\color{ocre} a_2^2} & a_3^2 \\
        {\color{red} a_1^3} & {\color{ocre} a_2^3} & a_3^3
    \end{bmatrix}}_{A}
\]
If we multiply the first row of $\adj A$ with the first column of $A$, we have the same result as the Laplace expansion along the first column:
\[
    {\color{red} a_1^1} A_1^1 - {\color{red} a_1^2} A_1^2 + {\color{red} a_1^3} A_1^3 = \begin{vmatrix}
        {\color{red} a_1^1} & a_2^1 & a_3^1 \\
        {\color{red} a_1^2} & a_2^2 & a_3^2 \\
        {\color{red} a_1^3} & a_2^3 & a_3^3
    \end{vmatrix} = \det A = \sum_{k = 1}^3 {\color{red} a_1^k} A_1^k = \sum_{k = 1}^3 {\color{red} a_1^k} (\adj A)_k^1
\]
If we multiply the first row of $\adj A$ with the second column of $A$, we have:
\[
    {\color{ocre} a_2^1} A_1^1 - {\color{ocre} a_2^2} A_1^2 + {\color{ocre} a_2^3} A_1^3 = \begin{vmatrix}
        {\color{ocre} a_2^1} & a_2^1 & a_3^1 \\
        {\color{ocre} a_2^2} & a_2^2 & a_3^2 \\
        {\color{ocre} a_2^3} & a_2^3 & a_3^3
    \end{vmatrix} = 0 = \sum_{k = 1}^3 {\color{ocre} a_2^k} A_1^k = \sum_{k = 1}^3 {\color{ocre} a_2^k} (\adj A)_k^1
\]

\newpage

\section{Vandermonde Determinant}

Consider the following determinant, here the superscript means the power:
\[
    \det V_n = \begin{vmatrix}
        1 & 1 & \cdots & 1 \\
        x_1 & x_2 & \cdots & x_n \\
        \vdots & \vdots & \ddots & \vdots \\
        x_1^{n - 1} & x_2^{n - 1} & \cdots & x_n^{n - 1}
    \end{vmatrix}
\]
Then we consider $x_1, x_2, \cdots, x_{n - 1}$ are fixed and we consider the determinant as a polynomial of $x_n$. Note that the degree of $x_n$ is $n - 1$, and the polynomial is:
\[
    \det V_n = (-1)^{n + 1} | \cdots | + (-1)^{n + 2} x_n | \cdots | + \cdots + (-1)^{n + n} x_n^{n - 1} \begin{vmatrix}
        1 & 1 & \cdots & 1 \\
        x_1 & x_2 & \cdots & x_{n - 1} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_1^{n - 1} & x_2^{n - 1} & \cdots & x_{n - 1}^{n - 1}
    \end{vmatrix}
\]
Also note that if $x_n = x_i$ for some $1 \leq i \leq n - 1$, let say $i = n - 1$, then the determinant becomes:
\[
    \begin{vmatrix}
        1 & 1 & \cdots & 1 & 1 \\
        x_1 & x_2 & \cdots & x_{n - 1} & x_{n - 1} \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        x_1^{n - 1} & x_2^{n - 1} & \cdots & x_{n - 1}^{n - 1} & x_{n - 1}^{n - 1}
    \end{vmatrix} = 0
\]
This means that $x_n - x_i$ is a factor of the polynomial. Therefore, by the fundamental theorem of algebra, we have:
\[
    \det V_n = C \overbrace{(x_n - x_1)(x_n - x_2) \cdots (x_n - x_{n - 1})}^{n - 1 \text{ factors}}
\]
Here $C$ is a constant that does not depend on $x_n$. To find $C$, we can consider the coefficient of $x_n^{n - 1}$. Note that the coefficient of $x_n^{n - 1}$ in the above polynomial expansion is $\det V_{n - 1}$. So $C = \det V_{n - 1}$. Then by induction, we have:
\[
    \det V_n = \prod_{1 \leq i < j \leq n} (x_j - x_i)
\]

\newpage

\section{Feynman Diagram Formula}

Consider the $\chart(\F) = 0$. Let $A$ be an $n \times n$ matrix and $I$ be the identity matrix of order $n$. Then we have the following formula:
\[
    \det (I + tA) = 1 - \tr A \ t + \left(\frac{(\tr A)^2}{2!} - \frac{\tr A^2}{2}\right) t^2 - \cdots + (-1)^n \det A \ t^n
\]
This is called the \emph{Feynman diagram formula}, as it is inspired by Feynman diagrams in quantum field theory. From this formula, the determinant can be expressed by traces.

It is hard to remember the coefficients in the formula. However, we can use the following method to derive them. Consider the following diagram for $t^1$ term:
\begin{center}
    \begin{tikzpicture}
        \draw (0, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
        \path (0.5, 0) node[right, ocre] {$A$};
    \end{tikzpicture}
\end{center}
Here the circle means a trace operation, and the arrow means $A$. So the coefficient is $- \tr A$. 

For $t^2$ term, we have diagram:
\begin{center}
    \begin{tikzpicture}
        \draw (0, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
        \path (0.5, 0) node[right, ocre] {$A$};
        \draw (2, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
        \path (2.5, 0) node[right, ocre] {$A$};

        \draw (5, 0) circle (1cm) node {$-1$}
            [arrow inside={end=stealth,opt={ocre,scale=1}}{0,0.5}];
        \path (6, 0) node[right, ocre] {$A$}
            (4, 0) node[left, ocre] {$A$}
            (5, 1) node {$|$}
            (5, -1) node {$|$};
    \end{tikzpicture}
\end{center}
The left two circles mean $(- \tr A)^2$, and we have to divide by $2!$ because of the symmetry of the two identical circles. The right circle means $- \tr A^2$, but this is a cyclic group of order 2, so we have to divide by $2$. Therefore, the total term for $t^2$ is:
\[
    \frac{(- \tr A)^2}{2!} - \frac{\tr A^2}{2} = \frac{(\tr A)^2}{2!} - \frac{\tr A^2}{2}
\]

For $t^3$ term, we have diagram:
\begin{center}
    \begin{tikzpicture}
        \draw (-2, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
        \path (-1.5, 0) node[right, ocre] {$A$};
        \draw (0, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
        \path (0.5, 0) node[right, ocre] {$A$};
        \draw (2, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
        \path (2.5, 0) node[right, ocre] {$A$};

        \draw (5, 0) circle (1cm) node {$-1$}
            [arrow inside={end=stealth,opt={ocre,scale=1}}{0,0.5}];
        \path (6, 0) node[right, ocre] {$A$}
            (4, 0) node[left, ocre] {$A$}
            (5, 1) node {$|$}
            (5, -1) node {$|$};
        \draw (7.5, 0) circle (0.5cm) node {$-1$}
			[arrow inside={end=stealth,opt={ocre,scale=1}}{0}];
        \path (8, 0) node[right, ocre] {$A$};

        \draw (10.5, 0) circle (1.5cm) node {$-1$}
            [arrow inside={end=stealth,opt={ocre,scale=1}}{0.0833, 0.4167, 0.755}];
        \path (11.8, 0.75) node[right, ocre] {$A$}
            (9.2, 0.75) node[left, ocre] {$A$}
            (10.5, -1.5) node[below, ocre] {$A$}
            (10.5, 1.5) node {$|$}
            (11.8, -0.75) node[rotate=240] {$|$}
            (9.2, -0.75) node[rotate=120] {$|$};
    \end{tikzpicture}
\end{center}
The left three circles mean $(- \tr A)^3$, and we have to divide by $3!$ because of the symmetry of the three identical circles. The second diagram means $(- \tr A)(- \tr A^2)$, and we have to divide by $2$ because of the cyclic group of order $2$ on the bigger circle. The last diagram means $- \tr A^3$, and this is a cyclic group of order 3, so we have to divide by $3$. Therefore, the total term for $t^3$ is:
\[
    \frac{(- \tr A)^3}{3!} + \frac{(- \tr A)(- \tr A^2)}{2} - \frac{\tr A^3}{3} = - \frac{(\tr A)^3}{3!} + \frac{(\tr A)(\tr A^2)}{2} - \frac{\tr A^3}{3}
\]

\chapter{Canonical Forms of Endomorphisms}

\epigraph{``Babies have to survive, so they have the strong desire to learn stuffs. You think you are not good at math because you don't have the strong desire to learn math.''}{Guowu Meng}

\section{Diagonal Forms}

Before, we have studied the canonical matrix representation of linear maps between two different dimension vector spaces. It is natural to ask what is the canonical form of linear maps from a vector space to itself, i.e. endomorphisms. Consider the following diagram:
\begin{center}
    \begin{tikzcd}
        V \arrow[r, "T"] \arrow[d, "{[-]_{\B}}"'] & V \arrow[d, "{[-]_{\B}}"] \\
        \F^n \arrow[d] \arrow[r, "A"] & \F^n \arrow[d] \\
        \F^n \arrow[r, "\bar{A}"] & \F^n
    \end{tikzcd}
\end{center}
As both the domain and codomain are the same vector space, both basis $\B$ are the same. So the matrix representation of $T$ is much more restricted. The $\bar{A}$ is simplest looking matrix repsentation of $T$, but what does it look like? 

Generically, we have the following form:
\[
    \bar{A} = \begin{bmatrix}
        \lambda_1 & & & \\
        & \lambda_2 & & \\
        & & \ddots & \\
        & & & \lambda_n
    \end{bmatrix}
\]
where empty places are filled with zeros. It is called the \emph{diagonal matrix}. Here $\lambda_i$ are the \emph{eigenvalues} of $T$. If such form exists, we say that $T$ is \emph{completely reducible}, or normally say that $T$ is \emph{diagonalisable}. If $T$ is not completely reducible, then we have to consider more complicated forms, which will be discussed later.

Then we have the diagram:
\begin{center}
    \begin{tikzcd}[ampersand replacement=\&]
        \F^n \arrow[r, "A"] \arrow[d, "P^{-1}"', "\cong"] \& \F^n \arrow[d, "P^{-1}", "\cong"'] \\
        \F^n \arrow[r, "D", "{\begin{bmatrix} \lambda_1 & & \\ & \ddots & \\ & & \lambda_n \end{bmatrix}}"'] \& \F^n
    \end{tikzcd}
\end{center}
Here $P$ is the change of basis matrix from the basis that gives $A$ to the basis that gives $D$. Then we have:
\[
    A = P D P^{-1}
\]
We have $A \sim D$, i.e. $A$ is similar to $D$. 

Then we have two questions:
\begin{enumerate}
    \item How do we know whether $T$ is completely reducible?
    \item If $T$ is completely reducible, how can we find $P$ and $D$?
\end{enumerate}

Assume that $D = \begin{bmatrix}
    \lambda_1 I_{n_1} & & \\
    & \ddots & \\
    & & \lambda_k I_{n_k}
\end{bmatrix}$, where $\lambda_i \in \F$ are distinct eigenvalues and $I_{n_i}$ are identity matrices of order $n_i$, $n_i > 0$ and $\sum_{i = 1}^k n_i = n$. For example, we have:
\[
    D = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 2
    \end{bmatrix}
\]
where $\lambda_1 = 1$, $\lambda_2 = 2$, $n_1 = 2$ and $n_2 = 1$.

Then we have the decomposition of $V$:
\[
    V = V_{\lambda_1} \oplus V_{\lambda_2} \oplus \cdots \oplus V_{\lambda_k}
\]
where $V_i = \ker{T - \lambda_i 1_V}$ are the \emph{eigenspaces} of $T$ corresponding to eigenvalues $\lambda_i$. Moreover, we have the decomposition of $\F^n$:
\[
    \F^n = \span{e_1, \cdots, e_{n_1}} \oplus \span{e_{n_1 + 1}, \cdots, e_{n_1 + n_2}} \oplus \cdots \oplus \span{e_{n_1 + \cdots + n_{k-1} + 1}, \cdots, e_{n_1 + \cdots + n_k}}
\]
Note that $\dim V_{\lambda_i} = n_i$ and $\sum_{i = 1}^k n_i = n$.

Then we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}[ampersand replacement=\&, column sep=7.2em]
        V \arrow[d] \arrow[r, "T = \lambda_1 1_{V_{\lambda_1}} \oplus \cdots \oplus \lambda_k 1_{V_{\lambda_k}}"', "{\begin{bmatrix} \lambda_1 1_{V_{\lambda_1}} & & \\ & \ddots & \\ & & \lambda_k 1_{V_{\lambda_k}} \end{bmatrix}}"] \& V \arrow[d] \\
        \F^n \arrow[r, "D", "{\begin{bmatrix} \lambda_1 I_{n_1} & & \\ & \ddots & \\ & & \lambda_k I_{n_k} \end{bmatrix}}"'] \& \F^n
    \end{tikzcd}
\end{center}

In other words, if $T$ is completely reducible, then there are distinct numbers $\lambda_1, \cdots, \lambda_k \in \F$ and a non-trivial decomposition $V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}$ such that $T|_{V_{\lambda_i}} = \lambda_i 1_{V_{\lambda_i}}$ for each $1 \leq i \leq k$, and $T = \lambda_1 1_{V_{\lambda_1}} \oplus \cdots \oplus \lambda_k 1_{V_{\lambda_k}}$. Each non-zero vector $v_i$ in $V_{\lambda_i}$ is an \emph{eigenvector} of $T$ corresponding to eigenvalue $\lambda_i$. This answered the first question.

Then how to find the eigenvalues and eigenspaces? We can consider the following linear map:
\[
    \lambda_i 1_{V_{\lambda_i}} : V_{\lambda_i} \to V_{\lambda_i}, \quad x \mapsto \lambda_i x
\]
Then we have the following equation:
\[
    Tx = \lambda_i x \iff (\lambda_i 1_V - T) x = 0 \iff x \in \ker{\lambda_i 1_V - T}
\]
As $x$ is non-zero, then $(\lambda_i 1_V - T)$ is not injective, i.e. not invertible. Therefore, we have:
\[
    \det (\lambda_i 1_V - T) = 0
\]
So the eigenvalues $\lambda_i$ are exactly the roots of the polynomial $\det (\lambda 1_V - T)$, which is called the \emph{characteristic polynomial} of $T$. Note that $p_T (\lambda) = \det (\lambda 1_V - T)$ is a polynomial of degree $n = \dim V$. Similarly, we can define the characteristic polynomial of a matrix $A$ as $p_A (\lambda) = \det (\lambda I_n - A)$.

For example, consider the following matrix:
\[
    A = \begin{bmatrix}
        1 & 3 \\
        0 & 2
    \end{bmatrix}, \quad \lambda I - A = \begin{bmatrix}
        \lambda - 1 & -3 \\
        0 & \lambda - 2
    \end{bmatrix}, \quad p_A (\lambda) = (\lambda - 1)(\lambda - 2)
\]
The roots of $p_A (\lambda)$ are $1$ and $2$, so the eigenvalues of $A$ are $1$ and $2$. Then we can find the eigenspaces:
\[
    \begin{split}
        V_{\lambda = 1} &= \nul{1 \cdot I - A} = \mathsf{Nul} \begin{bmatrix}
            0 & -3 \\
            0 & -1
        \end{bmatrix} = \mathsf{Nul} \begin{bmatrix}
            0 & 1 \\
            0 & 0
        \end{bmatrix} = \Span \begin{bmatrix}
            1 \\
            0
        \end{bmatrix} \\
        V_{\lambda = 2} &= \nul{2 \cdot I - A} = \mathsf{Nul} \begin{bmatrix}
            1 & -3 \\
            0 & 0
        \end{bmatrix} = \Span \begin{bmatrix}
            3 \\
            1
        \end{bmatrix}
    \end{split}
\]
Then we have:
\[
    A = \begin{bmatrix}
        1 & 3 \\
        0 & 2
    \end{bmatrix} = \begin{bmatrix}
        1 & 3 \\
        0 & 1
    \end{bmatrix} \begin{bmatrix}
        1 & 0 \\
        0 & 2
    \end{bmatrix} \begin{bmatrix}
        1 & 3 \\
        0 & 1
    \end{bmatrix}^{-1} = P D P^{-1}
\]
\begin{remark}
    To find the null space, we first use row operations to reduce the matrix to its row echelon form. Then we consider the number of free variables to find the number of basis vectors in the null space. Then we can let one free variable as $1$ and other free variables as $0$ to find the value of each pivot variable. Repeating this process for each free variable, we can find all basis vectors of the null space.

    For example, for the first matrix above, we have: $0 \cdot 1 + 1 \cdot x_2 = 0 \implies x_2 = 0$. So the null space is $\Span \begin{bmatrix} 1 \\ 0 \end{bmatrix}$. For the second matrix above, we have: $1 \cdot x_1 - 3 \cdot 1 = 0 \implies x_1 = 3$. So the null space is $\Span \begin{bmatrix} 3 \\ 1 \end{bmatrix}$.
\end{remark}

In matrix, we have:
\[
    \begin{bmatrix}
        A\vec{p_1} & \cdots & A\vec{p_n}
    \end{bmatrix} = AP = PD = \begin{bmatrix}
        \lambda_1 \vec{p_1} & \cdots & \lambda_n \vec{p_n}
    \end{bmatrix} \iff A\vec{p_i} = \lambda_i \vec{p_i}
\]

\begin{proposition}
    The following are equivalent:
    \begin{enumerate}
        \item $T$ is completely reducible.
        \item $T = \lambda_1 1_{V_{\lambda_1}} \oplus \cdots \oplus \lambda_k 1_{V_{\lambda_k}}$ for some distinct eigenvalues $\lambda_1, \cdots, \lambda_k$ and non-trivial decomposition $V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}$.
        \item $V$ has an eigenvector basis of $T$, i.e. there exists a basis of $V$ consisting of eigenvectors of $T$.
        \item $\dim V = \sum_{i = 1}^k \dim E_{\lambda_i} (T) = \sum_{i = 1}^k \dim V_{\lambda_i}$, where $\lambda_1, \cdots, \lambda_k$ are the distinct eigenvalues of $T$ and $V_{\lambda_i} = E_{\lambda_i} (T)$ are the eigenspaces of $T$.
    \end{enumerate}
\end{proposition}

Consider the following example:
\begin{example}
    $A = \begin{bmatrix}
        0 & 1 \\
        0 & 0
    \end{bmatrix}$ is not completely reducible. The $p_A (\lambda) = \lambda^2$, so the only eigenvalue is $0$. Then we have:
    \[
        V_{\lambda = 0} = \nul{0 \cdot I - A} = \mathsf{Nul} \begin{bmatrix}
            0 & -1 \\
            0 & 0
        \end{bmatrix} = \Span \begin{bmatrix}
            1 \\
            0
        \end{bmatrix}
    \]
    So there does not exist a eigenvector basis of $A$, as choosing any two vectors in $V_{\lambda = 0}$ will be linearly dependent. Therefore, $A$ is not completely reducible.
\end{example}

\begin{proposition}
    $E_{\lambda_1} + \cdots + E_{\lambda_k}$ is a direct sum.
\end{proposition}
\begin{proof}
    We just need to check if $x_1 + \cdots + x_k = 0$ with $x_i \in E_{\lambda_i}$, then each $x_i = 0$. We can use induction on $k$. For $k = 1$, we have $x_1 = 0 \implies x_1 = 0$. Assume that the statement holds for $k - 1$. Then we have:
    \[
        \begin{cases}
            x_1 + \cdots + x_k = 0 \\
            Tx_1 + \cdots + Tx_k = \lambda_1 x_1 + \cdots + \lambda_k x_k = 0
        \end{cases}
    \]
    Then we subtract $\lambda_k$ times the first equation from the second equation, we have:
    \[
        (\lambda_1 - \lambda_k) x_1 + \cdots + (\lambda_{k - 1} - \lambda_k) x_{k - 1} = 0
    \]
    Given that $\lambda_i$ are distinct, by the induction hypothesis, we have $(\lambda_i - \lambda_k) x_i = 0 \implies E_{\lambda_i} \ni x_i = 0$ for each $1 \leq i \leq k - 1$. Then by the first equation, we have $x_k = 0$. This completed the induction.
\end{proof}

Then we know that the sum of eigenspaces is a direct sum, i.e. $E_{\lambda_1} \oplus \cdots \oplus E_{\lambda_k}$. Then we have:
\[
    \dim V = \sum \dim E_{\lambda_i} (T)
\]

\begin{example}
    Let $A = \begin{bmatrix}
        1 & 0 & 4 \\
        0 & 1 & 3 \\
        0 & 0 & 2
    \end{bmatrix}$. Then we have $p_A (\lambda) = (\lambda - 1)^2 (\lambda - 2)$. The eigenvalues are $1$ and $2$, where $\lambda = 1$ has algebraic multiplicity $2$ and $\lambda = 2$ has algebraic multiplicity $1$. Then we can find the eigenspaces:
    \[
        \begin{split}
            E_{\lambda = 1} (A) &= \nul{1 \cdot I - A} = \mathsf{Nul} \begin{bmatrix}
                0 & 0 & -4 \\
                0 & 0 & -3 \\
                0 & 0 & -1
            \end{bmatrix} = \mathsf{Nul} \begin{bmatrix}
                0 & 0 & 1 \\
                0 & 0 & 0 \\
                0 & 0 & 0
            \end{bmatrix} = \Span \left\{ \begin{bmatrix}
                1 \\
                0 \\
                0
            \end{bmatrix}, \begin{bmatrix}
                0 \\
                1 \\
                0
            \end{bmatrix} \right\} \\
            E_{\lambda = 2} (A) &= \nul{2 \cdot I - A} = \mathsf{Nul} \begin{bmatrix}
                1 & 0 & -4 \\
                0 & 1 & -3 \\
                0 & 0 & 0
            \end{bmatrix} = \Span \begin{bmatrix}
                4 \\
                3 \\
                1
            \end{bmatrix}
        \end{split}
    \]
    Then we have $\dim E_{\lambda = 1} + \dim E_{\lambda = 2} = 2 + 1 = 3 = \dim V$. Therefore, $A$ is completely reducible. Then we can find the diagonalisation:
    \[
        \begin{bmatrix}
            1 & 0 & 4 \\
            0 & 1 & 3 \\
            0 & 0 & 2
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 & 4 \\
            0 & 1 & 3 \\
            0 & 0 & 1
        \end{bmatrix} \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 2
        \end{bmatrix} \begin{bmatrix}
            1 & 0 & 4 \\
            0 & 1 & 3 \\
            0 & 0 & 1
        \end{bmatrix}^{-1} = \begin{bmatrix}
            4 & 1 & 0 \\
            3 & 0 & 1 \\
            0 & 0 & 1
        \end{bmatrix} \begin{bmatrix}
            2 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix} \begin{bmatrix}
            4 & 1 & 0 \\
            3 & 0 & 1 \\
            0 & 0 & 1
        \end{bmatrix}^{-1}
    \]
\end{example}

Completely reducible matrix representations are ``the'' simplest forms of endomorphisms. Note that it is not unique, it is unique up to isomorphism, unless the field is ordered. However, not all endomorphisms are completely reducible. Then we have another term called \emph{semisimple}. These two terms are borrowed from representation theory of lie algebras.

\begin{definition}[Completely Reducible]
    We say $T$ is a completely reducible if there exists a matrix representation of $T$ of the following form:
    \[
        \begin{bmatrix}
            \lambda_1 I_{n_1} & & \\
            & \ddots & \\
            & & \lambda_k I_{n_k}
        \end{bmatrix}
    \]    
    Equivalently, $T$ is completely reducible if $V$ has a non-trivial decomposition $V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}$ with respect to which $T = \lambda_1 1_{V_{\lambda_1}} \oplus \cdots \oplus \lambda_k 1_{V_{\lambda_k}}$ for some distinct eigenvalues $\lambda_1, \cdots, \lambda_k$.
\end{definition}

\begin{definition}[Semisimple]
    We say $T$ is semisimple if $T \otimes_{\F} \bar{\F} : V \otimes_{\F} \bar{\F} \to V \otimes_{\F} \bar{\F}$ is completely reducible, where $\bar{\F}$ is the algebraic closure of $\F$ and $V \otimes_{\F} \bar{\F}$ is linear space over $\bar{\F}$.
\end{definition}
\begin{remark}
    We can take $\F = \R$, then $\bar{\F} = \mathbb{C}$. Algebraic closure means that every polynomial in $\bar{\F}[x]$ has a root in $\bar{\F}$. For example, $x^2 + 1$ has no root in $\R$, but it has roots $\pm i$ in $\mathbb{C}$.

    Note that $- \otimes \F \equiv \id_{\F}$, so if we change it to $- \otimes_{\F} \bar{\F}$, then we are just changing the field from $\F$ to $\bar{\F}$ without changing the values inside. For example, $1$ can be viewed as an element in $\R$ or $\mathbb{C}$.
\end{remark}

In general, $T$ is not semisimple, but it can be decomposed into a semisimple part and a \emph{nilpotent} part. Moreover, this decomposition is unique. 

We can consider the $\End{V} \equiv \M{n \times n}{\F} \equiv \F^{n^2}$ as a vector space. Then $T \in \F^{n^2}$ is a vector. Then such the set of containing such $T$ forms a dense open subset of $\End{V} = \F^{n^2}$. The dense open subset is in the Zariski topology. More precisely, the set of all completely reducible endomorphisms with distinct eigenvalues forms a dense open subset of $\End{V}$. We will study Zariski topology next section. 

Once we know that completely reducible endomorphisms are dense in $\End{V}$, then if we want to prove some identity, it suffices to prove it for completely reducible endormophisms. One of the example is the Cayley-Hamilton theorem.

\begin{theorem}[Cayley-Hamilton Theorem]
    Let $T : V \to V$ be an endomorphism of a finite dimensional vector space $V$ over $\F$. Then $T$ satisfies its own characteristic polynomial, i.e. $\left.p_T (\lambda)\right|_{\lambda = T} = 0$.
\end{theorem}
\begin{remark}
    $p_T (\lambda) = \det (\lambda 1_V - T) = \lambda^n + \cdots + (-1)^n \det(T) \lambda^0$, where $\lambda^0 = 1$ and $T^0 = 1_V$.
\end{remark}
\begin{proof}
    As $\left.p_T (\lambda)\right|_{\lambda = T}$ is a polynomial in $T$, it suffices to verify the theorem on a dense set. 

    Let $T = \lambda_1 1_{V_{\lambda_1}} \oplus \cdots \oplus \lambda_k 1_{V_{\lambda_k}}$ be a completely reducible endomorphism with distinct eigenvalues $\lambda_1, \cdots, \lambda_k$ and non-trivial decomposition $V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}$. Then we have $1_V = 1_{V_{\lambda_1}} \oplus \cdots \oplus 1_{V_{\lambda_k}}$. Therefore, we have:
    \[
        \lambda 1_V - T = (\lambda - \lambda_1) 1_{V_{\lambda_1}} \oplus \cdots \oplus (\lambda - \lambda_k) 1_{V_{\lambda_k}}
    \]
    Then the characteristic polynomial is:
    \[
        p_T (\lambda) = \det (\lambda 1_V - T) = (\lambda - \lambda_1)^{\dim V_{\lambda_1}} \cdots (\lambda - \lambda_k)^{\dim V_{\lambda_k}} = \prod_{i = 1}^k (\lambda - \lambda_i)^{n_i}
    \]
    where $n_i = \dim V_{\lambda_i}$. Note that $\lambda_i 1_{V_{\lambda_i}} - T = 0$ on $V_{\lambda_i}$, as $T|_{V_{\lambda_i}} = \lambda_i 1_{V_{\lambda_i}}$. Therefore, we have:
    \[
        \left.p_T (\lambda)\right|_{\lambda = T} = \prod_{i = 1}^k (\lambda_i 1_V - T)^{n_i} = 0
    \]
    As for any $v \in V$, we can write $v = v_1 + \cdots + v_k$ with $v_i \in V_{\lambda_i}$, then we have:
    \[
        (\lambda_i 1_{V_{\lambda_i}} - T)^{n_i} (v_i) = 0 \quad \forall i \implies \left.p_T (\lambda)\right|_{\lambda = T} (v) = 0
    \]
    This completed the proof.
\end{proof}

If $T$ is completely reducible, then 
\[
    n_i = \dim V_{\lambda_i}
\]
where $n_i$ is the algebraic multiplicity of eigenvalue $\lambda_i$ and $\dim V_{\lambda_i}$ is the geometric multiplicity of eigenvalue $\lambda_i$. In general, we have $n_i \geq \dim V_{\lambda_i}$. Then $\{ \lambda_1, \cdots, \lambda_k \}$ is the set of roots of $p_T (\lambda)$ and $V_{\lambda_i} = \ker{\lambda_i 1_V - T}$.

Then for any $T$, if the set of roots of $p_T (\lambda)$ in $\F$ is $\{ \lambda_1, \cdots, \lambda_k \}$, then we can define the \emph{generalised eigenspaces}:
\[
    V_{\lambda_i} = \ker{\lambda_i 1_V - T} \quad \forall 1 \leq i \leq k
\]
Then we check whether $\dim V = \sum_{i = 1}^k \dim V_{\lambda_i}$. If it holds, then $T$ is completely reducible. If not, then $T$ is not. So this characterise completely reducible endomorphisms.

\newpage

\section{Zariski Topology}

Before studying Zariski topology, we first introduce \emph{affine spaces}.

\begin{definition}[Affine Spaces]
    A set $\A$ is called an \emph{affine space} over a field $\F$ if it is a principal $(\F^n, +)$-set, i.e. there is a free and transitive action of the additive group $(\F^n, +)$ on $\A$:
    \[
        + : \A \times \F^n \to \A, \quad (P, \vec{v}) \mapsto P + \vec{v}
    \]
    Each element $P \in \A$ is called a \emph{point} in $\A$.
\end{definition}

Principal means that the group action is free and transitive. Free means that if $g$ is not the identity element, then $g \cdot x \neq x$ for any $x$ in the set. Transitivity means that any two elements $x, y$ in the set are related by some action of the group, $g$, such that $g \cdot x = y$.

For example, consider the $SO(2)$ action on the plane $\R^2$. The action is not free and not transitive. It is not free because rotating a point on the plane by $0$ degree (the identity element) keeps the point unchanged, but rotating it by any other angle will change the point. It is not transitive because there is no rotation that can map a point to another point with a different distance from the origin. However, if we consider the orbits of the action, i.e. circles centered at the origin, then the action is transitive on each orbit and free except for the origin.

Then we introduce what topology is.

\begin{definition}[Topology]
    Let $X$ be a set. A \emph{topology} on $X$ is a collection $\tau$ of subsets of $X$ such that:
    \begin{enumerate}
        \item $\phi, X \in \tau$;
        \item the union of any collection of sets in $\tau$ is also in $\tau$;
        \item the intersection of any finite number of sets in $\tau$ is also in $\tau$.
    \end{enumerate}
    The pair $(X, \tau)$ is called a \emph{topological space}. Each set in $\tau$ is called an \emph{open set} in $X$.
\end{definition}
We can define \emph{closed sets} in $X$ as the complements of open sets in $X$. Then we have the following equivalent definition of topology.
\begin{definition}[Topology (Closed Set Version)]
    Let $X$ be a set. A \emph{topology} on $X$ is a collection $\tau$ of subsets of $X$ such that:
    \begin{enumerate}
        \item $\phi, X \in \tau$;
        \item the intersection of any collection of sets in $\tau$ is also in $\tau$;
        \item the union of any finite number of sets in $\tau$ is also in $\tau$.
    \end{enumerate}
    The pair $(X, \tau)$ is called a \emph{topological space}. Each set in $\tau$ is called an \emph{closed set} in $X$.
\end{definition}

Then Zariski topology is defined as follows.
\begin{definition}[Zariski Topology]
    Let $\A$ be an affine space over a field $\F$. The \emph{Zariski topology} on $\A$ is defined by taking the closed sets to be the zero loci of sets of polynomials in $\F[x_1, \cdots, x_n]$. More precisely, for any set of polynomials $S \subseteq \F[x_1, \cdots, x_n]$, the corresponding closed set is:
    \[
        V(S) = \{ P \in \A : f(P) = 0 \quad \forall f \in S \} = \bigcap_{\alpha} \{ f_\alpha = 0 \}
    \]
    The pair $(\A, \tau_{Zar})$ is called a \emph{Zariski topological space}, where $\tau_{Zar}$ is the Zariski topology on $\A$.
\end{definition}

Then the $A \in \F^{n^2} \equiv \A_{\F}^{n^2}$ can be viewed as a point in the affine space $\A_{\F}^{n^2}$ over $\F$. Then the set of all completely reducible endomorphisms with distinct eigenvalues forms a dense open subset of $\End{V} = \F^{n^2}$ in the Zariski topology. Dense means that its closure is the whole space. Open means that its complement is a closed set, i.e. the zero locus of some set of polynomials in $\F[x_1, \cdots, x_{n^2}]$.

\newpage

\section{Ring Theory}

Before studying the canonical forms of not completely reducible endormorphisms, we need to introduce some concepts in ring theory.

\begin{definition}[Domain]
    A \emph{domain} is a non-trivial commutative ring $R$ with unity $1_R \neq 0_R$ if non-zero elements $a, b \in R$ satisfy $ab \neq 0_R$.
\end{definition}

\begin{example}
    $\Z$ is a domain. Given any two non-zero integers $a, b \in \Z$, we have $ab \neq 0$.
\end{example}

\begin{example}
    $\quotient{\Z}{6}$ is not a domain. For example, $2, 3 \in \quotient{\Z}{6}$ are non-zero elements, but $2 \cdot 3 = 0$ in $\quotient{\Z}{6}$.
\end{example}

\begin{definition}[Module]
    A module over $R$ is an abelian group $(M, +)$ together with a ring action of $R$ on $(M, +)$.
\end{definition}

\begin{example}
    $R$ itself is a module over $R$ with the ring action being the multiplication in $R$.
\end{example}

\begin{definition}[Submodule]
    A \emph{submodule} $N$ of a module $M$ over a ring $R$ is a subgroup of $(M, +)$ that is closed under the ring action of $R$ on $M$, i.e. for any $r \in R$ and $n \in N$, we have $r \cdot n \in N$.
\end{definition}

\begin{definition}[Ideal]
    An \emph{ideal} $I$ of a ring $R$ is a submodule of the module $R$ over itself.
\end{definition}

\begin{example}
    Consider $\F$ over itself. Then the only ideals are $\{ 0 \}$ and $\F$ itself. So the ideal of a field is trivial.
\end{example}

\begin{example}
    Consider $\Z$ over itself. Then the ideals are all of the form $(n) = n\Z = \{ nk : k \in \Z \}$ for some $n \in \Z$. So the ideals of $\Z$ are non-trivial. For example, $(2) = \{ 0, \pm 2, \pm 4, \cdots \}$.
\end{example}

\begin{definition}[Principal Ideal Domain]
    A \emph{principal ideal domain} (PID) is a domain $R$ such that every ideal of $R$ is of the form $(a) = aR$ for some $a \in R$.
\end{definition}

\begin{example}
    $\Z$ is a principal ideal domain, as every ideal of $\Z$ is of the form $(n) = n\Z$ for some $n \in \Z$.
\end{example}

\begin{example}
    $\F[x]$ is a principal ideal domain, as every ideal of $\F[x]$ is of the form $(f(x)) = f(x) \F[x]$ for some $f(x) \in \F[x]$. It can be proved using the division algorithm of polynomials.
\end{example}

\begin{definition}[Finitely Generated Module]
    A module $M$ over a ring $R$ is called \emph{finitely generated} if $M$ is the span of a finite set of elements in $M$, i.e., $M = \langle m_1, m_2, \cdots, m_k \rangle$ for some $m_1, m_2, \cdots, m_k \in M$. It may not be unique.
\end{definition}

Note that we do not use the definition of the finite dimensional vector space here, as a module over a ring may not have a basis. There exists something called the torsion module that prevents the existence of basis. We will discuss it later.

Then we introduce the following theorem which can derive Jordan canonical form. 

\begin{theorem}[Classification Theorem of Finitely Generated Modules over a PID]
    Let $R$ be a principal ideal domain and $M$ be a finitely generated module over $R$. Then $M$ is isomorphic to a finite direct sum of cyclic modules of the form:
    \[
        M \cong R^r \oplus \bigoplus_{i = 1}^m \quotient{R}{(a_i)} = R^r \oplus \quotient{R}{(a_1)} \oplus \cdots \oplus \quotient{R}{(a_m)}
    \]
    with $a_i \in R \setminus \{ 0 \}$ and $a_i | a_{i + 1}$ for each $1 \leq i \leq m - 1$.
\end{theorem}
\begin{remark}
    Note that $a | b$ means that there exists some $c \in R$ such that $b = ac$.
\end{remark}
Here $R^r$ is the free part of $M$ and $\bigoplus_{i = 1}^m \quotient{R}{(a_i)}$ is the torsion part of $M$. The torsion part prevents the existence of basis of $M$. If the torsion part is trivial, i.e. $m = 0$, then $M$ is a free module and has a basis. Moreover, $r$ is the rank of $M$ and is unique. $a_i$ are called the invariant factors of $M$ and are unique up to multiplication by units in $R$. This is called the invariant factor decomposition of $M$. There is another decomposition called primary decomposition, or elementary divisor decomposition, or Chinese Remainder decomposition. 
\begin{theorem}[Classification Theorem of Finitely Generated Modules over a PID (Primary Decomposition)]
    Let $R$ be a principal ideal domain and $M$ be a finitely generated module over $R$. Then $M$ is isomorphic to a finite direct sum of cyclic modules of the form:
    \[
        M \cong R^r \oplus \bigoplus_{i = 1}^m \quotient{R}{(p_i^{e_i})} = R^r \oplus \quotient{R}{(p_1^{e_1})} \oplus \cdots \oplus \quotient{R}{(p_m^{e_m})}
    \]
    with $p_i$ being prime or irreducible elements in $R$ and $e_i \in \Z^+$ for each $1 \leq i \leq m$.
\end{theorem}
\begin{remark}
    As $R$ is a PID, so every ideal is principal. Therefore, every ideal generated by a prime or irreducible element is a prime ideal. This is why we call it primary decomposition.
\end{remark}

For any ring $R$, we can decomposite as follows:
\[
    R = \{ 0 \} \cup R^{\times} \cup S
\]
where $R^{\times}$ is the set of units in $R$ and $S$ is the set of non-units and non-zero elements in $R$. Then any $u \in R$ is called a unit if there exists some $v \in R$ such that $uv = vu = 1_R$. For example, in $\Z$, the units are $\pm 1$. In $\F[x]$, the units are all non-zero constant polynomials.

Then the set of all prime elements and the set of all irreducible elements in $R$ are subsests of $S$. In general, they are not the same. The set of all prime elements is a subset of the set of all irreducible elements. However, in a principal ideal domain, they are the same. Irreducible elements are elements that cannot be factored into the product of two non-unit elements, i.e., if $x \neq 0$ and $x \notin R^{\times}$, then whenever $x = yz$, then $y$ or $z$ must be a unit.

\newpage

\section{Jordan Canonical Form}

Let $V$ be a finite dimensional linear space over an algebraically closed field $\F$, e.g. $\mathbb{C}$. Then $\F[x]$ is a principal ideal domain and $x - \lambda_i$ are the prime or irreducible elements in $\F[x]$ for each $\lambda_i \in \F$.
\begin{remark}
    If we take non-zero $\alpha \in \F$, then $\alpha (x - \lambda_i)$ is also an irreducible element in $\F[x]$, as $\alpha$ is a unit in $\F[x]$ and we have $(x - \lambda_i) = (\alpha(x - \lambda_i))$. Therefore, the irreducible elements are only unique up to multiplication by units. We can just choose monic polynomials as the irreducible elements.
\end{remark}

Then for any endomorphism $T : V \to V$. It is equivalent to consider $V$ as a module over $\F[x]$ with the ring action defined as:
\[
    F[x] \times V \to V, \quad (p(x), v) \mapsto p(T) v
\]

\begin{example}
    Let $p(x) = 2x^2 + 3x - 1 \in \F[x]$ and $T \in \End{V}$. Then for any $v \in V$, we have $p(T) v = 2 T^2 v + 3 T v - v$.
\end{example}

$V$ is the finite dimensional linear space over $\F$, so it is a finitely generated module over $\F[x]$ with rank $0$. It is the torsion part only. Therefore, by the classification theorem of finitely generated modules over a PID, we have:
\[
    V \cong \bigoplus_{i = 1}^m \frac{\F[x]}{(x - \lambda_i)^{e_i}} = \frac{\F[x]}{(x - \lambda_1)^{e_1}} \oplus \cdots \oplus \frac{\F[x]}{(x - \lambda_m)^{e_m}}
\]
Note that $T$ is the same as the multiplication by $x$ in the module, i.e., $x\cdot : V \to V$ defined as $v \mapsto x v$. Then for each cyclic module $\dfrac{\F[x]}{(x - \lambda_i)^{e_i}}$, we have the dimension being $e_i$. Therefore, we have the basis on $\dfrac{\F[x]}{(x - \lambda_i)^{e_i}}$ as:
\[
    \B_i = \{ 1, (x - \lambda_i), (x - \lambda_i)^2, \cdots, (x - \lambda_i)^{e_i - 1} \}
\]

Then we consider the following diagram:
\begin{center}
    \begin{tikzcd}
        \frac{\F[x]}{(x - \lambda_i)^{e_i}} \arrow[r, "x \cdot", "T_i"'] \arrow[d, "{[-]_{\B_i}}" swap] & \frac{\F[x]}{(x - \lambda_i)^{e_i}} \arrow[d, "{[-]_{\B_i}}"] \\
        \F^{e_i} \arrow[r, "J_{e_i} (\lambda_i)" swap] & \F^{e_i}
    \end{tikzcd}
\end{center}
Then what is $J_{e_i} (\lambda_i)$? We have:
\[
    x \cdot 1 = x = 1 \cdot (x - \lambda_i) + \lambda_i \cdot 1
\]
So the first column of $J_{e_i} (\lambda_i)$ is $[\lambda_i \quad 1 \quad 0 \quad \cdots \quad 0]^T$. Similarly, we have:
\[
    \begin{split}
        x \cdot (x - \lambda_i) &= 1 \cdot (x - \lambda_i)^2 + \lambda_i \cdot (x - \lambda_i) \\
        x \cdot (x - \lambda_i)^{e_i - 1} &= 1 \cdot (x - \lambda_i)^{e_i} + \lambda_i \cdot (x - \lambda_i)^{e_i - 1} = \lambda_i \cdot (x - \lambda_i)^{e_i - 1}
    \end{split}
\]
So the matrix representation of $x \cdot$ on $\dfrac{\F[x]}{(x - \lambda_i)^{e_i}}$ with respect to the basis $\B_i$ is:
\[
    J_{e_i} (\lambda_i) = \begin{bmatrix}
        \lambda_i & & & & \\
        1 & \lambda_i & & &\\
        & 1 & \lambda_i & & \\
        & & \ddots & \ddots & \\
        & & & 1 & \lambda_i
    \end{bmatrix}
\]
We can switch the order of basis elements in $\B_i$ to get the following equivalent representation:
\[
    J_{e_i} (\lambda_i) = \begin{bmatrix}
        \lambda_i & 1 & & & \\
        & \lambda_i & 1 & &\\
        & & \lambda_i & \ddots & \\
        & & & \ddots & 1 \\
        & & & & \lambda_i
    \end{bmatrix}
\]
This is called a \emph{Jordan block} of size $e_i$ with eigenvalue $\lambda_i$.

Then the matrix representation of $T$ on $V$ with respect to the basis $\B = \B_1 \cup \B_2 \cup \cdots \cup \B_m$ is:
\[
    J = \begin{bmatrix}
        J_{e_1} (\lambda_1) & & & \\
        & J_{e_2} (\lambda_2) & & \\
        & & \ddots & \\
        & & & J_{e_m} (\lambda_m)
    \end{bmatrix}
\]

\chapter{Euclidean Spaces}

\epigraph{``The idea of representation is one of the few great ideas in Mathematics.''}{Guowu Meng}

Before studying Euclidean spaces, we first review tensors and then introduce inner products.

\section{Tensor}

Let $V$ be a finite dimensional vector space over a field $\F$. Then we have the following definitions.

\begin{definition}[$k$-form]
    A $k$-\emph{form} on $V$ is a multilinear map:
    \[
        \underbrace{V \times V \times \cdots \times V}_{k \text{ times}} \to \F
    \]
    which is linear in each argument. It is an element in $(V^*)^{\otimes k}$.
\end{definition}

More concretely, for 1-form, it is a linear functional on $V$, i.e. an element in $V^*$. It is also called \emph{covector}. For 2-form, it is a bilinear map on $V$, i.e. an element in $V^* \otimes V^*$. To prove that the set of all 2-forms on $V$ is isomorphic to $V^* \otimes V^*$, we can consider the following diagram:
\begin{center}
    \begin{tikzcd}
        \Map^{\mathsf{ML}} (V \times V, \F) \arrow[r, "\equiv" description, phantom] \arrow[d, dashed] & \Hom(V, V^*) \arrow[d, "\vequiv" description, phantom] \\
        V^* \otimes V^* & \Hom(V, \F) \otimes V^* \arrow[l, "\equiv" description, phantom]
    \end{tikzcd}
\end{center}
Remember that $\Hom(V_1, V_2 \otimes V_3) \equiv \Hom(V_1, V_2) \otimes V_3$.

Moreover, we have the following two special types of 2-forms which are the elements inside the symmetric and exterior powers of $V^*$.

\begin{definition}[Symmetric and Skew-symmetric 2-forms]
    A 2-form $\omega : V \times V \to \F$ is called \emph{symmetric} if 
    \[
        \omega (u, v) = \omega (v, u)
    \]
    for all $u, v \in V$. It is an element in $\mathcal{S}^2{V^*}$. The 2-form $\omega$ is called \emph{skew-symmetric}, or antisymmetric, if 
    \[
        \omega (u, v) = - \omega (v, u)
    \]
    for all $u, v \in V$. It is an element in ${\bigwedge}^2{V^*}$.
\end{definition}

Then we define the tensor spaces.

\begin{definition}[Tensor Spaces]
    Let $V$ be a finite dimensional vector space over a field $\F$. The \emph{tensor space of type} $(r, s)$ on $V$ is defined as:
    \[
        \mathcal{T}^{r, s} V = \underbrace{V \otimes V \otimes \cdots \otimes V}_{r \text{ times}} \otimes \underbrace{V^* \otimes V^* \otimes \cdots \otimes V^*}_{s \text{ times}}
    \]
    Elements in $\mathcal{T}^{r, s} V$ are called \emph{tensors of type} $(r, s)$ on $V$, which is a mixed type if $r, s \neq 0$.
\end{definition}

If a tensor of type $(r, 0)$, then it is called a \emph{contravariant tensor} or simply a \emph{tensor}. If a tensor of type $(0, s)$, then it is called a \emph{covariant tensor} or simply a \emph{form}. For $\mathcal{T}^{0, 0} V$, it is defined as $\F$ itself. Any elements in $\mathcal{T}^{0, 0} V$ are \emph{scalar type} tensor on $V$, or simply \emph{scalars}.

Then we know that $\End{V} \equiv V \otimes V^* \equiv \mathcal{T}^{1, 1} V$. Therefore, any endomorphism on $V$ can be viewed as a tensor of type $(1, 1)$ on $V$, represented by $a^i_j$ with respect to a basis $\B_V = \{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n \}$ of $V$. Here the upper index $i$ represents the contravariant part and the lower index $j$ represents the covariant part. To know that what $a^i_j$ means, we can consider the following diagram:
\begin{center}
    \begin{tikzcd}
        V \arrow[r, "T"] \arrow[d, "{[-]_{\B_V}}" swap] & V \arrow[d, "{[-]_{\B_V}}"] \\
        \F^n \arrow[r, "{A = [a^i_j]_{\B_V}}" swap] & \F^n
    \end{tikzcd}
\end{center}
Then how to get the matrix representation $A = [a^i_j]_{\B_V}$ of $T$ with respect to the basis $\B_V$? We have:
\[
    \vec{a}_j = A \vec{e}_j, \qquad a^i_j = \vec{e}_i^T A \vec{e}_j = \hat{e}^i A \vec{e}_j = \langle \hat{e}^i, A \vec{e}_j \rangle.
\]
So we have $[a^i_j] = \langle \hat{v}^i, T \vec{v}_j \rangle$. We can have an identification between $\End{V}$ and $\mathcal{T}^{1, 1} V$ as follows:
\[
    T \leftrightarrow T\vec{v}_j \otimes \hat{v}^j
\]

For covariant and contravariant, we have the following table:

\begin{center}
\begin{tabularx}{\textwidth}{X X}
    \toprule
    \textbf{Object} & \textbf{Transformation Type} \\
    \midrule
    Standard Basis Vector ($\vec{e}_i$) & Covariant \\
    \midrule
    Dual Basis Vector ($\hat{e}^i$) & Contravariant \\
    \midrule
    Component of a Vector ($v^i$) & Contravariant \\
    \midrule
    Component Basis Vector ($v_i$) & Covariant \\
    \bottomrule
\end{tabularx}
\end{center}

An object is considered as covariant if it transform in the same way as the basis vectors of the original vector space. If you cannot understand it, make up some examples of scaling the vector spaces.

In general, an element $t \in \mathcal{T}^{r, s} V$ can be represented as:
\[
    t^{i_1 i_2 \cdots i_r}_{j_1 j_2 \cdots j_s} \vec{v}_{i_1} \otimes \vec{v}_{i_2} \otimes \cdots \otimes \vec{v}_{i_r} \otimes \hat{v}^{j_1} \otimes \hat{v}^{j_2} \otimes \cdots \otimes \hat{v}^{j_s}
\]
Note that the representation depends on the choice of basis $\B_V$ of $V$, i.e., the following two represents the same tensor with respect to different bases:
\[
    \left[t^{i_1 i_2 \cdots i_r}_{j_1 j_2 \cdots j_s}\right]_{\B_V} \sim \left[\tilde{t}^{\tilde{i}_1 \tilde{i}_2 \cdots \tilde{i}_r}_{\tilde{j}_1 \tilde{j}_2 \cdots \tilde{j}_s}\right]_{\widetilde{\B_V}}
\]
The two representations are related by the base change matrices::
\[
    (\tilde{v}_1, \tilde{v}_2, \cdots, \tilde{v}_n) = (v_1, v_2, \cdots, v_n) A, \quad A = [a^i_{\tilde{j}}]_{\B_V}^{\widetilde{\B_V}} \in \GL(V)
\]
\begin{remark}
    It is actually the right action of $\GL(V)$ on the set of all bases of $V$, $\B_V$:
    \[
        \B_V \times \GL(V) \to \B_V, \quad (v, A) \mapsto v A = \tilde{v}
    \]
\end{remark}
Then we have the following equation:
\[
    \tilde{v}_{\tilde{j}} = v_i a^i_{\tilde{j}}
\]
For $A^{-1} = [b_j^{\tilde{i}}]_{\widetilde{\B_V}}^{\B_V}$, we have $a^i_{\tilde{j}} b_j^{\tilde{k}} = \delta^{\tilde{k}}_{\tilde{j}}$ and $b_j^{\tilde{i}} a^j_{\tilde{k}} = \delta^{\tilde{i}}_{\tilde{k}}$. Therefore, we have:
\[
    v_k = \tilde{v}_{\tilde{j}} b_k^{\tilde{j}}
\]
\begin{remark}
    For easier memorising, we use the calculus operators:
    \[
        \frac{\partial \tilde{v}_{\tilde{j}}}{\partial v_i} = a^i_{\tilde{j}}, \quad \frac{\partial v_k}{\partial \tilde{v}_{\tilde{j}}} = b_k^{\tilde{j}}
    \]
    To memorise it, we consider the lower indices in denominators (lower) will flip to the upper indices in numerators. (As lower twice, so flip to upper)

    Then we can use the chain rule to verify the two equations of $A$ and $A^{-1}$:
    \[
        \frac{\partial \tilde{v}_{\tilde{j}}}{\partial v_i} \frac{\partial v_k}{\partial \tilde{v}_{\tilde{j}}} = \delta^i_k
    \]
\end{remark}

Then we have the transformation rule for the representation of $t \in \mathcal{T}^{r, s} V$ under the base change from $\B_V$ to $\widetilde{\B_V}$:
\[
    \tilde{t}^{{\color{ocre} \tilde{i}_1 \tilde{i}_2 \cdots \tilde{i}_r}}_{{\color{red} \tilde{j}_1 \tilde{j}_2 \cdots \tilde{j}_s}} = \left({\color{ocre} b_{i_1}^{\tilde{i}_1} b_{i_2}^{\tilde{i}_2} \cdots b_{i_r}^{\tilde{i}_r}}\right) t^{{\color{ocre} i_1 i_2 \cdots i_r}}_{{\color{red} j_1 j_2 \cdots j_s}} \left({\color{red} a^{j_1}_{\tilde{j}_1} a^{j_2}_{\tilde{j}_2} \cdots a^{j_s}_{\tilde{j}_s}}\right)
\]

Given that $\B_V = \{ \vec{v}_1, \cdots, \vec{v}_n \}$ is a basis of $V$, then we can define a basis of $\mathcal{T}^{r, s} V$ as follows:
\[
    \B_{\mathcal{T}^{r, s} V} = \{ \vec{v}_{i_1} \otimes \vec{v}_{i_2} \otimes \cdots \otimes \vec{v}_{i_r} \otimes \hat{v}^{j_1} \otimes \hat{v}^{j_2} \otimes \cdots \otimes \hat{v}^{j_s} : 1 \leq i_1, i_2, \cdots, i_r, j_1, j_2, \cdots, j_s \leq n \}
\]
Then for symmetric and skew-symmetric $k$-forms, we have:
\[
    \begin{split}
        \B_{\mathcal{S}^k V} &= \{ \vec{v}_{i_1} \vec{v}_{i_2} \cdots \vec{v}_{i_k} : 1 \leq i_1, i_2, \cdots, i_k \leq n \} \\
        \B_{{\bigwedge}^k V} &= \{ \vec{v}_{i_1} \wedge \vec{v}_{i_2} \wedge \cdots \wedge \vec{v}_{i_k} : 1 \leq i_1, i_2, \cdots, i_k \leq n \}
    \end{split}
\]
Then ``honest'' definition of symmetric basis is:
\[
    \{ \vec{v}_{i_1} \vec{v}_{i_2} \cdots \vec{v}_{i_k} : 1 \leq i_1 \leq i_2 \leq \cdots \leq i_k \leq n \}
\]
but it is redundant. We just have to make sure that the representation of any symmetric $k$-form is unique for a given basis. For example, in 2-form case with the basis $\{ \vec{e}_i \otimes \vec{e}_j \}$, we originally have to write:
\[
    t = \sum_{1 \leq i \leq j \leq n} t_{ij} \vec{e}_i \otimes \vec{e}_j
\]
but this is ugly, so we just write:
\[
    t = t^{ij} \vec{e}_i \vec{e}_j
\]
with $t^{ij} = t^{ji}$. If we ignored the condition on $t^{ij}$, then we have $a^{ij} = - a^{ji}$ such that:
\[
    t = t^{ij} \vec{e}_i \wedge \vec{e}_j + a^{ij} \vec{e}_i \wedge \vec{e}_j = (t^{ij} + a^{ij}) \vec{e}_i \wedge \vec{e}_j = 0
\]
As $a^{ij} = a^{ji} = - a^{ij}$. 

Then for skew-symmetric basis, let say $t \in \B_{{\bigwedge}^k V}$, then we have:
\[
    t = t^{\mathcal{I}} \vec{v}_{\mathcal{I}} = t^{i_1 i_2 \cdots i_k} \vec{v}_{i_1} \wedge \vec{v}_{i_2} \wedge \cdots \wedge \vec{v}_{i_k}
\]
with $\mathcal{I} = (i_1, i_2, \cdots, i_k)$ being an ordered index set with $1 \leq i_1 < i_2 < \cdots < i_k \leq n$. Then for any permutation $\sigma \in S_k$, to make sure it is unique, we require:
\[
    t^{\sigma(\mathcal{I})} = \sgn(\sigma) t^{\mathcal{I}}
\]
where $\sigma(\mathcal{I}) = (i_{\sigma(1)}, i_{\sigma(2)}, \cdots, i_{\sigma(k)})$.

In conclusion, we have to make sure that the representation of any symmetric or skew-symmetric $k$-form is unique for a given basis by the following conditions respectively:
\[
    \begin{split}
        &\text{Symmetric:} \quad t^{i_1 i_2 \cdots i_k} = t^{i_{\sigma(1)} i_{\sigma(2)} \cdots i_{\sigma(k)}} \\
        &\text{Skew-symmetric:} \quad t^{i_1 i_2 \cdots i_k} = \sgn(\sigma) t^{i_{\sigma(1)} i_{\sigma(2)} \cdots i_{\sigma(k)}}
    \end{split}
\]

\newpage

\section{Inner Product}

Let $V$ be a finite dimensional real linear space. Then we have the following definitions.

\begin{definition}[Inner Product]
    An inner product on $V$ is a map $\langle -, - \rangle : V \times V \to \R$ such that 
    \begin{enumerate}
        \item \emph{Bilinearity:} $\langle -, u \rangle$ and $\langle u, - \rangle$ are linear functionals on $V$ for all $u \in V$;
        \item \emph{Symmetry:} $\langle u, v \rangle = \langle v, u \rangle$ for all $u, v \in V$;
        \item \emph{Positive-definiteness:} $\langle v, v \rangle \geq 0$ for all $v \in V$ with equality if and only if $v = 0$.
    \end{enumerate}
\end{definition}

Note that an inner product on $V$ is a positive-definite symmetric 2-form on $V$.

\begin{definition}[Pseudo Inner Product]
    A pseudo inner product on $V$ is a non-degenerate symmetric bilinear form on $V$, i.e., an element $\langle-, -\rangle \in \mathcal{S}^2 V^*$ such that $\langle-, -\rangle_{\musNatural} : V \to V^*$ is isomorphic.
\end{definition}

Then a real linear space $V$ with an inner product $\langle -, - \rangle$ is called a \emph{Euclidean space}, denoted by $(V, \langle -, - \rangle)$. 

\begin{definition}[Metric Space]
    A metric space is a non-empty set $X$ together with a metric structure, i.e., a distance function $d : X \times X \to \R$ that sends $(x, y)$ to $d(x, y)$ such that
    \begin{enumerate}
        \item \emph{Positivity:} $d(x, y) \geq 0$ for all $x, y \in X$ with equality if and only if $x = y$;
        \item \emph{Symmetry:} $d(x, y) = d(y, x)$ for all $x, y \in X$;
        \item \emph{Triangle Inequality:} $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$.
    \end{enumerate}
\end{definition}

If we want to combine the metric structure with the linear structure on $V$, we have to make sure that the distance function $d : V \times V \to \R$ satisfies the two additional properties in order to be compatible with the linear structure. We would say the properties are \emph{harmonic} with the linear structure.

\begin{definition}[Normed Linear Space]
    A real normed linear space is a real linear space $V$ together with a compatible metric structure or a normed structure, i.e., a distance function $d : V \times V \to \R$ such that
    \begin{enumerate}
        \item \emph{Translation Invariance:} $d(u + w, v + w) = d(u, v)$ for all $u, v, w \in V$;
        \item \emph{Homogeneity:} $d(\alpha u, \alpha v) = |\alpha| d(u, v)$ for all $u, v \in V$ and $\alpha \in \R$.
    \end{enumerate}
    Then we can define the norm on $V$ as $\| v \| = d(v, 0)$ for all $v \in V$.
\end{definition}

Then a function $\| - \| : V \to \R$ that sends $v$ to $\| v \|$ is called a norm on $V$ if it satisfies:
\begin{enumerate}
    \item \emph{Positive-definiteness:} $\| v \| \geq 0$ for all $v \in V$ with equality if and only if $v = 0$;
    \item \emph{Homogeneity:} $\| \alpha v \| = |\alpha| \| v \|$ for all $v \in V$ and $\alpha \in \R$;
    \item \emph{Triangle Inequality:} $\| u + v \| \leq \| u \| + \| v \|$ for all $u, v \in V$.
\end{enumerate}
We can use the norm with the properties above to define the distance function by $d(x, y) = \| x - y \|$.

\begin{theorem}[Parallelogram Law]
    The parallelogram law states that the sum of the squares of the lengths of the four sides of a parallelogram equals the sum of the squares of the lengths of the two diagonals, i.e., with the following figure:
    \begin{center}
        \begin{tikzpicture}[scale=2]
            \draw (0, 0) coordinate (A) -- (2, 0) coordinate (B) -- (2.5, 1) coordinate (C) -- (0.5, 1) coordinate (D) -- cycle;
            \draw[violet, -latex, thick] (A) -- (C) node[pos=0.7, below right] {$u + v$};
            \draw[red, -latex, thick] (B) -- (D) node[pos=0.7, below left] {$u - v$};
            \draw[teal, -latex, thick] (A) -- (B) node[midway, below] {$u$};
            \draw[ocre, -latex, thick] (A) -- (D) node[midway, left] {$v$};
        \end{tikzpicture}
    \end{center}
    we have:
    \[
        \| u + v \|^2 + \| u - v \|^2 = 2 \| u \|^2 + 2 \| v \|^2
    \]
\end{theorem}

\begin{proposition}
    An inner product on $V$ is equivalence to a norm structure on $V$ which satisfies the parallelogram law.
\end{proposition}
\begin{proof}
    ($\Rightarrow$) Let $(V, \langle -, - \rangle)$ be a Euclidean space. Then we can define the norm on $V$ as $\| v \| = \sqrt{\langle v, v \rangle}$ for all $v \in V$. Then we have:
    \begin{enumerate}
        \item \emph{Positive-definiteness:} $\| v \| = \sqrt{\langle v, v \rangle} \geq 0$ for all $v \in V$ with equality if and only if $v = 0$;
        \item \emph{Homogeneity:} $\| \alpha v \| = \sqrt{\langle \alpha v, \alpha v \rangle} = \sqrt{\alpha^2 \langle v, v \rangle} = |\alpha| \| v \|$ for all $v \in V$ and $\alpha \in \R$;
        \item \emph{Triangle Inequality:} By Cauchy-Schwarz inequality, we have:
        \[
            \begin{split}
                \| u + v \| &= \sqrt{\langle u + v, u + v \rangle} = \sqrt{\langle u, u \rangle + \langle v, v \rangle + \langle u, v \rangle + \langle v, u \rangle} \\
                &= \sqrt{\| u \|^2 + \| v \|^2 + 2 \langle u, v \rangle} \\
                &\leq \sqrt{\| u \|^2 + \| v \|^2 + 2 \| u \| \| v \|} \\
                &= \sqrt{(\| u \| + \| v \|)^2} = \| u \| + \| v \|
            \end{split}
        \]
        Therefore, the triangle inequality holds.
        \item \emph{Parallelogram Law:} We have:
        \[
            \begin{split}
                \| u + v \|^2 + \| u - v \|^2 &= \langle u + v, u + v \rangle + \langle u - v, u - v \rangle \\
                &= \langle u, u \rangle + \langle v, v \rangle + \langle u, v \rangle + \langle v, u \rangle + \langle u, u \rangle + \langle v, v \rangle - \langle u, v \rangle - \langle v, u \rangle \\
                &= 2 \langle u, u \rangle + 2 \langle v, v \rangle = 2 \| u \|^2 + 2 \| v \|^2
            \end{split}
        \]
    \end{enumerate}

    ($\Leftarrow$) We define the inner product for all $u, v \in V$ as follows and the proof is left as an exercise:
    \[
        \langle u, v \rangle = \frac{1}{2} \left( \| u + v \|^2 - \| u \|^2 - \| v \|^2 \right)
    \]
    % TODO: Complete the proof
\end{proof}

\begin{theorem}[Cauchy-Schwarz Inequality]
    Let $(V, \langle -, - \rangle)$ be a Euclidean space. Then for all $u, v \in V$, we have:
    \[
        |\langle u, v \rangle| \leq \| u \| \| v \|
    \]
    with equality if and only if $u$ and $v$ are linearly dependent.
\end{theorem}
\begin{proof}
    Let $f(t) = \| tu + v \|^2 = \langle tu + v, tu + v \rangle = t^2 \| u \|^2 + 2t \langle u, v \rangle + \| v \|^2$ for all $t \in \R$. Then we have $f(t) \geq 0$ for all $t \in \R$. For $u = 0$, the inequality holds trivially. For $u \neq 0$, the quadratic function $f(t)$ has at most one real root, so its discriminant is less than or equal to zero:
    \[
        \Delta = 4 \langle u, v \rangle^2 - 4 \| u \|^2 \| v \|^2 \leq 0 \implies \langle u, v \rangle^2 \leq \| u \|^2 \| v \|^2
    \]
\end{proof}

\begin{definition}
    If both $u, v \in V$ are non-zero vectors in a Euclidean space $(V, \langle -, - \rangle)$, then the angle $\theta$ between $u$ and $v$ is defined as:
    \[
        \theta = \arccos{\left( \frac{\langle u, v \rangle}{\| u \| \| v \|} \right)}
    \]
    Moreover, if $\langle u, v \rangle = 0$, then we say that $u$ and $v$ are orthogonal.
\end{definition}

\newpage

\section{Orthogonality}

Let $V$ be a Euclidean space with inner product $\langle -, - \rangle$ and $W \subseteq V$ is a subspace of $V$. Then we claim that $W$ inherits an Euclidean structure from $\langle -, - \rangle$ in $V$. We can simply restrict the inner product $\langle -, - \rangle$ on $V$ to $W$:
\begin{center}
    \begin{tikzcd}
        W \times W \arrow[r, hook] \arrow[rr, bend right, "{\langle -, - \rangle}"'] & V \times V \arrow[r, "{\langle -, - \rangle}"] & \R
    \end{tikzcd}
\end{center}
Note that the restriction $\langle -, - \rangle$ is still an inner product on $W$. Also, the positive-definiteness of $\langle -, - \rangle$ implies that $\langle -, - \rangle$ is non-degenerate, i.e., the map $\langle -, - \rangle_{\musNatural} : W \to W^*$ is isomorphism. Note that $W$ and $W^*$ have the same dimension and it has a trivial kernel: $\langle u, - \rangle_W = 0$ implies $\langle u, u \rangle_W = 0$ implies $u = 0$. Now, suppose $w = (w_1, \cdots, w_k)$ is a basis of $W$ and $w^* = (w_1^*, \cdots, w_k^*)$ is the dual basis of $W^*$, then we have the following diagram:
\begin{center}
    \begin{tikzcd}
        0 \arrow[r] & \ker{\lambda_w} \arrow[r] & V \arrow[r, "\lambda_w"', two heads] & \R^k \arrow[r] \arrow[l, bend right, "s"'] & 0 \\
        & & W \arrow[u, hook] \arrow[r, "{\langle -, - \rangle_{\musNatural}}", hook, two heads] & W^* \arrow[u, "{[-]_{w^*}}"', two heads, hook] \\[-3.6em]
        & & {\scriptstyle w_i} \arrow[r, mapsto] & {\scriptstyle \langle w_i, - \rangle}
    \end{tikzcd}
\end{center}
where $\lambda_w = \begin{bmatrix}
    \langle w_1, - \rangle \\
    \vdots \\
    \langle w_k, - \rangle
\end{bmatrix}$, and $s$ is a section of $\lambda_w$ with image $W$. Then we have the decomposition:
\[
    V = \im{s} \oplus \ker{\lambda_w} = W \oplus \ker{\lambda_w}
\]
Note that it is an internal direct sum. Then we define the orthogonal complement of $W$ in $V$ as follows.
\begin{definition}[Orthogonal Complement]
    The orthogonal complement of $W$ in $V$, denoted by $W^\perp$, is defined as:
    \[
        W^\perp = \{ v \in V \mid \langle v, w \rangle = 0 \text{ for all } w \in W \} = \{ v \in V \mid \langle v, w_i \rangle = 0 \text{ for all basis } w_i \in W \}
    \]
    Then we have the decomposition:
    \[
        V = W \oplus W^\perp
    \]
\end{definition}

Then any vector $v \in V$ can be uniquely decomposed as $v = w + w^\perp$ with $w = \proj_W(v) \in W$ and $w^\perp = \proj_{W^\perp}(v) \in W^\perp$. The map $\proj_W : V \to W$ is called the orthogonal projection onto $W$ along $W^\perp$. Take a look at the following figure:
\begin{center}
    \begin{tikzpicture}
        \draw (-0.1, 0.2) -- (-0.3, 0.1) -- (-0.2, -0.1);
        \draw[ocre] (-4, -2) -- (4, 2) node[right] {$W$};
        \draw[red] (-1.5, 3) node[above] {$W^\perp$} -- (1.5, -3);
        \draw[thick, -latex] (0, 0) -- (4, 0) node[right] {$\vec{v}$};
        \draw[ocre, -latex] (0, 0) -- (3.2, 1.6) node[midway, above, rotate=26.57] {$\scriptstyle w = \proj_W v$};
        \draw[red, -latex] (0, 0) -- (0.8, -1.6) node[midway, below, rotate=-63.43] {$\scriptstyle w^\perp = \proj_{W^\perp} v$};
        \draw[ocre, dashed] (4, 0) -- (3.2, 1.6);
        \draw[red, dashed] (4, 0) -- (0.8, -1.6);
    \end{tikzpicture}
\end{center}

Then we have the following properties of the orthogonal projection:
\begin{enumerate}
    \item $(\proj_W)^2 = \proj_W$;
    \item $\im{\proj_W} = W$;
    \item $\ker{\proj_W} = W^\perp$;
    \item $\proj_W + \proj_{W^\perp} = \id_V$.
\end{enumerate}

\begin{definition}[Orthonormal Basis]
    A basis $v$ is orthogonal if $\langle v_i, v_j \rangle = 0$ for all $i \neq j$. An orthogonal basis is orthonormal if $\| v_i \| = 1$ for all $i$.
\end{definition}

Then we have the following proposition.
\begin{proposition}
    For any Euclidean space $V$ with inner product, there exists an orthonormal basis of $V$. Moreover, there exists a linear isometric isomorphism between $V$ and $\R^n$ with the standard inner product, the dot product.
\end{proposition}
Note that $(\R^n, \cdot)$ is up to isomorphism the only Euclidean space with dimension $n$, where $\cdot$ denotes the standard dot product.

Moreover, if $w = (w_1, w_2, \cdots, w_k)$ is an orthonormal basis of $W$, then 
\[
    \proj_W u = \sum_{i = 1}^k \langle w_i, u \rangle w_i
\]
for all $u \in V$. In case $w$ is orthogonal but not orthonormal, then we have:
\[
    \proj_W u = \sum_{i = 1}^k \frac{\langle w_i, u \rangle}{\langle w_i, w_i \rangle} w_i
\]

\newpage

\section{Gram-Schmidt Process}

Let $w = (w_1, w_2, \cdots, w_k)$ be an orthonormal basis of $W \subseteq V$. Then we have:
\[
    x = \underbrace{\sum_{i = 1}^k \langle w_i, x \rangle w_i}_{\in W} + \underbrace{x - \sum_{i = 1}^k \langle w_i, x \rangle w_i}_{\in W^\perp} = \proj_W x + \proj_{W^\perp} x.
\]
To show that $\proj_{W^\perp} x \in W^\perp$, it suffices to show that $\langle w_j, \proj_{W^\perp} x \rangle = 0$ for all $1 \leq j \leq k$:
\[
    \begin{split}
        \langle w_j, \proj_{W^\perp} x \rangle &= \langle w_j, x - \sum_{i = 1}^k \langle w_i, x \rangle w_i \rangle \\
        &= \langle w_j, x \rangle - \sum_{i = 1}^k \langle w_i, x \rangle \langle w_j, w_i \rangle \\
        &= \langle w_j, x \rangle - \langle w_j, x \rangle = 0
    \end{split}
\]
Note that the key step is to use the bilinearity of the inner product and the orthonormality of $w$.

Now, given any basis $x = (x_1, x_2, \cdots, x_n)$ of $V$, we can use the Gram-Schmidt process to construct an orthonormal basis $w = (w_1, w_2, \cdots, w_n)$ of $V$ by inductive argument. The idea is: We have $V_n \supset V_{n - 1} \supset \cdots \supset V_2 \supset V_1 \supset V_0 = \{ 0 \}$ with the dimension $n, n - 1, \cdots, 2, 1, 0$ respectively. Then we have $w_1$ as the orthonormal basis of $V_1$, then we can extend it to $w_1, w_2$ as the orthonormal basis of $V_2$, and so on and so forth until we reach $V_n = V$.

Then we consider the first two cases to illustrate the idea. Let $v_1 = u_1$. Then we have $w_1 = \frac{v_1}{\| v_1 \|}$ as the orthonormal basis of $V_1 = \Span\{u_1\}$. Then we want to find the $w_2$ such that $w_1, w_2$ is the orthonormal basis of $V_2 = \Span\{u_1, u_2\}$. We can consider the following diagram:
\begin{center}
    \begin{tikzpicture}
        \draw (-0.1, 0.2) -- (-0.3, 0.1) -- (-0.2, -0.1);
        \draw[ocre] (-4, -2) -- (4, 2) node[right] {$V_1$};
        \draw[red] (-1.5, 3) node[above] {$V_1^\perp$} -- (1.5, -3);
        \draw[thick, -latex] (0, 0) -- (1, 3) node[above] {$x_2$};
        \draw[thick, -latex] (0, 0) -- (3, 1.5) node[above, magenta] {$v_1$} node[below right] {$x_1$};
        \draw[magenta, -latex, thick] (0, 0) -- (-1, 2) node[left] {$v_2$};

        \draw[violet, -latex, very thick] (0, 0) -- (0.894, 0.447) node[below right] {$w_1$};
        \draw[violet, -latex, very thick] (0, 0) -- (-0.447, 0.894) node[left] {$w_2$};

        \draw[ocre, dashed] (1, 3) -- (2, 1) node[midway, above, rotate=-63.43] {$\scriptstyle \proj_{V_1} x_2$};
        \draw[red, dashed] (1, 3) -- (-1, 2) node[midway, above, rotate=26.57] {$\scriptstyle x_2 - \proj_{V_1} x_2$};
    \end{tikzpicture}
\end{center}
Then $v_2 = x_2 - \proj_{V_1} x_2 = x_2 - \langle w_1, x_2 \rangle w_1$ is orthogonal to $w_1$. Note that $w_1$ is normalised. Then we can normalise $v_2$ to get $w_2 = \frac{v_2}{\| v_2 \|}$. Therefore, $w_1, w_2$ is the orthonormal basis of $V_2$. Then for general $k$-th step, we have:
\[
    v_k = x_k - \sum_{i = 1}^{k - 1} \langle w_i, x_k \rangle w_i = x_k - \sum_{i = 1}^{k - 1} \frac{\langle v_i, x_k \rangle}{\langle v_i, v_i \rangle} v_i, \quad w_k = \frac{v_k}{\| v_k \|}
\]
given that $w_1, w_2, \cdots, w_{k - 1}$ is the orthonormal basis of $V_{k - 1} = \Span\{x_1, x_2, \cdots, x_{k - 1}\}$ and the orthogonal basis of $V_{k - 1}$, $v_1, v_2, \cdots, v_{k - 1}$.

Then there is a useful corollary of the Gram-Schmidt process, the $QR$ Decomposition.

Let $V$ be a Euclidean space. We can interpret it as $(\R^n, \cdot)$ up to isomorphism. Then we have a basis $(\vec{x}_1, \vec{x}_2, \cdots, \vec{x}_n)$ of $V$ and we can form an invertible matrix $A$ whose columns are the vectors $\vec{x}_1, \vec{x}_2, \cdots, \vec{x}_n$, i.e.,
\[
    A = \begin{bmatrix}
        | & | & & | \\
        \vec{x}_1 & \vec{x}_2 & \cdots & \vec{x}_n \\
        | & | & & |
    \end{bmatrix}
\]
Then we have an orthogonal basis $(\vec{v}_1, \cdots, \vec{v}_n)$ of $V$ and an orthonormal basis $(\vec{w}_1, \cdots, \vec{w}_n)$ obtained by the Gram-Schmidt process. Then we should have an invertible matrix to convert between bases. Then what is the matrix to convert from the original basis to the orthonormal basis? 

Note that each $\vec{x}_k$ can be expressed as a linear combination of $\vec{w}_1, \cdots, \vec{w}_k$:
\[
    \vec{x}_k = \vec{v}_k + \sum_{i = 1}^{k - 1} \frac{\langle \vec{v}_i, \vec{x}_k \rangle}{\langle \vec{v}_i, \vec{v}_i \rangle} \vec{v}_i = \| \vec{v}_k \| \vec{w}_k + \sum_{i = 1}^{k - 1} \langle \vec{w}_i, \vec{x}_k \rangle \vec{w}_i
\]
Also, we can express $\vec{x}_k$ as follows:
\[
    \vec{x}_k = \begin{bmatrix}
        | & | & & | \\
        \vec{w}_1 & \vec{w}_2 & \cdots & \vec{w}_n \\
        | & | & & |
    \end{bmatrix} \begin{bmatrix}
        \langle \vec{w}_1, \vec{x}_k \rangle \\
        \langle \vec{w}_2, \vec{x}_k \rangle \\
        \vdots \\
        \langle \vec{w}_{k - 1}, \vec{x}_k \rangle \\
        \| \vec{v}_k \| \\
        0 \\
        \vdots \\
        0
    \end{bmatrix}
\]
Then we have the matrix equation:
\[
    \underbrace{\begin{bmatrix}
        | & | & & | \\
        \vec{x}_1 & \vec{x}_2 & \cdots & \vec{x}_n \\
        | & | & & |
    \end{bmatrix}}_{A} = \underbrace{\begin{bmatrix}
        | & | & & | \\
        \vec{w}_1 & \vec{w}_2 & \cdots & \vec{w}_n \\
        | & | & & |
    \end{bmatrix}}_{Q} \underbrace{\begin{bmatrix}
        \langle \vec{w}_1, \vec{x}_1 \rangle & \langle \vec{w}_1, \vec{x}_2 \rangle & \cdots & \langle \vec{w}_1, \vec{x}_n \rangle \\
        0 & \langle \vec{w}_2, \vec{x}_2 \rangle & \cdots & \langle \vec{w}_2, \vec{x}_n \rangle \\
        0 & 0 & \ddots & \vdots \\
        0 & 0 & 0 & \langle \vec{w}_n, \vec{x}_n \rangle
    \end{bmatrix}}_{R}
\]
which is called the \emph{QR Decomposition} of $A$, where $Q$ is an orthogonal matrix and $R$ is an upper-triangular matrix with positive diagonal entries. However, normally we denote the orthogonal matrix by $O$ instead of $Q$ and an upper-triangular matrix by $U$ instead of $R$.

\newpage

\section{Orthogonal Group and Special Orthogonal Group}

Let $V$ be a Euclidean space with inner product $\langle -, - \rangle$. Then we view $V$ as a linear space, and we have $\Aut(V) = \GL(V)$. If we view $V$ as a Euclidean space, then we have $\Aut(V) = \Orth(V) \subseteq \GL(V)$, where $\Orth(V)$ is the subgroup of $\GL(V)$ that respects the Euclidean structure, i.e., for all $T \in \Orth(V)$, we have:
\[
    \langle T(u), T(v) \rangle = \langle u, v \rangle
\]
for all $u, v \in V$, so length and angles are preserved under $T$. Or equivalently, the following diagram commutes:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        & V \times V \arrow[dr, "{\langle -, - \rangle}"] \\
        V \times V \arrow[ur, "T \times T"] \arrow[rr, "{\langle -, - \rangle}"'] & & \R
    \end{tikzcd}
\end{center}
We can also define the orthogonal group $\Orth(n)$ using this property. Let $V = \R^n$ with the dot product. Then for any $A \in \GL_n(\R)$, $A \in \Orth(n)$ if and only if $A$ satisfies:
\[
    \langle \vec{a}_i, \vec{a}_j \rangle = \langle A\vec{e}_i, A\vec{e}_j \rangle = \langle \vec{e}_i, \vec{e}_j \rangle = \delta_{ij}
\]
It is equivalent to say that $A^T A = I_n$, i.e., $A^T = A^{-1}$. Therefore, we have:
\[
    \Orth(n) = \{ A \in \GL_n(\R) \mid A^T A = I_n \}
\]
Note that $\det(A^T) = \det(A)^T = \det(A)$. Therefore, we have $\det(A)^2 = 1$ for all $A \in \Orth(n)$, i.e., $\det(A) = \pm 1$.

Then consider the following exact sequence:
\begin{center}
    \begin{tikzcd}
        1 \arrow[r] & \SL(V) \arrow[r, hook] & \GL(V) \arrow[r, "\det", two heads] & \R^\times \arrow[r] & 1
    \end{tikzcd}
\end{center}
where $\R^\times = \GL_1(\R) = \R \setminus \{ 0 \}$ is the multiplicative group of non-zero real numbers. As for any automorphism $A \in \GL(V)$, we have a determinant $\det{A} \in \R^\times$, which is surjective. $\SL(V)$ is defined as the kernel of the determinant map, i.e., $\SL(V) = \{ A \in \GL(V) \mid \det{A} = 1 \}$. 

Similarly, we have the special orthogonal group $\SO(V)$ as the subgroup of $\Orth(V)$ with determinant $1$:
\[
    \SO(V) = \{ A \in \Orth(V) \mid \det{A} = 1 \}
\]

\newpage

\section{Matrix Representation of Inner Products}

Let $V$ be a Euclidean space with inner product $\langle -, - \rangle$. Then we can choose a basis $v = (v_1, v_2, \cdots, v_n)$ of $V$. Then we have 
\[
    x = x^i v_i = \begin{bmatrix}
        x^1 \\
        x^2 \\
        \vdots \\
        x^n
    \end{bmatrix}, \quad y = y^i v_i = \begin{bmatrix}
        y^1 \\
        y^2 \\
        \vdots \\
        y^n
    \end{bmatrix}
\]
Then the inner product $\langle x, y \rangle$ can be represented as:
\[
    \langle x, y \rangle = x^i y^j \langle v_i, v_j \rangle = x^T \omega y = x \cdot (\omega y)
\]
where we let $\omega = [\langle v_i, v_j \rangle]$ be the matrix representation of the inner product with respect to the basis $v$. Then $\langle -, - \rangle = \cdot \omega -$. To find the canonical form of the inner product, we left it to the next chapter.

\begin{proposition}[Spectral Theorem for Real Symmetric Matrices]\label{prop:spectral-theorem-real-symmetric-matrices}
    Let $A$ be a $n \times n$ real symmetric matrix. Then there exists an orthogonal matrix $O$ and a diagonal matrix $D$ such that:
    \[
        A = O D O^{-1} = O D O^T
    \]
    where the entries of $D$ are the eigenvalues of $A$. Or equivalently, there exists an orthonormal basis of $\R^n$ consisting of eigenvectors of $A$.
\end{proposition}
To prove this proposition, we would use the result in Hermitian spaces, so we leave the proof to the next chapter.

\chapter{Hermitian Spaces}

\epigraph{``In Mathematics, one of the great ideas is anytime you are interested in vector space over real numbers, but real number is not as nice as complex numbers. So you should turn the problem into complex case, then use the result there to do it in real case.''}{Guowu Meng}

\section{Hermitian Forms and Unitary Groups}

\subsection{Hermitian Forms}

Similar to the definitions in Euclidean spaces, we can define Hermitian forms and Hermitian spaces as follows.
\begin{definition}[Hermitian Form]
    Let $V$ be a complex vector space. A \emph{Hermitian form} or \emph{Hermitian product} on $V$ is a map $\langle -, - \rangle : V \times V \to \mathbb{C}$ such that the following properties hold:
    \begin{enumerate}
        \item \emph{Sesquilinearity:} For all $u, v \in V$ and $\alpha \in \mathbb{C}$, we have:
        \begin{enumerate}
            \item Biadditivity
            \item $\langle u, \alpha v \rangle = \alpha \langle u, v \rangle$
            \item $\langle \alpha u, v \rangle = \overline{\alpha} \langle u, v \rangle$
        \end{enumerate}
        \item \emph{Conjugate Symmetry:} For all $u, v \in V$, we have:
        \[
            \langle u, v \rangle = \overline{\langle v, u \rangle} = \langle u, v \rangle^\dagger
        \]
        The dagger symbol $\dagger$ is defined as $\langle u, v \rangle^\dagger = \overline{\langle v, u \rangle}$.
        \item \emph{Positive-Definiteness:} For all $v \in V$, we have:
        \[
            \langle v, v \rangle \geq 0
        \]
    \end{enumerate}
    When the positive-definiteness property becomes non-degeneracy, i.e., $\langle v, v \rangle = 0$ implies $v = 0$, then the Hermitian form is called a \emph{pseudo Hermitian form}.
\end{definition}
We can also define the norm of a vector $v \in V$ as:
\[
    \| v \| = \sqrt{\langle v, v \rangle}
\]
The other four properties of norm is the same as in Euclidean spaces. Moreover the Cauchy-Schwarz inequality is as follows:
\[
    |\langle u, v \rangle| \leq \| u \| \| v \|
\]
for all $u, v \in V$, with equality if and only if $u$ and $v$ are linearly dependent. The proof is left as an exercise. 
% TODO: Add proof of Cauchy-Schwarz inequality in Hermitian spaces.

The sesquilinear map $\langle -, - \rangle$ can be defined as a bilinear map $\overline{V} \times V \to \mathbb{C}$, where $\overline{V}$ is the complex conjugate vector space of $V$, or linear map $\overline{V} \otimes V \to \mathbb{C}$. The complex conjuage vector space $\overline{V}$ is defined as the same set as $V$ with the same addition operation, but the scalar multiplication is defined as:
\[
    \mathbb{C} \times \overline{V} \to \overline{V}, \quad (\alpha, v) \mapsto \overline{\alpha} v
\]

Then we have the following examples:
\begin{example}
    We define the standard Hermitian form on $\mathbb{C}^n$ as:
    \[
        \langle \vec{u}, \vec{v} \rangle = \vec{u}^\dagger \vec{v} = \overline{\vec{u}}^T \vec{v}
    \]
    for all $\vec{u}, \vec{v} \in \mathbb{C}^n$. It is straightforward to verify that it satisfies all the properties of Hermitian forms. For example, the positive-definiteness property holds since:
    \[
        \vec{u}^\dagger \vec{u} = \sum_{i = 1}^n \overline{u_i} u_i = \sum_{i = 1}^n |u_i|^2 \geq 0
    \]
\end{example}

Then a complex linear space $V$ with an Hermitian form $\langle -, - \rangle$ is called a \emph{Hermitian space}. Also, the model / standard Hermitian space is $(\mathbb{C}^n, \langle -, - \rangle)$ with the standard Hermitian form, that is, the inner product defined above.

Let $V$ be a Hermitian space with Hermitian form $\langle -, - \rangle$. Then we say $u, v \in V$ are orthogonal if $\langle u, v \rangle = 0$. Similar to the Euclidean case, we can define orthogonal complement, orthogonal projection, orthonormal basis, and Gram-Schmidt process in Hermitian spaces. We also have the decomposition $V = W^\perp \oplus W$ for any subspace $W \subseteq V$.

Similarly, there is only one Hermitian space up to isomorphism with dimension $n$, that is, $(\mathbb{C}^n, \langle -, - \rangle)$ with the standard Hermitian form, i.e., for any Hermitian space $V$ with dimension $n$, there exists a linear isometric isomorphism between $V$ and $(\mathbb{C}^n, \langle -, - \rangle)$.

\subsection{Unitary Groups}

Similar to the orthogonal groups in Euclidean spaces, we can define unitary groups in Hermitian spaces as the automorphism groups that respect the Hermitian structure. Then we have 
\[
    \Uni(n) = \{ A \in \GL_n(\mathbb{C}) \mid A^\dagger A = I \}
\]
where $A^\dagger = \overline{A}^T$ is the conjugate transpose of $A$. Note that $\det(A^\dagger) = \overline{\det(A)}$. Therefore, we have $|\det(A)|^2 = 1$ for all $A \in \Uni(V)$, i.e., $|\det(A)| = 1$. This means $\Uni(1) = \{ z \in \mathbb{C} \mid |z| = 1 \}$ is the unit circle in the complex plane. Graphically we have:
\begin{center}
    \begin{tikzpicture}
        \draw[thick, -latex] (-3, 0) -- (3, 0) node[right] {Re};
        \draw[thick, -latex] (0, -3) -- (0, 3) node[above] {Im};
        \draw[thick] (0, 0) circle(2cm);
        \filldraw[fill=red] (2, 0) circle(2pt) node[below right] {1};
        \filldraw[fill=red] (-2, 0) circle(2pt) node[below left] {-1};
    \end{tikzpicture}
\end{center}
where the unit circle represents $\Uni(1)$ in the complex plane. Also in orthogonal group, the determinant of any orthogonal matrix is either $1$ or $-1$. This is the special case of unitary group when the entries are real numbers. Also we have the special unitary group $\SU(n)$ as the subgroup of $\Uni(n)$ with determinant $1$. 

Then we have the following definition similar to orthogonal matrices:
\begin{definition}[Unitary Matrix]
    A matrix $A \in \GL_n(\mathbb{C})$ is called a \emph{unitary matrix} if $A^\dagger A = I_n$, i.e., $A^{-1} = A^\dagger$.
\end{definition}
Using similar Gram-Schmidt process in Euclidean spaces, we get the following $QR$ decomposition in Hermitian spaces:
\[
    A = QR
\]
where $Q$ is a unitary matrix and $R$ is an upper-triangular matrix with positive real diagonal entries. However, normally we denote the unitary matrix by $U$ instead of $Q$. One reason why others use $QR$ instead is to distinguish the same notation on unitary and upper-triangular matrices in Hermitian spaces and orthogonal and upper-triangular matrices in Euclidean spaces.

\subsection{Matrix representation of Hermitian forms}

Then we have the matrix representation of Hermitian forms as follows.

Let $V$ be a Hermitian space with Hermitian form $\langle -, - \rangle$. Then we can choose a basis $v = (v_1, v_2, \cdots, v_n)$ of $V$. Then we have
\[
    \omega = [\langle v_i, v_j \rangle]
\]
Note that $\omega$ is a Hermitian matrix, i.e., $\omega^\dagger = \omega$. Then we claim that if $A$ and $\tilde{A}$ are two matrix representations of the Hermitian form $\langle -, - \rangle$ with respect to two different bases $v$ and $\tilde{v}$ respectively, then there exists an invertible matrix $P \in \GL_n(\mathbb{C})$ such that:
\[
    \tilde{A} = P^\dagger A P
\]
where $P$ is the change-of-basis matrix from $v$ to $\tilde{v}$. Or equivalently, 
\[
    \mathsf{H}_n(\mathbb{C}) \times \GL_n(\mathbb{C}) \to \mathsf{H}_n(\mathbb{C}), \quad (A, P) \mapsto P^\dagger A P
\]
where $\mathsf{H}_n(\mathbb{C})$ is the real linear space of Hermitian matrix of order $n$. The reason why it is real, as it is not closed under multiplication by complex numbers. Take $n = 1$, then $\mathsf{H}_1(\mathbb{C}) = \mathbb{R}$, which is not closed under multiplication by complex numbers.

\newpage

\section{Self-Adjoint Operators and Unitary Operators}

Let $V$ be a Hermitian space with Hermitian form $\langle -, - \rangle$. Then we have the following definitions.
\begin{definition}[Self-Adjoint Operator]
    A linear operator $T : V \to V$ is called a \emph{self-adjoint operator} or \emph{Hermitian operator} if:
    \[
        \langle Tu, v \rangle = \langle u, Tv \rangle
    \]
    for all $u, v \in V$. Or equivalently, $T = T^\dagger$, where $T^\dagger$ is the adjoint operator of $T$ defined as the unique operator satisfying:
    \[
        \langle Tu, v \rangle = \langle u, T^\dagger v \rangle
    \]
\end{definition}
\begin{definition}[Unitary Operator]
    A linear operator $U : V \to V$ is called a \emph{unitary operator} if:
    \[
        \langle Uu, Uv \rangle = \langle u, v \rangle
    \]
    for all $u, v \in V$. Or equivalently, $U^\dagger = U^{-1}$.
\end{definition}
\begin{definition}[Normal Operator]
    A linear operator $N : V \to V$ is called a \emph{normal operator} if:
    \[
        N^\dagger N = N N^\dagger
    \]
\end{definition}

\begin{proposition}
    For $T : V \to W$ a linear operator between two Hermitian spaces $V$ and $W$, there also exists a unique adjoint operator $T^\dagger : W \to V$ satisfying:
    \[
        \langle Tu, w \rangle_W = \langle u, T^\dagger w \rangle_V
    \]
\end{proposition}
\begin{proof}
    We can reduce the problem to $\mathbb{C}^n$ and $\mathbb{C}^m$ with standard Hermitian forms by choosing orthonormal bases of $V$ and $W$. Then we have $T$ represented by a matrix $A \in \M{m \times n}{\mathbb{C}}$. Then we propose there is a matrix $B \in \M{n \times m}{\mathbb{C}}$ such that for all $\vec{e}_i \in \mathbb{C}^n$ and $\vec{f}_j \in \mathbb{C}^m$, we have:
    \[
        \langle A\vec{e}_i, \vec{f}_j \rangle = (A\vec{e}_i)^\dagger \vec{f}_j = \vec{e}_i^\dagger A^\dagger \vec{f}_j = \vec{e}_i^T A^\dagger \vec{f}_j
    \]
    which is the $(i, j)$-th entry of $A^\dagger$. On the other hand, we have:
    \[
        \langle \vec{e}_i, B\vec{f}_j \rangle = \vec{e}_i^\dagger (B\vec{f}_j) = \vec{e}_i^T B \vec{f}_j
    \]
    which is the $(i, j)$-th entry of $B$. Therefore, we have $B = A^\dagger$. This proves the existence of the adjoint operator. The uniqueness is straightforward.
\end{proof}

\begin{proposition}
    Let $T$ be a self-adjoint operator on a Hermitian space $V$. Then we have the following properties:
    \begin{enumerate}
        \item All eigenvalues of $T$ are real numbers.
        \item Eigenspaces of $T$ are mutually orthogonal, i.e., if $u$ and $v$ are eigenvectors of $T$ corresponding to distinct eigenvalues, then $\langle u, v \rangle = 0$.
        \item $V$ is the direct sum of the eigenspaces of $T$.
    \end{enumerate}
    So $T$ is completely reducible.
\end{proposition}
\begin{proof}
    Given that $T^\dagger = T$, we have:
    \begin{enumerate}
        \item Let $\lambda \neq 0$ be an eigenvalue of $T$, so there exists a non-zero eigenvector $v$ such that $Tv = \lambda v$. Then we have:
        \[
            \langle Tv, v \rangle = \langle v, T^\dagger v \rangle = \langle v, Tv \rangle
        \]
        which implies that:
        \[
            \lambda \langle v, v \rangle = \overline{\lambda} \langle v, v \rangle
        \]
        Since $v \neq 0$, we have $\langle v, v \rangle > 0$. Therefore, we have $\lambda = \overline{\lambda}$, i.e., $\lambda$ is a real number.
        \item Let $\lambda_1$ and $\lambda_2$ be two distinct eigenvalues of $T$ with corresponding eigenvectors $v_1$ and $v_2$. Then we have:
        \[
            \langle Tv_1, v_2 \rangle = \langle v_1, T^\dagger v_2 \rangle
        \]
        which implies that:
        \[
            \lambda_1 \langle v_1, v_2 \rangle = \overline{\lambda_2} \langle v_1, v_2 \rangle
        \]
        Since $\lambda_1 \neq \lambda_2$, we have $\langle v_1, v_2 \rangle = 0$.
        \item We know that $V_{\lambda_1} (T) \otimes \cdots V_{\lambda_k} (T) \subseteq V$, where the spectrum of $T$, $\sigma(T) = \{ \lambda_1, \lambda_2, \cdots, \lambda_k \}$. To show the equality, we let $W = V_{\lambda_1} (T) \otimes \cdots V_{\lambda_k} (T)$ and consider the orthogonal complement $W^\perp$. Since $T$ is self-adjoint, we have $W^\perp$ is $T$-invariant, i.e., for all $w^\perp \in W^\perp$, we have $T w^\perp \in W^\perp$. As for all $w \in W$ and $w^\perp \in W^\perp$, we have:
        \[
            \langle T w^\perp, w \rangle = \langle w^\perp, T^\dagger w \rangle = \langle w^\perp, T w \rangle = 0
        \]
        where $T w \in W$ since $W$ is $T$-invariant. Then we propose that $W^\perp = \{ 0 \}$. If not, then we have an eigenvector $w^\perp \in W^\perp$ with eigenvalue $\lambda$, such that there exists a map $\tilde{T} : W^\perp \to W^\perp$ defined by $\tilde{T}(w^\perp) = T(w^\perp)$. Then $\tilde{T} w^\perp = \lambda w^\perp$ and $\tilde{T} w^\perp = T w^\perp$ by definition. So we know that $\lambda$ is an eigenvalue of $T$, i.e., $\lambda \in \sigma(T)$. Say $\lambda = \lambda_1$. Then we have $w^\perp \in V_{\lambda_1} (T) \subseteq W$, which contradicts the assumption that $w^\perp \in W^\perp$. Therefore, we have $W^\perp = \{ 0 \}$, which implies that $V = W$.
    \end{enumerate}
\end{proof}

\begin{proposition}
    Let $T$ be a unitary operator on a Hermitian space $V$. Then we have the following properties:
    \begin{enumerate}
        \item All eigenvalues of $T$ are complex numbers with absolute value $1$.
        \item Eigenspaces of $T$ are mutually orthogonal, i.e., if $u$ and $v$ are eigenvectors of $T$ corresponding to distinct eigenvalues, then $\langle u, v \rangle = 0$.
        \item $V$ is the direct sum of the eigenspaces of $T$.
    \end{enumerate}
    So $T$ is completely reducible.
\end{proposition}
The proof is left as an exercise.
% TODO: Add proof

\newpage

\section{Spectral Theorem}

The canonical matrix representation of self-adjoint operator is a real diagonal matrix, and the canonical matrix representation of unitary operator is a diagonal matrix with entries on the unit circle in the complex plane. This is stated in the following spectral theorem.

\begin{theorem}[Spectral Theorem]
    A Hermitian or unitary operator $T$ on a Hermitian space $V$ is ``diagonalisable'' by a unitary matrix in the following sense: Choose an orthonormal basis of $V$ such that $T$ is represented by a Hermitian matrix $A$. Then there is a unitary matrix $U$ and a diagonal matrix $D$ such that:
    \[
        A = U D U^{-1} = U D U^\dagger
    \]
\end{theorem}
Note that the diagonal entries of $D$ are all real numbers if $T$ is Hermitian, and the diagonal entries of $D$ are all complex numbers with modulus $1$ if $T$ is unitary. Moreover, $D$ is the cononical form of $T$, i.e., there exists a set of distinct complex eigenvalues $\{ \lambda_1, \lambda_2, \cdots, \lambda_k \}$ and a set of non-trivial complex linear subspaces $\{ V_{\lambda_1}, V_{\lambda_2}, \cdots, V_{\lambda_k} \}$ such that:
\[
    V = V_{\lambda_1} \oplus V_{\lambda_2} \oplus \cdots \oplus V_{\lambda_k}
\]
with respect to which $T$ has the decomposition:
\[
    T = \lambda_1 1_{V_{\lambda_1}} + \lambda_2 1_{V_{\lambda_2}} + \cdots + \lambda_k 1_{V_{\lambda_k}}
\]

If $U$ is a unitary matrix, then the columns of $U$ form an orthonormal basis of $\mathbb{C}^n$. Moreover, the columns of $U$ are eigenvectors of $A$ corresponding to the eigenvalues on the diagonal of $D$. As $\mathbb{C}^n = \bigoplus_i E_{\lambda_i} (A)$, where $\lambda_i$ are the eigenvalues of $A$, we have found an orthonormal basis consisting of eigenvectors of $A$.

If $V$ is a complex linear space, then $V$ is a real linear space with dimension doubled and we write $V_\R$ for the underlying real linear space of $V$. Then we losed some information from $V$ to $V_\R$. Then we add an extra structure $\J : V_\R \to V_\R$ defined by $\J(v) = iv$ for all $v \in V$. Then we have $\J^2 = -1_{V_\R}$. Such a structure is called an \emph{complex structure} on $V_\R$. Moreover, we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        \mathbb{C} \times V \arrow[r, "\text{complex scalar mult.}"] & V \\
        \mathbb{R} \times V_\R \arrow[u, "\iota \times \text{id}", hook] \arrow[ur, "\text{real scalar mult.}"'] 
    \end{tikzcd}
\end{center}
For example, we can write $(a + bi) v = a v + b \J(v)$ for all $a + bi \in \mathbb{C}$ and $v \in V$. Note that as $(\det \J)^2 = (-1)^{\dim_\R V}$, we have $\dim_\R V$ is even. The dimension doubled as we consider $v = (v_1, v_2, \cdots, v_n) \in V$ as $v_\R = (v_1, v_2, \cdots, v_n, \J v_1, \J v_2, \cdots, \J v_n) \in V_\R$.

Then we can do the reverse process. Let $W$ be a real linear space. The complexification of $W$, denoted by $W_\mathbb{C}$, is defined to be the following complex linear space:
\[
    W \otimes_\R \mathbb{C}
\]
Then we have the following natural identification:
\[
    \begin{split}
        W &\subseteq W \otimes_\R \mathbb{C} = W_\mathbb{C} \\
        w &\mapsto w \otimes_\R 1
    \end{split}
\]
Then $W$ is a real linear subspace of $W_\mathbb{C}$. Note that $\dim_\mathbb{C} W_\mathbb{C} = \dim_\R W$.

There are two corollories of the spectral theorem as follows.
\begin{corollary}
    If $A$ is a real symmetric matrix, then $A$ can be diagonalised by an orthogonal matrix, i.e., there is an orthogonal matrix $O$ and a diagonal matrix $D$ such that:
    \[
        A = O D O^{-1} = O D O^T
    \]
\end{corollary}
\begin{proof}
    \textbf{The proof is just a draft and needs to be polished.} % TODO: Polish the proof.

    Let $A$ be a real symmetric matrix. Then we can consider $A$ as a Hermitian matrix over $\mathbb{C}$. By the spectral theorem, there exists a unitary matrix $U$ and a diagonal matrix $D$ such that:
    \[
        A = U D U^\dagger
    \]
    where the entries of $D$ are all real numbers as $A$ is Hermitian. Then $D$ is also a real diagonal matrix. Given that $A$ is a real matrix and $D$ is a real diagonal matrix, then the eigenvectors of $A$ corresponding to the eigenvalues on the diagonal of $D$ can be chosen to be real vectors. Therefore, we can choose $U$ to be a real matrix. Moreover, as $U$ is unitary and real, we have $U$ is orthogonal. This completes the proof.
\end{proof}

\begin{corollary}
    The canonical form of a orthogonal matrix $O$ of order $n$ is of the following form:
    \[
        \begin{bmatrix}
            R_{\theta_1} J_q & & & & \\
            & R_{\theta_2} & & & \\
            & & \ddots & & \\
            & & & R_{\theta_k} & \\
            & & & & I_{p} \\
        \end{bmatrix}
    \]
    where $R_{\theta_i} = \begin{bmatrix}
        \cos \theta_i & -\sin \theta_i \\
        \sin \theta_i & \cos \theta_i
    \end{bmatrix}$ is the rotation matrix of angle $\theta_i$, $p = 1$ if $n$ is odd and $p = 0$ if $n$ is even, with $n = 2k + p$, and $J_q$ is $I_2$ if $\det O = 1$ and $\begin{bmatrix}
        -1 & 0 \\
        0 & 1
    \end{bmatrix}$ if $\det O = -1$.
\end{corollary}

\begin{corollary}
    The matrix representation $H$ of the Hermitian form on a complex vector space $V$ with respect to a basis $v$ is a Hermitian matrix. Moreover, there exists a unitary matrix $U$ and a real diagonal matrix $D$ such that:
    \[
        H = U D U^\dagger
    \]
    Then the Hermitian form can be represented as:
    \[
        \langle x, y \rangle = x^\dagger H y = x^\dagger U D U^\dagger y = (U^\dagger x)^\dagger D (U^\dagger y)
    \]
    Moreover, $D$ can be expressed as:
    \[
        D = \begin{bmatrix}
            \lambda \\
            & -\mu
            & & 0
        \end{bmatrix}
    \]
    where $\lambda = \begin{bmatrix}
        \lambda_1 & & \\
        & \ddots & \\
        & & \lambda_r
    \end{bmatrix}$ and $\mu = \begin{bmatrix}
        \mu_1 & & \\
        & \ddots & \\
        & & \mu_s
    \end{bmatrix}$ with $\lambda_i, \mu_j > 0$ for all $i, j$. The pair $(r, s)$ is called the \emph{signature} of the Hermitian form. We can further decompose the Hermitian form as:
    \[
        D = \begin{bmatrix}
            \sqrt{\lambda} \\
            & -\sqrt{\mu} \\
            & & 0
        \end{bmatrix} \begin{bmatrix}
            I_r \\
            & -I_s \\
            & & 0
        \end{bmatrix} \begin{bmatrix}
            \sqrt{\lambda} \\
            & -\sqrt{\mu} \\
            & & 0
        \end{bmatrix} = U' I_{r, s} U'^\dagger
    \]
    where $\sqrt{\lambda} = \begin{bmatrix}
        \sqrt{\lambda_1} & & \\
        & \ddots & \\
        & & \sqrt{\lambda_r}
    \end{bmatrix}$ and $\sqrt{\mu} = \begin{bmatrix}
        \sqrt{\mu_1} & & \\
        & \ddots & \\
        & & \sqrt{\mu_s}
    \end{bmatrix}$.

    So the Hermitian form can be represented as:
    \[
        \langle x, y \rangle = (U'^\dagger U^\dagger x)^\dagger I_{r, s} (U'^\dagger U^\dagger y)
    \]
\end{corollary}

In summary,
\begin{itemize}
    \item Any Hermitian form on a complex vector space can be represented by a Hermitian matrix.
    \item The canonical representation of Hermitian form is $I_{r, s}$ up to a unitary change of basis. If the Hermitian form is positive-definite, then the canonical representation is $I_n$.
    \item Any symmetric 2-form $\omega$ on a real vector space can be represented by a real symmetric matrix.
    \item The canonical representation of symmetric 2-form is $\begin{bmatrix}
        I_{r} & \\
        & -I_{s} \\
        & & 0
    \end{bmatrix}$ up to an orthogonal change of basis. If the symmetric 2-form is positive-definite, then the canonical representation is $I_n$.
    \item The canonical representation of pseudo inner product is $\begin{bmatrix}
        I_{p} & \\
        & -I_{q}
    \end{bmatrix}$ up to an orthogonal change of basis, with $n = p + q$. Then we call $(p, q)$ the signature of the pseudo inner product.
    \item $V$ is a real vector space of dimension $n$. Then up to isomorphism, there are $n + 1$ different pseudo inner products on $V$, corresponding to the signatures $(n, 0), (n - 1, 1), \cdots, (1, n - 1), (0, n)$.
    \item Any pseudo inner product $V$ is isomorphic to $(\R^n , I_{p,q}) = \R^{p,q}$. As we send $(x, y) \to x_1 y_1 + \cdots + x_p y_p - x_{p+1} y_{p+1} - \cdots - x_n y_n$.
\end{itemize}

The set of inner products on a real vector space $V$ of dimension $n$ is isomorphic to the orbit space of the right action of group $\Orth(n)$ on $\GL_n(\R)$ $\GL_n(\R) / \Orth(n)$, where $\Orth(n)$ is the orthogonal group of order $n$. 
\[
    \GL_n(\R) \times \Orth(n) \to \GL_n(\R), \quad (X, g) \mapsto X \cdot g
\]
As $\GL_n(\R)$ and $\Orth(n)$ have the same homotopy type, the orbit space $\GL_n(\R) / \Orth(n)$ is trivially contractible. We may consider the following example:
\[
    \GL_1(\R) = \R^\times \quad \Orth(1) = \{ -1, 1 \}
\]
Then we have:
\[
    \GL_1(\R) / \Orth(1) \cong \R_{> 0}
\]

Similarly, the set of Hermitian forms on a complex vector space $V$ of dimension $n$ is isomorphic to the orbit space $\GL_n(\mathbb{C}) / \Uni(n)$, where $\Uni(n)$ is the unitary group of order $n$. Again, it is contractible.

We have a simple introduction to the Lorentz inner product on $\R^4$. It sends $(x, y) \in \R^4 \times \R^4$ to $x \cdot y = x_0 y_0 - \vec{x} \cdot \vec{y}$, where $x = \begin{bmatrix}
    x_0 \\
    \vec{x}
\end{bmatrix}$ and $y = \begin{bmatrix}
    y_0 \\
    \vec{y}
\end{bmatrix}$.

\chapter{Symplectic Vector Spaces}

\epigraph{By now, you should feel comfortable switching between the 2 pictures. One is the abstract picture. Another is a concrete presentation.}{Guowu Meng}

\section{Symplectic Forms}

Let $(V, \langle -, - \rangle)$ be a Hermitian space. Then we have:
\begin{center}
    \begin{tikzcd}
        & & \R \\
        \overline{V} \times V \arrow[r, "{\langle -, - \rangle}"] \arrow[urr, bend left, "{g(-, -)}"] \arrow[drr, bend right, "\omega"'] & \mathbb{C} \arrow[ur, "\Re"] \arrow[dr, "\Im"'] \\
        & & \R 
    \end{tikzcd}
\end{center}
where $g(-, -)$ is the real part of the Hermitian product and $\omega$ is the imaginary part of the Hermitian product. Both of them are 2-forms on $V_\R$. $\omega$ is called a \emph{symplectic form} on $V$.

\begin{definition}[Symplectic form]
    A \emph{symplectic form} on a real vector space $V$ is a non-degenerate, skew-symmetric 2-form $\omega : V \times V \to \R$.
\end{definition}
A symplectic vector space is a pair $(V, \omega)$.

We have $\J \in \End{V_\R}$ defined as the scalar multiplication by $i$ on $V_\R$, such that $\J^2 = -1_{V_\R}$. 

Note that we have three structures on $V_\R$:
\begin{itemize}
    \item \textbf{Complex structure:} $\J : V_\R \to V_\R$ with $\J^2 = -1_{V_\R}$;
    \item \textbf{Symplectic structure:} $\omega : V_\R \times V_\R \to \R$ is a non-degenerate, skew-symmetric bilinear form;
    \item \textbf{Riemannian structure:} $g : V_\R \times V_\R \to \R$ is a positive-definite, symmetric bilinear form.
\end{itemize}

Then we have the following equation:
\[
    \langle x, y \rangle = g(x, y) + i \omega (x, y)
\]
for all $x, y \in V_\R$. Moreover, we have:
\[
    \langle ix, y \rangle = -i \langle x, y \rangle \implies g(\J x, y) + i \omega (\J x, y) = \omega (x, y) - i g(x, y)
\]
for all $x, y \in V_\R$. This implies that:
\[
    \omega (x, y) = g(\J x, y), \quad g(x, y) = -\omega (x, \J y)
\]
Consider the following equation:
\[
    \langle ix, iy \rangle = \langle x, y \rangle \implies g(\J x, \J y) + i \omega (\J x, \J y) = g(x, y) + i \omega (x, y)
\]
for all $x, y \in V_\R$. This implies that:
\[
    g(\J x, \J y) = g(x, y), \quad \omega (\J x, \J y) = \omega (x, y)
\]
for all $x, y \in V_\R$. Or equivalently, we have $\J^* g = g$ and $\J^* \omega = \omega$.

Note that the Hermitian product is positive-definite, so we have
\[
    \langle x, x \rangle > 0 \implies g(x, x) > 0, \omega (x, x) = 0
\]
for all $x \in V_\R \setminus \{ 0 \}$. If $x = 0$, then we have $\langle 0, 0 \rangle = 0$, $g(0, 0) = 0$ and $\omega (0, 0) = 0$. Also, we have 
\[
    \overline{\langle y, x \rangle} = \langle x, y \rangle \implies g(y, x) - i \omega (y, x) = g(x, y) + i \omega (x, y)
\]
for all $x, y \in V_\R$. This implies that:
\[
    g(x, y) = g(y, x), \quad \omega (x, y) = -\omega (y, x)
\]
for all $x, y \in V_\R$, i.e., $g$ is symmetric and $\omega$ is skew-symmetric.

As $\omega (x, y) = g(\J x, y)$ for all $x, y \in V_\R$, so $\omega$ is non-degenerate if $g$ is non-degenerate. Then we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}[column sep=normal]
        V_\R \arrow[dr, "\J"'] \arrow[rr, "\omega_{\musNatural}"] & & V_\R^* \\
        & V_\R \arrow[ur, "g_{\musNatural}"'] &
    \end{tikzcd}
\end{center}
As $\omega_{\musNatural} (x) = g_{\musNatural}(\J x)$ for all $x \in V_\R$.

Then we can recover a Hermitian space from a real vector space with these structures. Let $V$ be a real vector space. If any two of the above three structures are given and compatible, the third will be determined. Moreover, we have a Hermitian product on $V$ on the complex linear space $(V, \J)$ where $i v = \J v$ for all $v \in V$.

The meaning of being compatible pair:
\begin{itemize}
    \item $(g, \J)$ are compatible if $\J^* g = g$, i.e., $J \in \Aut(W, g) = \Orth(W, g)$; Then we can define $\omega (x, y) = g(\J x, y)$ and $\langle -, - \rangle = g + i \omega$. We can check that $\omega$ is skew-symmetric and non-degenerate, and $\langle -, - \rangle$ is a Hermitian product:
    \[
        \omega (y, x) = g(\J y, x) = g(\J \J y, \J x) = g(-y, \J x) = -g(\J x, y) = -\omega (x, y)
    \]
    Also, if $\omega (x, y) = 0$ for all $y \in V$, then we have $g(\J x, y) = 0$ for all $y \in V$, which implies that $\J x = 0$ as $g$ is non-degenerate, i.e., $x = 0$. Therefore, $\omega$ is non-degenerate. As for the Hermitian product, the sesquilinearity is shown as follows:
    \[
        \langle ix, y \rangle = g(\J x, y) + i \omega (\J x, y) = \omega (x, y) - i g(x, y) = -i (g(x, y) + i \omega (x, y)) = -i \langle x, y \rangle
    \]
    For the conjugate symmetry, we have:
    \[
        \langle y, x \rangle = g(y, x) + i \omega (y, x) = g(x, y) - i \omega (x, y) = \overline{\langle x, y \rangle}
    \]
    for all $x, y \in V$. Also, we have:
    \[
        \langle x, x \rangle = g(x, x) + i \omega (x, x) = g(x, x) > 0
    \]

    \item $(\omega, \J)$ are compatible if $\J^* \omega = \omega$, i.e., $\J \in \Aut(W, \omega) = \Symp(W, \omega)$ and $-\omega (\J x, x) \geq 0$ and equality holds if and only if $x = 0$. Then we can define $g(x, y) = -\omega (x, \J y)$ and $\langle -, - \rangle = g + i \omega$. We can check that $g$ is symmetric and positive-definite, and $\langle -, - \rangle$ is a Hermitian product:
    \[
        g(y, x) = -\omega (y, \J x) = -\omega (\J y, \J \J x) = -\omega (\J y, -x) = -\omega (x, \J y) = g(x, y)
    \]
    Also, as $-\omega (\J x, x) \geq 0$ for all $x \in V$ and equality holds if and only if $x = 0$, we have $g(x, x) \geq 0$ for all $x \in V$ and equality holds if and only if $x = 0$. Therefore, $g$ is positive-definite. For the Hermitian product, we may use the similar proof as above.

    \item $(g, \omega)$ are compatible if $\omega (x, y) = g(A x, y)$ for some $A \in \End{V}$. If $A^2 = -1$, then $\J = A$. In general, $A$ is skew-symmetric, i.e., $g(A x, y) = g(x, -A y)$, as $\omega$ is skew-symmetric. Since $A A^\dagger$ is symmetric and positive-definite, we can define $\J = \sqrt{A A^\dagger}^{-1} A$, which satisfies that $\J^2 = -1$, as $\J$ commutes with $A$ and $\sqrt{A A^\dagger}$. Then we have $A = \sqrt{A A^\dagger} \J$ and let $P = \sqrt{A A^\dagger}$. Therefore, we have:
    \[
        \omega (\J x, \J y) = g(A \J x, \J y) = g(P \J \J x, \J y) = -g(P x, \J y) = g(\J P x, y) = g(A x, y) = \omega (x, y).
    \]
    Also, we have:
    \[
        -\omega (\J x, x) = -g(A \J x, x) = -g(P \J \J x, x) = g(P x, x) > 0
    \]
    for all $x, y \in V$ and $x \neq 0$. Then we can define $\langle -, - \rangle = g + i \omega$. We can check that $\langle -, - \rangle$ is a Hermitian product by the similar proof as above.
\end{itemize}

Let $V$ be a vector space over $\F$ where $\chart \F \neq 2$. We define the double $D(V) = V \oplus V^*$. Then we have a natural symplectic form on $D(V)$ defined as:
\[
    \omega ((u, \alpha), (v, \beta)) = \alpha(v) - \beta(u)
\]
Also $D(V)$ is called the \emph{canonical symplectic vector space} associated to $V$. Then when we choose a basis $\{ \vec{e}_1, \vec{e}_2, \cdots, \vec{e}_n \}$ of $V$ and the dual basis $\{ \hat{e}^1, \hat{e}^2, \cdots, \hat{e}^n \}$ of $V^*$, we have the matrix representation of $\omega$ on $D(V)$ as:
\[
    \begin{bmatrix}
        \omega (\vec{e}_i, \vec{e}_j) & \omega (\vec{e}_i, \hat{e}^j) \\
        \omega (\hat{e}^i, \vec{e}_j) & \omega (\hat{e}^i, \hat{e}^j)
    \end{bmatrix} = \begin{bmatrix}
        0 & I_n \\
        -I_n & 0
    \end{bmatrix}
\]
Also the basis $\{ \vec{e}_1, \cdots, \vec{e}_n, \hat{e}^1, \cdots, \hat{e}^n \}$ is called a \emph{symplectic basis} of $D(V)$.

\newpage

\section{Matrix Representation and Canonical Form}

We may revise all the canonical forms we have learned before as follows.

\subsection{Linear Maps}
Consider a linear map $T : V_1 \to V_2$ between two vector spaces $V_1$ and $V_2$ of dimensions $n$ and $m$ respectively. Then we have:
\begin{center}
    \begin{tikzcd}
        \F^n \arrow[r, "A'"] \arrow[dd, bend right, "P"'] & \F^m \\
        V_1 \arrow[u, "\cong"] \arrow[d, "\cong"'] \arrow[r, "T"] & V_2 \arrow[u, "\cong"'] \arrow[d, "\cong"] \\
        \F^n \arrow[r, "A"'] & \F^m \arrow[uu, bend right, "Q"']
    \end{tikzcd}
\end{center}
where $A$ and $A'$ are the matrix representations of $T$ with respect to different bases of $V_1$ and $V_2$ and $P \in \GL_n(\F)$ and $Q \in \GL_m(\F)$ are the change-of-basis matrices. $P$ represents the column operations on $A$ and $Q$ represents the row operations on $A$. Then we have:
\[
    A P = Q A', \quad A = Q A' P^{-1}
\]
Then we have the left group action of $\GL_m(\F) \times \GL_n(\F)$ on the set of $m \times n$ matrices $\M{m \times n}{\F}$ defined as:
\[
    (Q, P) \cdot A = Q A P^{-1}
\]
The canonical form of $A$ under this group action is:
\[
    \begin{bmatrix}
        I_r & 0 \\
        0 & 0
    \end{bmatrix}
\]

\subsection{Linear Endomorphisms}
Consider a linear endomorphism $T : V \to V$ on a vector space $V$ of dimension $n$. Then we have:
\begin{center}
    \begin{tikzcd}
        \F^n \arrow[r, "A'"] \arrow[dd, bend right, "P"'] & \F^n \arrow[dd, bend left, "P"'] \\
        V \arrow[u, "\cong"] \arrow[d, "\cong"'] \arrow[r, "T"] & V \arrow[u, "\cong"'] \arrow[d, "\cong"] \\
        \F^n \arrow[r, "A"'] & \F^n
    \end{tikzcd}
\end{center}
where $A$ and $A'$ are the matrix representations of $T$ with respect to different bases of $V$ and $P \in \GL_n(\F)$ is the change-of-basis matrix. Then we have:
\[
    A P = P A', \quad A = P A' P^{-1}
\]
Then we have the left group action of $\GL_n(\F)$ on the set of $n \times n$ matrices $\M{n \times n}{\F}$ defined as:
\[
    P \cdot A = P A P^{-1}
\]

The actual canonical form of $A$ is complicated (Rational Canonical Form), but in generic case, they are diagonal matrix.

\subsection{2-Forms}
Consider a 2-form $\omega : V \times V \to \F$ on a vector space $V$ of dimension $n$. Then we have:
\begin{center}
    \begin{tikzcd}
        V \times V \arrow[r, "\omega"] & \F \\
        \F^n \times \F^n \arrow[u, "\cong"] \arrow[ur, dashed]
    \end{tikzcd}
\end{center}
Then $[\omega]_{v} = [\omega (v_i, v_j)]$ is the matrix representation of $\omega$ with respect to the basis $v = \{ v_1, v_2, \cdots, v_n \}$ of $V$. If we change the basis of $V$ to $u$, then there is a unique invertible matrix, $P \in \GL_n(\F)$, such that $u_j = \sum_i v_i P^i_j$ for all $j$. Then we have:
\[
    \begin{split}
        [\omega]_u = [\omega (u_i, u_j)] &= [\omega (\sum_k v_k P^k_i, \sum_l v_l P^l_j)] \\
        &= [\sum_{k, l} P^k_i \omega (v_k, v_l) P^l_j] \\
        &= [\sum_{k, l} (P^T)^i_k \omega (v_k, v_l) P^l_j] \\
        &= P^T [\omega (v_k, v_l)] P \\
    \end{split}
\]
So we have the right group action of $\GL_n(\F)$ on the set of $n \times n$ matrices $\M{n \times n}{\F}$ defined as:
\[
    A \cdot P = P^T A P
\]
We may check that $(A \cdot P_1) \cdot P_2 = A \cdot (P_1 P_2)$ for all $A \in \M{n \times n}{\F}$ and $P_1, P_2 \in \GL_n(\F)$.

Note that the right action leaves the symmetric and skew-symmetric properties invariant, i.e., if $A^T = A$ (or $A^T = -A$), then we have $(P^T A P)^T = P^T A P$ (or $(P^T A P)^T = -P^T A P$) for all $P \in \GL_n(\F)$. For symmetric 2-forms, as $(P^T A P)^T = P^T A^T (P^T)^T = P^T A^T P$, where $A^T = A$, so we have $(P^T A P)^T = P^T A P$. For skew-symmetric 2-forms, as $(P^T A P)^T = P^T A^T (P^T)^T = P^T (-A) P$, where $A^T = -A$, so we have $(P^T A P)^T = -P^T A P$.

When $\F = \R$, then the $\omega$ being symmetric or skew-symmetric corresponds to the matrix representation being real symmetric or real skew-symmetric respectively. If $\omega$ is symmetric, then the representation $A$ is a Hermitian matrix, and we have $A = O D O^T$ for some orthogonal matrix $O$ and diagonal matrix $D$. Then the canonical form of symmetric 2-form is:
\[
    \begin{bmatrix}
        I_r & & \\
        & -I_s & \\
        & & 0
    \end{bmatrix}
\]
where $r + s \leq n$ and $r + s + t = n$. If $\omega$ is skew-symmetric, then the $iA$ is a Hermitian matrix, and we have $iA = U D U^\dagger$ for some unitary matrix $U$ and real diagonal matrix $D$. Then the canonical form of skew-symmetric 2-form is:
\[
    \begin{bmatrix}
        J_2 & & \\
        & \ddots \\
        & & J_2 \\
        & & & 0
    \end{bmatrix}
\]
where $J_2 = \begin{bmatrix}
    0 & -1 \\
    1 & 0
\end{bmatrix}$ and the canonical form can be represented by $J_2 \oplus J_2 \oplus \cdots \oplus J_2 \oplus 0$. Note that $J_2^2 = -I_2$.

The canonical form of a pseudo inner product on a real linear space of dimension $n$ is $I_{p, q} = \begin{bmatrix}
    I_p & 0 \\
    0 & -I_q
\end{bmatrix}$ where $p + q = n$. The basis inside the canonical representation is called \emph{pseudo-orthonormal} basis.

A pseudo Euclidean space is isomorphic to $\R^{p, q} := (\R^n, (\vec{x}, \vec{y}) \mapsto \vec{x} \cdot I_{p, q} \vec{y})$. In case the dimension of $V$ is $n$, then up to isomorphism, there are $n + 1$ pseudo Euclidean structures on $V$, namely, $\R^{0, n}, \R^{1, n-1}, \cdots, \R^{n, 0}$. Note that $(v_i, v_j) = \delta_{ij}$ for $1 \leq i, j \leq p$, $(v_i, v_j) = -\delta_{ij}$ for $p + 1 \leq i, j \leq n$ and $(v_i, v_j) = 0$ otherwise.

Up to isomorphism, there is only one real symplectic vector space of dimension $2n$, i.e., $D(\R^n) := \R^n \oplus (\R^n)^*$ with the canonical symplectic form. The representation of the symplectic form is 
\[
\begin{bmatrix}
    0 & I \\
    -I & 0
\end{bmatrix}
\]
with respect to the symplectic basis: $(x_1, \cdots, x_n, x^1, \cdots, x^n)$, where $\{ x_1, \cdots, x_n \}$ is the standard basis of $\R^n$ and $\{ x^1, \cdots, x^n \}$ is the dual basis of $(\R^n)^*$. Also, $\omega(x_i, x_j) = \omega(x^i, x^j) = 0$ and $\omega(x_i, x^j) = \delta_i^j = -\omega(x^j, x_i)$ for all $i, j$.

Note that we have $A^T = -A$ where $A$ is the representation of a symplectic form. As $\det A^T = \det A = (-1)^n \det A$, we know that $n$ has to be even. Moreover, if we consider the a non-degenerate skew-symmetric 2-form on a real vector space of dimension $2n$, then its canonical form is:
\[
    \begin{bmatrix}
        J_2 & & \\
        & \ddots \\
        & & J_2
    \end{bmatrix}
\]
where $J_2 = \begin{bmatrix}
    0 & -1 \\
    1 & 0
\end{bmatrix}$. Note that this is similar to the canonical form of symplectic forms mentioned above.

\chapter{Further Topics}

\epigraph{When you studying higher maths, whether algebra, geometry or anything, you realised that the hard part is the language. It takes time. People are impatient. If you are impatient, you cannot learn mathematics. But if you are patient enough, you learn the language, you understand the basic facts. No tricks, tricks are useless. And then towards the end, you enjoy the fruit, that means, everything become so easy. Just do a simple calculation. You can get many result. The center of mathematics is always like that.}{Guowu Meng}

\section{Polar Decomposition and Singular Value Decomposition}
\subsection{Polar Decomposition}
If $z \neq 0$, then $z = \rho e^{i \theta}$ for a unique $\rho > 0$ and $e^{i \theta}$ being a complex number of modulus 1. This is called the polar decomposition of $z$. Then we have the following isomorphism:
\[
    \GL_1 (\mathbb{C}) = \Uni(1) \cdot \mathsf{H}_1^{> 0} (\mathbb{C}), \quad [z] \mapsto [e^{i \theta}] \cdot [\rho]
\]
where $\mathsf{H}_1^{> 0} (\mathbb{C})$ is the set of positive Hermitian $1 \times 1$ matrices, i.e., positive real numbers, and $\Uni(1)$ is the set of complex numbers of modulus 1. 

Then we may generalise this to matrices, i.e.,
\[
    \GL_n (\mathbb{C}) = \Uni(n) \cdot \mathsf{H}_n^{> 0} (\mathbb{C}), \quad [A] \mapsto [U] \cdot [P]
\]
where $\mathsf{H}_n^{> 0} (\mathbb{C})$ is the set of positive Hermitian $n \times n$ matrices and $\Uni(n)$ is the unitary group of order $n$. Then we claim that any invertible matrix $A$ can be uniquely decomposed as $A = P U$ for some $P \in \mathsf{H}_n^{> 0} (\mathbb{C})$ and $U \in \Uni(n)$, and we call this the \emph{polar decomposition} of $A$.

\begin{proof}
    Assume the existance, if $A = U P$ then $A^\dagger = P U^\dagger$. Then we have:
    \[
        A^\dagger A = P U^\dagger U P = P^2
    \]
    As $A$ is invertible, so is $A^\dagger A$. Therefore, $P = \sqrt{A^\dagger A}$ is a positive Hermitian matrix. Then we have $U = A P^{-1}$. Also, we have:
    \[
        (A^\dagger A)^\dagger = A^\dagger A \implies P^\dagger P^\dagger = P^2 \implies P^\dagger = P
    \]
    and
    \[
        \vec{z}^\dagger A^\dagger A \vec{z} = (A \vec{z})^\dagger (A \vec{z}) > 0 \implies \| P \vec{z} \| \geq 0
    \]
    for all $\vec{z}$ and equal to 0 if and only if $\vec{z} = 0$ as $A \in \GL_n(\mathbb{C})$. Therefore, $P$ and $A^\dagger A$ are positive Hermitian. Then we know that $A^\dagger A = U' D U'^\dagger$ where $U' \in \Uni(n)$ and $D$ is a diagonal matrix with positive real numbers on the diagonal. Then we have $P = U' \sqrt{D} U'^\dagger$. Also, we have:
    \[
        P^2 = U' \sqrt{D} U'^\dagger U' \sqrt{D} U'^\dagger = U' D U'^\dagger = A^\dagger A
    \]
    Then we have:
    \[
        U^\dagger U = P^{-1} A^\dagger A P^{-1} = P^{-1} P^2 P^{-1} = I_n
    \]
    Therefore, $U \in \Uni(n)$.
\end{proof}

If it is real number, then we have the similar polar decomposition:
\[
    \GL_n (\mathbb{R}) = \Orth(n) \cdot \mathsf{S}_n^{> 0} (\mathbb{R}), \quad [A] \mapsto [O] \cdot [S]
\]
where $\mathsf{S}_n^{> 0} (\mathbb{R})$ is the set of positive symmetric $n \times n$ matrices and $\Orth(n)$ is the orthogonal group of order $n$.

\subsection{Singlular Value Decomposition}
The corollary of polar decomposition is the singular value decomposition.

We consider the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        \nul{A} \arrow[d, hook] & \col{A} \arrow[d, hook] \\
        \mathbb{C}^n \arrow[r, "A"] \arrow[d, equal] & \mathbb{C}^m \arrow[d, equal] \\
        (\nul{A})^\perp \oplus \nul{A} \arrow[d, "\cong"'] \arrow[r, "A"] & \col{A} \oplus (\col{A})^\perp \arrow[d, "\cong"] \\
        \mathbb{C}^r \oplus \mathbb{C}^{n-r} \arrow[d, "\cong"'] & \mathbb{C}^r \oplus \mathbb{C}^{m-r} \arrow[d, "\cong"] \\
        \mathbb{C}^n \arrow[r, "A'"] & \mathbb{C}^m \\[-3.6em]
        \Span\{ \vec{e}_1, \cdots, \vec{e}_r \} \oplus \Span\{ \vec{e}_{r+1}, \cdots, \vec{e}_n \} & \Span\{ \vec{e}_1, \cdots, \vec{e}_r \} \oplus \Span\{ \vec{e}_{r+1}, \cdots, \vec{e}_m \}
    \end{tikzcd}
\end{center}
where $A' = \begin{bmatrix}
    \overline{A} & 0 \\
    0 & 0
\end{bmatrix}$ with $\overline{A} \in \GL_r(\mathbb{C})$. Moreover, the direct sum in $(\nul{A})^\perp \oplus \nul{A}$ and $\col{A} \oplus (\col{A})^\perp$ are orthogonal direct sums; the direct sum in $\mathbb{C}^r \oplus \mathbb{C}^{n-r}$ and $\mathbb{C}^r \oplus \mathbb{C}^{m-r}$ are external direct sums; the direct sum in $\Span\{ \vec{e}_1, \cdots, \vec{e}_r \} \oplus \Span\{ \vec{e}_{r+1}, \cdots, \vec{e}_n \}$ and $\Span\{ \vec{e}_1, \cdots, \vec{e}_r \} \oplus \Span\{ \vec{e}_{r+1}, \cdots, \vec{e}_m \}$ are internal direct sums. Note that all the isomorphisms in the diagram are of Hermitian spaces. Then we may simplify the diagram as follows:
\begin{center}
    \begin{tikzcd}
        \mathbb{C}^n \arrow[r, "A"] \arrow[d, "U_1"'] & \mathbb{C}^m \arrow[d, "U_2"] \\
        \mathbb{C}^n \arrow[r, "A'"] & \mathbb{C}^m
    \end{tikzcd}
\end{center}
As $\overline{A} \in \GL_r(\mathbb{C})$, we have the polar decomposition $\overline{A} = U_3 P$ for some $P \in \mathsf{H}_r^{> 0} (\mathbb{C})$ and $U_3 \in \Uni(r)$. Moreover, we may further decompose $P$ as $P = U_4 D_\lambda U_4^\dagger$ for some $U_4 \in \Uni(r)$ and $D_\lambda$ being a diagonal matrix with positive real numbers on the diagonal. Then we have:
\[
    \begin{split}
        U_2 A &= \begin{bmatrix}
            \overline{A} & 0 \\
            0 & 0
        \end{bmatrix} U_1 \\
        A &= U_2^\dagger \begin{bmatrix}
            U_3 U_4 D_\lambda U_4^\dagger & 0 \\
            0 & 0
        \end{bmatrix} U_1 \\
        &= \left(U_2^\dagger \begin{bmatrix}
            U_3 U_4 & 0 \\
            0 & I_{m-r}
        \end{bmatrix}\right) \begin{bmatrix}
            D_\lambda & 0 \\
            0 & 0
        \end{bmatrix} \left(\begin{bmatrix}
            U_4^\dagger & 0 \\
            0 & I_{n-r}
        \end{bmatrix} U_1\right)
    \end{split}
\]
Then we have the singular value decomposition of $A$:
\[
    A = U \Sigma V^\dagger
\]
where $U = U_2^\dagger \begin{bmatrix}
    U_3 U_4 & 0 \\
    0 & I_{m-r}
\end{bmatrix}$, \quad
$\Sigma = \begin{bmatrix}
    D_\lambda & 0 \\
    0 & 0
\end{bmatrix}$, \quad
$V = U_1^\dagger \begin{bmatrix}
    U_4 & 0 \\
    0 & I_{n-r}
\end{bmatrix}$.

\begin{theorem}[Singular Value Decomposition]
    For any $A \in \M{m \times n}{\mathbb{C}}$, there exist unitary matrices $U \in \Uni(m)$, $V \in \Uni(n)$ and a set of positive numbers $\{ \lambda_1, \cdots, \lambda_r \}$ such that:
    \[
        A = U \Sigma V^\dagger, \quad \Sigma = \begin{bmatrix}
            \lambda_1 & & & & \\
            & \lambda_2 & & & \\
            & & \ddots & & \\
            & & & \lambda_r & \\
            & & & & 0
        \end{bmatrix}
    \]
\end{theorem}

\newpage

\section{Simultaneous Diagonalisation Theorem}

\begin{theorem}[Simultaneous Diagonalisation Theorem]
    Suppose that $A_1, \cdots, A_k$ are mutually commuting Hermitian matrices of order $n$, i.e., $A_i \in \mathsf{H}_n(\mathbb{C})$ and $[A_i, A_j] := A_i A_j - A_j A_i = 0$ for all $1 \leq i, j \leq k$, where $[A_i, A_j]$ is called the commutator of $A_i$ and $A_j$. Then there is a set of distinct vectors $\vec{\lambda_\alpha} \in \R^k$ for $\alpha = 1, 2, \cdots, l$ and an orthogonal decomposition of $\mathbb{C}^n$ into non-trivial subspaces:
    \[
        \mathbb{C}^n = \bigoplus_{\alpha=1}^l E_{\vec{\lambda_\alpha}}
    \]
    such that for all $\vec{z} \in E_{\vec{\lambda_\alpha}}$ and $A_i \vec{z} = \lambda_\alpha(i) \vec{z}$ for all $1 \leq i \leq k$. In particular, there is a unitary matrix $U \in \Uni(n)$ such that:
    \[
        A_i = U D_i U^\dagger, \quad D_i = \begin{bmatrix}
            d_1 (i) \\
            & \ddots \\
            & & d_n (i)
        \end{bmatrix} \in \M{n \times n}{\R}
    \]
    for all $1 \leq i \leq k$ and $d_j(i)$ are distinct.
\end{theorem}
\begin{proof}
    We may induct on $k$ or prove the case $k = 2$. For $k = 2$, as $A_1, A_2$ are Hermitian, we have $A_1 A_2 = A_2 A_1$. Then we have $A_1$ acts on $\mathbb{C}^n = E_{\lambda_1} (A_1) \oplus E_{\lambda_2} (A_1) \oplus \cdots \oplus E_{\lambda_k} (A_1)$ where $\lambda_1, \lambda_2, \cdots, \lambda_k$ are the distinct eigenvalues of $A_1$. Then we also consider $A_2$ acts on $\mathbb{C}^n$. We have the following claim: The action of $A_2$ on $\mathbb{C}^n$ leaves each eigenspace of $A_1$ invariant. For any $\vec{z} \in E_{\lambda_i} (A_1)$, we have:
    \[
        A_1 (A_2 \vec{z}) = A_2 (A_1 \vec{z}) = A_2 (\lambda_i \vec{z}) = \lambda_i (A_2 \vec{z})
    \]
    Hence, $A_2 \vec{z} \in E_{\lambda_i} (A_1)$. Then, we have $A_2 = A_2^1 \oplus A_2^2 \oplus \cdots \oplus A_2^k$. We claim that each $A_2^i$ is Hermitian on $E_{\lambda_i} (A_1)$. For any $\vec{x}, \vec{y} \in E_{\lambda_i} (A_1)$, we have:
    \[
        \langle \vec{x}, A_2^i \vec{y} \rangle = \langle \vec{x}, A_2 \vec{y} \rangle = \langle A_2 \vec{x}, \vec{y} \rangle = \langle A_2^i \vec{x}, \vec{y} \rangle
    \]
    So, $A_2^i$ is diagonalisable on $E_{\lambda_i} (A_1)$ with an orthonormal eigenbasis and distinct eigenvalues $\mu_j$. Therefore, we have:
    \[
        E_{\lambda_i} (A_1) = \bigoplus_j E_{\lambda_i, \mu_j} (A_1, A_2).
    \]
    Then we have:
    \[
        \mathbb{C}^n = \bigoplus_{i, j} E_{\lambda_i, \mu_j} (A_1, A_2).
    \]
    We may also write $\lambda_i, \mu_j$ as a vector in $\R^2$, i.e., $\vec{\lambda_{i, j}} = (\lambda_i, \mu_j)$.
\end{proof}

We can use the simultaneous diagonalisation theorem to prove the spectral theorem for normal operators.
\begin{theorem}
    A complex square matrix can be diagonalised by a unitary matrix if and only if it is normal.
\end{theorem}
\begin{proof}
    $(\Rightarrow)$ Assume that $A$ can be diagonalised by a unitary matrix, i.e., there is a unitary matrix $U$ such that $A = U D U^\dagger$ where $D$ is a diagonal matrix. Then we have:
    \[
        A^\dagger = U D^\dagger U^\dagger
    \]
    where $D^\dagger$ is also a diagonal matrix. Then we have:
    \[
        A A^\dagger = U D U^\dagger U D^\dagger U^\dagger = U D D^\dagger U^\dagger = U D^\dagger D U^\dagger = A^\dagger A
    \]
    $D D^\dagger = D^\dagger D$ as we have the following equality:
    \[
        \begin{bmatrix}
            d_1 & & \\
            & \ddots & \\
            & & d_n
        \end{bmatrix} \begin{bmatrix}
            \overline{d_1} & & \\
            & \ddots & \\
            & & \overline{d_n}
        \end{bmatrix} = \begin{bmatrix}
            |d_1|^2 & & \\
            & \ddots & \\
            & & |d_n|^2
        \end{bmatrix}.
    \]
    Therefore, $A$ is normal.

    $(\Leftarrow)$ Assume that $A$ is normal, i.e., $A A^\dagger = A^\dagger A$. Then we write $A = B + iC$ where $B = \frac{A + A^\dagger}{2}$ and $C = \frac{A - A^\dagger}{2i}$. Then we claim that $[B, C] = 0$ if and only if $A$ is normal. We have:
    \[
        \begin{split}
            A A^\dagger &= (B + iC)(B - iC) = B^2 + C^2 - i [B, C] \\
            A^\dagger A &= (B - iC)(B + iC) = B^2 + C^2 + i [B, C]
        \end{split}
    \]
    Therefore, $A A^\dagger = A^\dagger A$ if and only if $[B, C] = 0$. Also, we may check that $B$ and $C$ are Hermitian:
    \[
        B^\dagger = \left(\frac{A + A^\dagger}{2}\right)^\dagger = \frac{A^\dagger + A}{2} = B, \quad C^\dagger = \left(\frac{A - A^\dagger}{2i}\right)^\dagger = \frac{A^\dagger - A}{-2i} = C.
    \]
    Then, by the simultaneous diagonalisation theorem, there is a unitary matrix $U$ such that:
    \[
        B = U D_B U^\dagger, \quad C = U D_C U^\dagger
    \]
    where $D_B$ and $D_C$ are diagonal matrices. Therefore, we have:
    \[
        A = B + iC = U D_B U^\dagger + i U D_C U^\dagger = U (D_B + i D_C) U^\dagger = U D_A U^\dagger
    \]
    where $D_A = D_B + i D_C$ is also a diagonal matrix. Hence, $A$ can be diagonalised by a unitary matrix.
\end{proof}

\newpage

The following chapters are not in the exam syllabus, but for your reference.

\section{Affine Spaces}

A line or a plane can be regarded as an affine space. An affine space differs from a vector space in that it does not have a distinguished origin. We may say that $\mathcal{T}_O \A$ is the tangent space of an affine space $\A$ at a point $O \in \A$. We also have symmetric spaces, which can be a sphere.

Let $\F$ be a field. An affine space of dimension $n$ over $\F$, $\A$, is a principal $(\F^n, +)$-set.  A $G$-set, the set on which $G$ acts, is called principal $G$-set if the action is principal, i.e., transitive and free.

\begin{example}
    $\F^n$ is an affine space of dimension $n$ over $\F$ with the usual addition action of $(\F^n, +)$ on itself.
    \[
        \begin{split}
            (\F^n, +) \times \F^n &\to \F^n \\
            (\vec{v}, \vec{x}) &\mapsto \vec{v} + \vec{x}
        \end{split}
    \]
    For any $\vec{x}, \vec{y} \in \F^n$, there is a unique $\vec{v} = \vec{y} - \vec{x} \in \F^n$ such that $\vec{v} + \vec{x} = \vec{y}$. Therefore, the action is transitive and free.
\end{example}

In fact, any $\F$-linear space is a $\F$-affine space. 

\begin{problem}
    Any set with 2 elements is an affine space over $\mathbb{Z}_2$ in the unique way. However, for 3 elements, there does not have a unique affine space structure over $\mathbb{Z}_3$. 
\end{problem}

The model one of the $\A$ is $\A_\F^n := \{ (x_1, \cdots, x_n) \mid x_i \in \F \}$. Then the group action is:
\[
    \begin{split}
        (\F^n, +) \times \A_\F^n &\to \A_\F^n \\
        (\vec{v}, \vec{x}) &\mapsto \vec{v} + \vec{x} := (v_1 + x_1, \cdots, v_n + x_n)
    \end{split}
\]
for all $\vec{v} = (v_1, \cdots, v_n) \in \F^n$ and $\vec{x} = (x_1, \cdots, x_n) \in \A_\F^n$. Moreover, up to isomorphism, there is only one affine space of dimension $n$ over $\F$.

Similarly, we have the following conversion table:
\begin{center}
    \begin{tabularx}{\textwidth}{C C}
        \toprule
        \textbf{Vector Space} & \textbf{Affine Space} \\
        \midrule
        Linear Combinations & Affine Combinations \\
        Basis & Affine Frame \\
        Span & Affine Span/Hull \\
        Subspace & Affine Subspace \\
        Linear Map & Affine Map \\
        Linear Independence & Affine Independence \\
        Vectors & Points \\
        \bottomrule
    \end{tabularx}
\end{center}

For the affine combinations, we have:
\[
    p_0, p_1, \cdots, p_k \in \A, \quad c^0, c^1, \cdots, c^k \in \F
\]
with $\sum_i c_i = 1$, then the affine combination is defined as:
\[
    \sum_i c^i p_i := O + \sum_i c^i (p_i - O)
\]
for some $O \in \A$ and $c^i (p_i - O)$ is the linear combination in the vector space $\mathcal{T}_O \A$. Note that we may have different $O$, let say $O'$. We may check the independence of the choice of $O$: We know that $O' = O + (O' - O)$, then we have:
\[
    \begin{split}
        c^i p_i &= O' + \sum_i c^i (p_i - O') \\
        &= O + (O' - O) + \sum_i c^i (p_i - O') \\
        &= O + \sum_i c^i (O' - O) + \sum_i c^i (p_i - O') \\
        &= O + \sum_i c^i ((O' - O) + (p_i - O')) \\
        &= O + \sum_i c^i (p_i - O) \\
        &= c^i p_i
    \end{split}
\]

For affine subspaces and spans, we consider the following diagram:
\begin{center}
    \tdplotsetmaincoords{70}{30}
    \begin{tikzpicture}[tdplot_main_coords]
        \fill[ocre!30,opacity=0.5] (3,-3,1) -- (-3,-3,1) -- (-3,3,1) -- (3,3,1) -- cycle;
        \draw[red] (-3,-3,1) -- (3,3,1);
        \fill (-1,-1,1) circle (1pt) node[anchor=north] {$p_0$};
        \fill (1,1,1) circle (1pt) node[anchor=north] {$p_1$};
        \fill (-2,0,1) circle (1pt) node[anchor=north] {$p_2$};
    \end{tikzpicture}
\end{center}
The red line is the smallest affine subspace containing $p_0$ and $p_12$, i.e., the affine span of $p_0$ and $p_1$. We may write $\Span\{ p_0, p_1 \} := \{ c^0 p_0 + c^1 p_1 \mid c^0 + c^1 = 1, c^i \in \R \} = \{ tp_0 + (1 - t) p_1 \mid t \in \R \}$. Note that $\overline{p_0 p_1} = \{ tp_0 + (1 - t) p_1 \mid t \in [0, 1] \}$ is a subset of the affine span.

For the affine frame, we may consider the same picture above. Then $\{ p_0, p_1, p_2 \}$ is an affine frame of the affine space (the plane) as no point is in the affine span of the other two points.

For the representation of the affine map, we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        \A_1 \arrow[d] \arrow[r, "\phi"] & \A_2 \arrow[d] \\
        \A_\F^n \arrow[d] \arrow[r] & \A_\F^m \arrow[d] \\
        \F^n \arrow[r, "A"] & \F^m \\[-3.6em]
        \vec{x} = x - 0 \arrow[r, mapsto] & A\vec{x} + \vec{b}
    \end{tikzcd}
\end{center}
where $A \in \M{m \times n}{\F}$ and $\vec{b} \in \F^m$. Note that the representation of $\phi$ depends on the choice of origins in $\A_1$ and $\A_2$. 

A Euclidean space is a finite dimensional real affine space with a Euclidean structure on its tangent space. The Euclidean structure means the translation invariant assignment of inner product to each tangent space of $\A$. Let $\A$ be an $n$-dimensional real affine space. Take $p \in \A$. Then the pointed affine space $(\A, p)$ is isomorphic to the vector space $\mathcal{T}_p \A$. Moreover, it is equivalent to $\R^n$ with the standard inner product, and $q \in (\A, p)$ corresponds to the vector $\vec{v} = q - p \in \R^n$. Then we have:
\[
    \alpha_1 q_1 + \alpha_2 q_2 = p + \alpha_1 (q_1 - p) + \alpha_2 (q_2 - p)
\]
Note that $\alpha_1 + \alpha_2$ need not be 1 here, as it is linear combination. Then the translation invariant means that the length and angle remains unchanged in the inner product after translation, i.e., $\langle \vec{pq}, \vec{pr} \rangle = \langle \vec{p'q'}, \vec{p'r'} \rangle$. 
\begin{center}
    \tdplotsetmaincoords{70}{30}
    \begin{tikzpicture}[tdplot_main_coords]
        \fill[ocre!30,opacity=0.5] (3,-3,1) -- (-3,-3,1) -- (-3,3,1) -- (3,3,1) -- cycle;
        \fill (-1.5,-1.5,1) coordinate (P) circle (1pt) node[left] {$p$};
        \draw[->] (P) -- ($(P) + (0,1.5,0)$) coordinate (Q) node[above] {$q$};
        \draw[->] (P) -- ($(P) + (1.5,0,0)$) coordinate (R) node[below] {$r$};
        \fill (0.5,0.5,1) coordinate(P') circle (1pt) node[left] {$p'$};
        \draw[->] (P') -- ($(P') + (0,1.5,0)$) coordinate (Q') node[above] {$q'$};
        \draw[->] (P') -- ($(P') + (1.5,0,0)$) coordinate (R') node[below] {$r'$};
        \draw[dashed, ->, red] (Q) -- (Q') node[midway, above] {$\vec{v}$};
        \draw[dashed, ->, red] (R) -- (R') node[midway, below] {$\vec{v}$};
    \end{tikzpicture}
\end{center}
Then $q = q' + \vec{v}$ and $r = r' + \vec{v}$. Note that $\mathcal{T}_p \A$ is different from $\mathcal{T}_{p'} \A$ as they are tangent spaces at different points, but they are isomorphic via translation by $\vec{v}$. We may consider the tangent line on the circle at different points as an example.

Up to isomorphism, there is only one Euclidean space of dimension $n$, denoted by $\mathbb{E}^n := (\A_\R^n, \langle \cdot, \cdot \rangle)$ where $\langle \cdot, \cdot \rangle$ is:
\[
    \langle \vec{pq}, \vec{pr} \rangle = (q - p) \cdot (r - p)
\]
where the $\cdot$ is the standard dot product on $\R^n$. This is equivalent to say that an orthogonal frame exists, i.e., the rectangular coordinate system.

For an affine map $\phi : \A_1 \to \A_2$ between two affine spaces, we say that $\phi$ is injective implies that $\dim \A_1 \leq \dim \A_2$. The proof is by picking a point $p_1 \in \A_1$ and take $p_2 = \phi(p_1)$. Then we have the following commutative diagram:
\begin{center}
    \begin{tikzcd}
        (\A_1, p_1) \arrow[r, "\mathcal{T}_{p_1} \phi"] \arrow[d, "\cong"'] & (\A_2, p_2) \arrow[d, "\cong"] \\
        \A_1 \arrow[r, "\phi"] & \A_2
    \end{tikzcd}
\end{center}

We have two space-time affine space in Physics, namely Minkowski and Galilean. 

The Minkowski space-time $\mathbb{M}$ is a 4-dimensional real affine space $\A_\R^4$ with a Lorentz structure. Take a point $p \in \A_\R^4$ and $u = (u_0, \vec{u}), v = (v_0, \vec{v}) \in \R^4$. Then the Lorentzian inner product is $\langle u, v \rangle_p = u_0 v_0 - \vec{u} \cdot \vec{v}$. 

The Galilean space-time $\mathbb{G}$ is a 4-dimensional real affine space $\A_\R^4$ with a Galilean structure. It is the Minkowski space-time taking the limit of light speed $c \to \infty$. We have the following diagram:
\begin{center}
    \begin{tikzpicture}
        \filldraw[gray!10] (-4,-2) rectangle (4,2);
        \draw[step=0.5cm,gray!20] (-4,-2) grid (4,2);

        \draw[thick] (-2.5,-2) -- (-2.5,2) node [above] {\scriptsize $\pi^{-1} (t_1)$};
        \draw[thick] (-1,-2)  -- (-1,2) node [above] {\scriptsize $\pi^{-1} (t_2)$};
        \draw[thick] (0.5,-2) -- (0.5,2) node [above] {\scriptsize $\pi^{-1} (t_3)$};
        \draw[thick] (2,-2)  -- (2,2) node [above] {\scriptsize $\pi^{-1} (t_4)$};

        \path (3.5, 2) node [above] {\scriptsize $\cdots$};

        \draw[decoration={brace,raise=5pt},decorate]
            (-4,-2) -- node [left=6pt] {$\displaystyle \A_\R^4$} (-4,2);
        \draw[-Stealth] (-4 cm - 18 pt,-15 pt) -- (-4 cm - 18 pt, -4 cm + 15 pt) node [midway, left] {$\pi$};

        \draw[thick] (-4,-4) node [left=6pt] {Time: $E^1$} -- (4,-4);

        \draw[dashed] (-2.5,-2) -- (-2.5,-4);
        \draw[dashed] (-1,-2) -- (-1,-4);
        \draw[dashed] (0.5,-2) -- (0.5,-4);
        \draw[dashed] (2,-2) -- (2,-4);

        \draw[red] (-2.75,1) node[left] {Event} -- (-2.25,1);
        \draw[red, -Stealth] (-2.5, -1) -- (-1, 1);
        \draw[red, -Stealth] (-1, -1) -- (0.5, 1);
        \draw[red, -Stealth] (0.5, -1) -- (2, 1);

        \filldraw (-2.5,-4) circle (1.5pt) node [below] {$t_1$};
        \filldraw (-1,-4) circle (1.5pt) node [below] {$t_2$};
        \filldraw (0.5,-4) circle (1.5pt) node [below] {$t_3$};
        \filldraw (2,-4) circle (1.5pt) node [below] {$t_4$};
    \end{tikzpicture}
\end{center}

\newpage

\section{Quadratic Form and Clifford Algebra}

Let $V$ be a vector space over a field $\F$. A quadratic form on $V$ is a map $Q : V \to \F$ such that:
\begin{itemize}
    \item $Q(\alpha v) = \alpha^2 Q(v)$ for all $\alpha \in \F$ and $v \in V$;
    \item The map $B : V \times V \to \F$ defined by $B(u, v) = Q(u + v) - Q(u) - Q(v)$ is bilinear.
\end{itemize}
In case $\chart \F \neq 2$, the set of all quadratic forms on $V$ is equivalent to the set of all symmetric 2-forms on $V$. A quadratic form $Q$ can define a symmetric 2-form as $B(u, v) = \frac{1}{2} (Q(u + v) - Q(u) - Q(v))$; a symmetric 2-form $B$ can define a quadratic form $Q(u) := B(u, u)$. We have the matrix representation of symmetric 2-form with respects to a basis. So we can also have the matrix representation of quadratic form, which is the symmetric matrices over $\F$ of order $\dim V = n$. Moreover, $(V, Q)$ forms a quadratic space. 

A Clifford algebra $\Cl(V, Q) := \T V / I_Q$ is an associative algebra over $\F$ generated by $v \otimes v - Q(v) 1$ for all $v \in V$. The ideal is equivalent to the ideal generated by $u \otimes v + v \otimes u - 2 B(u, v) 1$ for all $u, v \in V$. Note that $\Cl (V, Q)$ is $\quotient{\Z}{2}$ graded algebra.

We have the following isomorphisms:
\begin{itemize}
    \item $\Cl (\R^{0, 1}) \cong \mathbb{C}$ as $\R$-algebras, where elements in $\Cl(\R^{0, 1})$ are of the form $a + b e_1$ with $e_1^2 = -1$;
    \item $\Cl (\R^{1, 0}) \cong \R \oplus \R$, the split-complex number, where elements in $\Cl(\R^{1, 0})$ are of the form $a + b e_1$ with $e_1^2 = 1$;
    \item $\Cl (\R^{0, 2}) \cong \mathbb{H}$, the quaternion, as $\R$-algebras, where elements in $\Cl(\R^{0, 2})$ are of the form $a + b e_1 + c e_2 + d e_1 e_2$ with $e_1^2 = e_2^2 = -1$ and $e_1 e_2 = - e_2 e_1$;
    \item $\Cl (\R^{1, 1}) \cong \M{2 \times 2}{\R}$, the split-quaternion, as $\R$-algebras, where elements in $\Cl(\R^{1, 1})$ are of the form $a + b e_1 + c e_2 + d e_1 e_2$ with $e_1^2 = 1$, $e_2^2 = -1$ and $e_1 e_2 = - e_2 e_1$;
    \item $\Cl (\R^{2, 0}) \cong \M{2 \times 2}{\R}$, the split-quaternion, as $\R$-algebras.
\end{itemize}

The $\R$, $\mathbb{C}$ and $\mathbb{H}$ are called the associative real division algebras.

\newpage

This is the end of the main content. Thank you for your support! I hope you have enjoyed my notes!! By the way, I would like to remake this notes later after the final. If you are interested, find me through Discord \texttt{@stupidbenz}, or Instagram \texttt{@stupid.benz.0621}.






%----------------------------------------------------------------------------------------
%	APPENDICES
%----------------------------------------------------------------------------------------

\begin{appendices}

\renewcommand{\chaptername}{Appendix} % Change the chapter name to Appendix, i.e. "Appendix A: Title", instead of "Chapter A: Title" in the headers

%------------------------------------------------

\chapter{Universal Properties}

We first state the formal definition of universal properties.

\begin{definition}[Universal Properties]
    Let $F : \C \to \D$ be a functor between two categories $\C$ and $\D$. A universal morphism from an object $X \in \Ob{\mathbb{D}}$ to the functor $F$ is a unique pair $(A, u : X \to F(A))$ in $\D$ such that for any morphism $f : X \to F(A')$ in $\D$, there exists a unique morphism $\bar{f} : A \to A'$ in $\C$ such that the following diagram commutes:
    \begin{center}
        \begin{tikzcd}
            X \arrow[r, "u", red] \arrow[dr, "f" swap] & F({\color{ocre} A}) \arrow[d, "F(\bar{f})", dashed] & {\color{ocre} A} \arrow[d, "\bar{f}", dashed] \\
            & F(A') & A'
        \end{tikzcd}
    \end{center}
    Such a property is called the \emph{universal property} of the object $A$. Note that the dual version of universal morphism from $F$ to $X$ can be defined similarly.
\end{definition}
\begin{remark}
    Such an object $A$ is an initial object in a new category:
    \begin{itemize}
        \item Objects: all pairs $(B, f : X \to F(B))$ for all $B \in \Ob{\C}$;
        \item Morphisms: commutative diagrams in $\C$:
    \end{itemize}
    \begin{center}
        \begin{tikzcd}[column sep=normal]
            & X \arrow[dl, "f" swap] \arrow[dr, "f'"] & \\
            F(B) \arrow[rr, "F(h)" swap] & & F(B')
        \end{tikzcd}
    \end{center}
    The initial object in this category is exactly the object $A$ with the universal property. This type of construction is called the \emph{comma category} and is denoted by $(X \downarrow F)$.

    Similarly, the dual version of terminal object can be defined for the dual version of universal morphism, and the category is denoted by $(F \downarrow X)$.
\end{remark}

\section{Universal Properties of Limits}

The following are the universal properties of some common limits in category theory.

\begin{itemize}
    \item \textbf{Products:}
    \begin{center}
        \begin{tikzcd}
            X_{\alpha} & \prod X_{\alpha} \arrow[l, "\pi_{\alpha}" swap, two heads] \\
            & Z \arrow[u, "u" description, dashed] \arrow[ul, "f"]
        \end{tikzcd}
    \end{center}

    \item \textbf{Kernel:}
    \begin{center}
        \begin{tikzcd}
            & X \arrow[dr, "f"] & \\
            & \ker{f} \arrow[u, "\iota" swap, hook] \arrow[r, "0"] & Y \\
            Z \arrow[urr, "0" swap] \arrow[ur, "u" description, dashed] \arrow[uur, "\iota'", hook]
        \end{tikzcd}
    \end{center}

    \item \textbf{Subspaces:}
    \begin{center}
        \begin{tikzcd}
            & V \arrow[dr, "\pi", two heads] & \\
            & U \arrow[u, "\iota" swap, hook] \arrow[r, "0"] & \quotient{V}{U} \\
            Z \arrow[urr, "0" swap] \arrow[ur, "u" description, dashed] \arrow[uur, "\iota'", hook]
        \end{tikzcd}
    \end{center}
\end{itemize}

\newpage

\section{Universal Properties of Colimits}

The following are the universal properties of some common colimits in category theory.
\begin{itemize}
    \item \textbf{Coproducts:}
    \begin{center}
        \begin{tikzcd}
            X_{\alpha} \arrow[r, "\iota_{\alpha}", hook] \arrow[dr, "f"] & \coprod X_{\alpha} \arrow[d, "u" description, dashed] \\
            & Z
        \end{tikzcd}
    \end{center}

    \item \textbf{Cokernel:}
    \begin{center}
        \begin{tikzcd}
            & Y \arrow[d, "\pi" swap, two heads] \arrow[ddr, "\pi'", two heads] \\
            X \arrow[ur, "f"] \arrow[r, "0"] \arrow[drr, "0" swap] & \coker{f} \arrow[dr, "u" description, dashed] \\
            & & Z
        \end{tikzcd}
    \end{center}

    \item \textbf{Quotient Spaces:}
    \begin{center}
        \begin{tikzcd}
            & V \arrow[d, "\pi" swap, two heads] \arrow[ddr, "\pi'", two heads] \\
            U \arrow[ur, "f"] \arrow[r, "0"] \arrow[drr, "0" swap] & \quotient{V}{U} \arrow[dr, "u" description, dashed] \\
            & & Z
        \end{tikzcd}
    \end{center}

    \item \textbf{Free Vector Spaces:}
    \begin{center}
        \begin{tikzcd}
            X \arrow[r, "\iota", hook] \arrow[dr, "f"] & \F[X] \arrow[d, "{\F[u]}" description, dashed] & X \arrow[d, "u" description, dashed] \\
            & Z & {|Z|}
        \end{tikzcd}
    \end{center}

    \item \textbf{Tensor Products:}
    \begin{center}
        \begin{tikzcd}
            U \times V \arrow[r, "\iota"] \arrow[dr, "\phi"] & U \otimes V \arrow[d, "u" description, dashed] \\
            & Z
        \end{tikzcd}
    \end{center}
\end{itemize}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\chapter*{Bibliography}
\markboth{\sffamily\normalsize\bfseries Bibliography}{\sffamily\normalsize\bfseries Bibliography} % Set the page headers to display a Bibliography chapter name
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Bibliography}} % Add a Bibliography heading to the table of contents

% \section*{Articles}
% \addcontentsline{toc}{section}{Articles} % Add the Articles subheading to the table of contents

% \printbibliography[heading=bibempty, type=article] % Output article bibliography entries

% \section*{Books}
% \addcontentsline{toc}{section}{Books} % Add the Books subheading to the table of contents

% \printbibliography[heading=bibempty, type=book] % Output book bibliography entries

\section*{Websites}
\addcontentsline{toc}{section}{Websites} % Add the Websites subheading to the table of contents
\printbibliography[heading=bibempty, type=online] % Output online bibliography entries

%------------------------------------------------

\end{appendices}


\end{document}
