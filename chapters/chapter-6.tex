%----------------------------------------------------------------------------------------
%	CHAPTER 6
%----------------------------------------------------------------------------------------

\chapter{Euclidean Vector Spaces}

Before studying Euclidean vector spaces, we first review tensors and then introduce inner products.

\section{Tensor}

Let $V$ be a finite dimensional vector space over a field $\F$. Then we have the following definitions.

\begin{definition}[$k$-form]
    A $k$-\emph{form}\index{form} on $V$ is a multilinear map:
    \[
        \underbrace{V \times V \times \cdots \times V}_{k \text{ times}} \to \F
    \]
    which is linear in each argument. It is an element in $(V^*)^{\otimes k}$.
\end{definition}

More concretely, for 1-form, it is a linear functional on $V$, i.e. an element in $V^*$. It is also called \emph{covector}\index{covector}. For 2-form, it is a bilinear map on $V$, i.e. an element in $V^* \otimes V^*$. To prove that the set of all 2-forms on $V$ is isomorphic to $V^* \otimes V^*$, we can consider the following diagram:
\begin{center}
    \begin{tikzcd}
        \Map^{\mathsf{ML}} (V \times V, \F) \arrow[r, "\equiv" description, phantom] \arrow[d, dashed] & \Hom(V, V^*) \arrow[d, "\equiv" description, phantom] \\
        V^* \otimes V^* & \Hom(V, \F) \otimes V^* \arrow[l, "\vequiv" description, phantom]
    \end{tikzcd}
\end{center}

Moreover, we have the following two special types of 2-forms.

\begin{definition}[Symmetric and Skew-symmetric 2-forms]
    A 2-form $\omega : V \times V \to \F$ is called \emph{symmetric}\index{symmetric} if
    \[
        \omega (u, v) = \omega (v, u)
    \]
    for all $u, v \in V$. It is an element in $\mathcal{S}^2{V^*}$. The 2-form $\omega$ is called \emph{skew-symmetric}, or antisymmetric, if 
    \[
        \omega (u, v) = - \omega (v, u)
    \]
    for all $u, v \in V$. It is an element in ${\bigwedge}^2{V^*}$.
\end{definition}

Then we define the tensor spaces.

\begin{definition}[Tensor Spaces]
    Let $V$ be a finite dimensional vector space over a field $\F$. The \emph{tensor space of type} $(r, s)$ on $V$ is defined as:
    \[
        \mathcal{T}^{r, s} V = \underbrace{V \otimes V \otimes \cdots \otimes V}_{r \text{ times}} \otimes \underbrace{V^* \otimes V^* \otimes \cdots \otimes V^*}_{s \text{ times}}
    \]
    Elements in $\mathcal{T}^{r, s} V$ are called \emph{tensors of type} $(r, s)$ on $V$, which is a mixed type if $r, s \neq 0$.
\end{definition}

If a tensor of type $(r, 0)$, then it is called a \emph{contravariant tensor} or simply a \emph{tensor}\index{tensor}. If a tensor of type $(0, s)$, then it is called a \emph{covariant tensor} or simply a \emph{form}\index{form}. For $\mathcal{T}^{0, 0} V$, it is defined as $\F$ itself. Any elements in $\mathcal{T}^{0, 0} V$ are \emph{scalar type} tensor on $V$, or simply \emph{scalars}\index{scalars}.

Then we know that $\End{V} \equiv V \otimes V^* \equiv \mathcal{T}^{1, 1} V$. Therefore, any endomorphism on $V$ can be viewed as a tensor of type $(1, 1)$ on $V$, represented by $a^i_j$ with respect to a basis $\B_V = \{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n \}$ of $V$. Here the upper index $i$ represents the contravariant part and the lower index $j$ represents the covariant part. To know that what $a^i_j$ means, we can consider the following diagram:
\begin{center}
    \begin{tikzcd}
        V \arrow[r, "T"] \arrow[d, "{[-]_{\B_V}}" swap] & V \arrow[d, "{[-]_{\B_V}}"] \\
        \F^n \arrow[r, "{A = [a^i_j]_{\B_V}}" swap] & \F^n
    \end{tikzcd}
\end{center}
Then how to get the matrix representation $A = [a^i_j]_{\B_V}$ of $T$ with respect to the basis $\B_V$? We have:
\[
    \vec{a}_j = A \vec{e}_j, \qquad a^i_j = \vec{e}_i^T A \vec{e}_j = \hat{e}^i A \vec{e}_j = \langle \hat{e}^i, A \vec{e}_j \rangle.
\]
So we have $[a^i_j] = \langle \hat{v}^i, T \vec{v}_j \rangle$. We can have an identification between $\End{V}$ and $\mathcal{T}^{1, 1} V$ as follows:
\[
    T \leftrightarrow T\vec{v}_j \otimes \hat{v}^j
\]

For covariant and contravariant, we have the following table:

\begin{center}
\begin{tabularx}{\textwidth}{X X}
    \toprule
    \textbf{Object} & \textbf{Transformation Type} \\
    \midrule
    Standard Basis Vector ($\vec{e}_i$) & Covariant \\
    \midrule
    Dual Basis Vector ($\hat{e}^i$) & Contravariant \\
    \midrule
    Component of a Vector ($v^i$) & Contravariant \\
    \midrule
    Component Basis Vector ($v_i$) & Covariant \\
    \bottomrule
\end{tabularx}
\end{center}

An object is considered as covariant if it transform in the same way as the basis vectors of the original vector space. If you cannot understand it, make up some examples of scaling the vector spaces.

In general, an element $t \in \mathcal{T}^{r, s} V$ can be represented as:
\[
    t^{i_1 i_2 \cdots i_r}_{j_1 j_2 \cdots j_s} \vec{v}_{i_1} \otimes \vec{v}_{i_2} \otimes \cdots \otimes \vec{v}_{i_r} \otimes \hat{v}^{j_1} \otimes \hat{v}^{j_2} \otimes \cdots \otimes \hat{v}^{j_s}
\]
Note that the representation depends on the choice of basis $\B_V$ of $V$, i.e., the following two represents the same tensor with respect to different bases:
\[
    \left[t^{i_1 i_2 \cdots i_r}_{j_1 j_2 \cdots j_s}\right]_{\B_V} \sim \left[\tilde{t}^{\tilde{i}_1 \tilde{i}_2 \cdots \tilde{i}_r}_{\tilde{j}_1 \tilde{j}_2 \cdots \tilde{j}_s}\right]_{\widetilde{\B_V}}
\]
The two representations are related by the base change matrices::
\[
    (\tilde{v}_1, \tilde{v}_2, \cdots, \tilde{v}_n) = (v_1, v_2, \cdots, v_n) A, \quad A = [a^i_{\tilde{j}}]_{\B_V}^{\widetilde{\B_V}} \in \GL(V)
\]
\begin{remark}
    It is actually the right action of $\GL(V)$ on the set of all bases of $V$, $\B_V$:
    \[
        \B_V \times \GL(V) \to \B_V, \quad (v, A) \mapsto v A = \tilde{v}
    \]
\end{remark}
Then we have the following equation:
\[
    \tilde{v}_{\tilde{j}} = v_i a^i_{\tilde{j}}
\]
For $A^{-1} = [b_j^{\tilde{i}}]_{\widetilde{\B_V}}^{\B_V}$, we have $a^i_{\tilde{j}} b_j^{\tilde{k}} = \delta^{\tilde{k}}_{\tilde{j}}$ and $b_j^{\tilde{i}} a^j_{\tilde{k}} = \delta^{\tilde{i}}_{\tilde{k}}$. Therefore, we have:
\[
    v_k = \tilde{v}_{\tilde{j}} b_k^{\tilde{j}}
\]
\begin{remark}
    For easier memorising, we use the calculus operators:
    \[
        \frac{\partial \tilde{v}_{\tilde{j}}}{\partial v_i} = a^i_{\tilde{j}}, \quad \frac{\partial v_k}{\partial \tilde{v}_{\tilde{j}}} = b_k^{\tilde{j}}
    \]
    To memorise it, we consider the lower indices in denominators (lower) will flip to the upper indices in numerators. (As lower twice, so flip to upper)

    Then we can use the chain rule to verify the two equations of $A$ and $A^{-1}$:
    \[
        \frac{\partial \tilde{v}_{\tilde{j}}}{\partial v_i} \frac{\partial v_k}{\partial \tilde{v}_{\tilde{j}}} = \delta^i_k
    \]
\end{remark}

Then we have the transformation rule for the representation of $t \in \mathcal{T}^{r, s} V$ under the base change from $\B_V$ to $\widetilde{\B_V}$:
\[
    \tilde{t}^{{\color{ocre} \tilde{i}_1 \tilde{i}_2 \cdots \tilde{i}_r}}_{{\color{red} \tilde{j}_1 \tilde{j}_2 \cdots \tilde{j}_s}} = \left({\color{ocre} b_{i_1}^{\tilde{i}_1} b_{i_2}^{\tilde{i}_2} \cdots b_{i_r}^{\tilde{i}_r}}\right) t^{{\color{ocre} i_1 i_2 \cdots i_r}}_{{\color{red} j_1 j_2 \cdots j_s}} \left({\color{red} a^{j_1}_{\tilde{j}_1} a^{j_2}_{\tilde{j}_2} \cdots a^{j_s}_{\tilde{j}_s}}\right)
\]

Given that $\B_V = \{ \vec{v}_1, \cdots, \vec{v}_n \}$ is a basis of $V$, then we can define a basis of $\mathcal{T}^{r, s} V$ as follows:
\[
    \B_{\mathcal{T}^{r, s} V} = \{ \vec{v}_{i_1} \otimes \vec{v}_{i_2} \otimes \cdots \otimes \vec{v}_{i_r} \otimes \hat{v}^{j_1} \otimes \hat{v}^{j_2} \otimes \cdots \otimes \hat{v}^{j_s} : 1 \leq i_1, i_2, \cdots, i_r, j_1, j_2, \cdots, j_s \leq n \}
\]
Then for symmetric and skew-symmetric $k$-forms, we have:
\[
    \begin{split}
        \B_{\mathcal{S}^k V} &= \{ \vec{v}_{i_1} \vec{v}_{i_2} \cdots \vec{v}_{i_k} : 1 \leq i_1, i_2, \cdots, i_k \leq n \} \\
        \B_{{\bigwedge}^k V} &= \{ \vec{v}_{i_1} \wedge \vec{v}_{i_2} \wedge \cdots \wedge \vec{v}_{i_k} : 1 \leq i_1, i_2, \cdots, i_k \leq n \}
    \end{split}
\]
Then ``honest'' definition of symmetric basis is:
\[
    \{ \vec{v}_{i_1} \vec{v}_{i_2} \cdots \vec{v}_{i_k} : 1 \leq i_1 \leq i_2 \leq \cdots \leq i_k \leq n \}
\]
but it is redundant. We just have to make sure that the representation of any symmetric $k$-form is unique for a given basis. For example, in 2-form case with the basis $\{ \vec{e}_i \otimes \vec{e}_j \}$, we originally have to write:
\[
    t = \sum_{1 \leq i \leq j \leq n} t_{ij} \vec{e}_i \otimes \vec{e}_j
\]
but this is ugly, so we just write:
\[
    t = t^{ij} \vec{e}_i \vec{e}_j
\]
with $t^{ij} = t^{ji}$. If we ignored the condition on $t^{ij}$, then we have $a^{ij} = - a^{ji}$ such that:
\[
    t = t^{ij} \vec{e}_i \wedge \vec{e}_j + a^{ij} \vec{e}_i \wedge \vec{e}_j = (t^{ij} + a^{ij}) \vec{e}_i \wedge \vec{e}_j = 0
\]
As $a^{ij} = a^{ji} = - a^{ij}$. 

Then for skew-symmetric basis, let say $t \in \B_{{\bigwedge}^k V}$, then we have:
\[
    t = t^{\mathcal{I}} \vec{v}_{\mathcal{I}} = t^{i_1 i_2 \cdots i_k} \vec{v}_{i_1} \wedge \vec{v}_{i_2} \wedge \cdots \wedge \vec{v}_{i_k}
\]
with $\mathcal{I} = (i_1, i_2, \cdots, i_k)$ being an ordered index set with $1 \leq i_1 < i_2 < \cdots < i_k \leq n$. Then for any permutation $\sigma \in S_k$, to make sure it is unique, we require:
\[
    t^{\sigma(\mathcal{I})} = \sgn(\sigma) t^{\mathcal{I}}
\]
where $\sigma(\mathcal{I}) = (i_{\sigma(1)}, i_{\sigma(2)}, \cdots, i_{\sigma(k)})$.

In conclusion, we have to make sure that the representation of any symmetric or skew-symmetric $k$-form is unique for a given basis by the following conditions respectively:
\[
    \begin{split}
        &\text{Symmetric:} \quad t^{i_1 i_2 \cdots i_k} = t^{i_{\sigma(1)} i_{\sigma(2)} \cdots i_{\sigma(k)}} \\
        &\text{Skew-symmetric:} \quad t^{i_1 i_2 \cdots i_k} = \sgn(\sigma) t^{i_{\sigma(1)} i_{\sigma(2)} \cdots i_{\sigma(k)}}
    \end{split}
\]

\newpage

\section{Inner Product}

Let $V$ be a finite dimensional real linear space. Then we have the following definitions.

\begin{definition}[Inner Product]
    An inner product on $V$ is a map $\langle -, - \rangle : V \times V \to \R$ such that 
    \begin{enumerate}
        \item \emph{Bilinearity:} $\langle -, u \rangle$ and $\langle u, - \rangle$ are linear functionals on $V$ for all $u \in V$;
        \item \emph{Symmetry:} $\langle u, v \rangle = \langle v, u \rangle$ for all $u, v \in V$;
        \item \emph{Positive-definiteness:} $\langle v, v \rangle \geq 0$ for all $v \in V$ with equality if and only if $v = 0$.
    \end{enumerate}
\end{definition}

Note that an inner product on $V$ is a positive-definite symmetric 2-form on $V$.

\begin{definition}[Pseudo Inner Product]
    A pseudo inner product on $V$ is a non-degenerate symmetric bilinear form on $V$, i.e., an element $\langle-, -\rangle \in \mathcal{S}^2 V^*$ such that $\langle-, -\rangle_{\musNatural} : V \to V^*$ is isomorphic.
\end{definition}

Then a real linear space $V$ with an inner product $\langle -, - \rangle$ is called a \emph{Euclidean vector space}, denoted by $(V, \langle -, - \rangle)$. 

\begin{definition}[Metric Space]
    A metric space is a non-empty set $X$ together with a metric structure, i.e., a distance function $d : X \times X \to \R$ that sends $(x, y)$ to $d(x, y)$ such that
    \begin{enumerate}
        \item \emph{Positivity:} $d(x, y) \geq 0$ for all $x, y \in X$ with equality if and only if $x = y$;
        \item \emph{Symmetry:} $d(x, y) = d(y, x)$ for all $x, y \in X$;
        \item \emph{Triangle Inequality:} $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$.
    \end{enumerate}
\end{definition}

If we want to combine the metric structure with the linear structure on $V$, we have to make sure that the distance function $d : V \times V \to \R$ satisfies the two additional properties in order to be compatible with the linear structure. We would say the properties are \emph{harmonic}\index{harmonic} with the linear structure.

\begin{definition}[Normed Linear Space]
    A real normed linear space is a real linear space $V$ together with a compatible metric structure or a normed structure, i.e., a distance function $d : V \times V \to \R$ such that
    \begin{enumerate}
        \item \emph{Translation Invariance:} $d(u + w, v + w) = d(u, v)$ for all $u, v, w \in V$;
        \item \emph{Homogeneity:} $d(\alpha u, \alpha v) = |\alpha| d(u, v)$ for all $u, v \in V$ and $\alpha \in \R$.
    \end{enumerate}
    Then we can define the norm on $V$ as $\| v \| = d(v, 0)$ for all $v \in V$.
\end{definition}

Then a function $\| - \| : V \to \R$ that sends $v$ to $\| v \|$ is called a norm on $V$ if it satisfies:
\begin{enumerate}
    \item \emph{Positive-definiteness:} $\| v \| \geq 0$ for all $v \in V$ with equality if and only if $v = 0$;
    \item \emph{Homogeneity:} $\| \alpha v \| = |\alpha| \| v \|$ for all $v \in V$ and $\alpha \in \R$;
    \item \emph{Triangle Inequality:} $\| u + v \| \leq \| u \| + \| v \|$ for all $u, v \in V$.
\end{enumerate}
We can use the norm with the properties above to define the distance function by $d(x, y) = \| x - y \|$.

\begin{theorem}[Parallelogram Law]
    The parallelogram law states that the sum of the squares of the lengths of the four sides of a parallelogram equals the sum of the squares of the lengths of the two diagonals, i.e., with the following figure:
    \begin{center}
        \begin{tikzpicture}[scale=2]
            \draw (0, 0) coordinate (A) -- (2, 0) coordinate (B) -- (2.5, 1) coordinate (C) -- (0.5, 1) coordinate (D) -- cycle;
            \draw[violet, -latex, thick] (A) -- (C) node[pos=0.7, below right] {$u + v$};
            \draw[red, -latex, thick] (B) -- (D) node[pos=0.7, below left] {$u - v$};
            \draw[teal, -latex, thick] (A) -- (B) node[midway, below] {$u$};
            \draw[ocre, -latex, thick] (A) -- (D) node[midway, left] {$v$};
        \end{tikzpicture}
    \end{center}
    we have:
    \[
        \| u + v \|^2 + \| u - v \|^2 = 2 \| u \|^2 + 2 \| v \|^2
    \]
\end{theorem}

\begin{proposition}
    An inner product on $V$ is equivalence to a norm structure on $V$ which satisfies the parallelogram law.
\end{proposition}
\begin{proof}
    ($\Rightarrow$) Let $(V, \langle -, - \rangle)$ be a Euclidean vector space. Then we can define the norm on $V$ as $\| v \| = \sqrt{\langle v, v \rangle}$ for all $v \in V$. Then we have:
    \begin{enumerate}
        \item \emph{Positive-definiteness:} $\| v \| = \sqrt{\langle v, v \rangle} \geq 0$ for all $v \in V$ with equality if and only if $v = 0$;
        \item \emph{Homogeneity:} $\| \alpha v \| = \sqrt{\langle \alpha v, \alpha v \rangle} = \sqrt{\alpha^2 \langle v, v \rangle} = |\alpha| \| v \|$ for all $v \in V$ and $\alpha \in \R$;
        \item \emph{Triangle Inequality:} By Cauchy-Schwarz inequality, we have:
        \[
            \begin{split}
                \| u + v \| &= \sqrt{\langle u + v, u + v \rangle} = \sqrt{\langle u, u \rangle + \langle v, v \rangle + \langle u, v \rangle + \langle v, u \rangle} \\
                &= \sqrt{\| u \|^2 + \| v \|^2 + 2 \langle u, v \rangle} \\
                &\leq \sqrt{\| u \|^2 + \| v \|^2 + 2 \| u \| \| v \|} \\
                &= \sqrt{(\| u \| + \| v \|)^2} = \| u \| + \| v \|
            \end{split}
        \]
        Therefore, the triangle inequality holds.
        \item \emph{Parallelogram Law:} We have:
        \[
            \begin{split}
                \| u + v \|^2 + \| u - v \|^2 &= \langle u + v, u + v \rangle + \langle u - v, u - v \rangle \\
                &= \langle u, u \rangle + \langle v, v \rangle + \langle u, v \rangle + \langle v, u \rangle + \langle u, u \rangle + \langle v, v \rangle - \langle u, v \rangle - \langle v, u \rangle \\
                &= 2 \langle u, u \rangle + 2 \langle v, v \rangle = 2 \| u \|^2 + 2 \| v \|^2
            \end{split}
        \]
    \end{enumerate}

    ($\Leftarrow$) We define the inner product for all $u, v \in V$ as follows and the proof is left as an exercise:
    \[
        \langle u, v \rangle = \frac{1}{2} \left( \| u + v \|^2 - \| u \|^2 - \| v \|^2 \right)
    \]
\end{proof}

\begin{theorem}[Cauchy-Schwarz Inequality]
    Let $(V, \langle -, - \rangle)$ be a Euclidean vector space. Then for all $u, v \in V$, we have:
    \[
        |\langle u, v \rangle| \leq \| u \| \| v \|
    \]
    with equality if and only if $u$ and $v$ are linearly dependent.
\end{theorem}
\begin{proof}
    Let $f(t) = \| tu + v \|^2 = \langle tu + v, tu + v \rangle = t^2 \| u \|^2 + 2t \langle u, v \rangle + \| v \|^2$ for all $t \in \R$. Then we have $f(t) \geq 0$ for all $t \in \R$. For $u = 0$, the inequality holds trivially. For $u \neq 0$, the quadratic function $f(t)$ has at most one real root, so its discriminant is less than or equal to zero:
    \[
        \Delta = 4 \langle u, v \rangle^2 - 4 \| u \|^2 \| v \|^2 \leq 0 \implies \langle u, v \rangle^2 \leq \| u \|^2 \| v \|^2
    \]
\end{proof}

\begin{definition}
    If both $u, v \in V$ are non-zero vectors in a Euclidean vector space $(V, \langle -, - \rangle)$, then the angle $\theta$ between $u$ and $v$ is defined as:
    \[
        \theta = \arccos{\left( \frac{\langle u, v \rangle}{\| u \| \| v \|} \right)}
    \]
    Moreover, if $\langle u, v \rangle = 0$, then we say that $u$ and $v$ are orthogonal.
\end{definition}

\newpage

\section{Orthogonality}

Let $V$ be a Euclidean vector space with inner product $\langle -, - \rangle$ and $W \subseteq V$ is a subspace of $V$. Then we claim that $W$ inherits an Euclidean structure from $\langle -, - \rangle$ in $V$. We can simply restrict the inner product $\langle -, - \rangle$ on $V$ to $W$:
\begin{center}
    \begin{tikzcd}
        W \times W \arrow[r, hook] \arrow[rr, bend right, "{\langle -, - \rangle}"'] & V \times V \arrow[r, "{\langle -, - \rangle}"] & \R
    \end{tikzcd}
\end{center}
Note that the restriction $\langle -, - \rangle$ is still an inner product on $W$. Also, the positive-definiteness of $\langle -, - \rangle$ implies that $\langle -, - \rangle$ is non-degenerate, i.e., the map $\langle -, - \rangle_{\musNatural} : W \to W^*$ is isomorphism. Note that $W$ and $W^*$ have the same dimension and it has a trivial kernel: $\langle u, - \rangle_W = 0$ implies $\langle u, u \rangle_W = 0$ implies $u = 0$. Now, suppose $w = (w_1, \cdots, w_k)$ is a basis of $W$ and $w^* = (w_1^*, \cdots, w_k^*)$ is the dual basis of $W^*$, then we have the following diagram:
\begin{center}
    \begin{tikzcd}
        0 \arrow[r] & \ker{\lambda_w} \arrow[r] & V \arrow[r, "\lambda_w"', two heads] & \R^k \arrow[r] \arrow[l, bend right, "s"'] & 0 \\
        & & W \arrow[u, hook] \arrow[r, "{\langle -, - \rangle_{\musNatural}}", hook, two heads] & W^* \arrow[u, "{[-]_{w^*}}"', two heads, hook] \\[-3.6em]
        & & {\scriptstyle w_i} \arrow[r, mapsto] & {\scriptstyle \langle w_i, - \rangle}
    \end{tikzcd}
\end{center}
where $\lambda_w = \begin{bmatrix}
    \langle w_1, - \rangle \\
    \vdots \\
    \langle w_k, - \rangle
\end{bmatrix}$, and $s$ is a section of $\lambda_w$ with image $W$. Then we have the decomposition:
\[
    V = \Im{s} \oplus \ker{\lambda_w} = W \oplus \ker{\lambda_w}
\]
Note that it is an internal direct sum. Then we define the orthogonal complement of $W$ in $V$ as follows.
\begin{definition}[Orthogonal Complement]
    The orthogonal complement of $W$ in $V$, denoted by $W^\perp$, is defined as:
    \[
        W^\perp = \{ v \in V \mid \langle v, w \rangle = 0 \text{ for all } w \in W \} = \{ v \in V \mid \langle v, w_i \rangle = 0 \text{ for all basis } w_i \in W \}
    \]
    Then we have the decomposition:
    \[
        V = W \oplus W^\perp
    \]
\end{definition}

Then any vector $v \in V$ can be uniquely decomposed as $v = w + w^\perp$ with $w = \proj_W(v) \in W$ and $w^\perp = \proj_{W^\perp}(v) \in W^\perp$. The map $\proj_W : V \to W$ is called the orthogonal projection onto $W$ along $W^\perp$. Take a look at the following figure:
\begin{center}
    \begin{tikzpicture}
        \draw (-0.1, 0.2) -- (-0.3, 0.1) -- (-0.2, -0.1);
        \draw[ocre] (-4, -2) -- (4, 2) node[right] {$W$};
        \draw[red] (-1.5, 3) node[above] {$W^\perp$} -- (1.5, -3);
        \draw[thick, -latex] (0, 0) -- (4, 0) node[right] {$\vec{v}$};
        \draw[ocre, -latex] (0, 0) -- (3.2, 1.6) node[midway, above, rotate=26.57] {$\scriptstyle w = \proj_W v$};
        \draw[red, -latex] (0, 0) -- (0.8, -1.6) node[midway, below, rotate=-63.43] {$\scriptstyle w^\perp = \proj_{W^\perp} v$};
        \draw[ocre, dashed] (4, 0) -- (3.2, 1.6);
        \draw[red, dashed] (4, 0) -- (0.8, -1.6);
    \end{tikzpicture}
\end{center}

Then we have the following properties of the orthogonal projection:
\begin{enumerate}
    \item $(\proj_W)^2 = \proj_W$;
    \item $\Im{\proj_W} = W$;
    \item $\ker{\proj_W} = W^\perp$;
    \item $\proj_W + \proj_{W^\perp} = \id_V$.
\end{enumerate}

\begin{definition}[Orthonormal Basis]
    A basis $v$ is orthogonal if $\langle v_i, v_j \rangle = 0$ for all $i \neq j$. An orthogonal basis is orthonormal if $\| v_i \| = 1$ for all $i$.
\end{definition}

Then we have the following proposition.
\begin{proposition}
    For any Euclidean vector space $V$ with inner product, there exists an orthonormal basis of $V$. Moreover, there exists a linear isometric isomorphism between $V$ and $\R^n$ with the standard inner product, the dot product.
\end{proposition}
\begin{remark}
    Note that $\R^n$ is up to isomorphism the only Euclidean vector space with dimension $n$.
\end{remark}

\newpage

\section{Gram-Schmidt Process}
