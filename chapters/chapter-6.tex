%----------------------------------------------------------------------------------------
%	CHAPTER 6
%----------------------------------------------------------------------------------------

\chapter{Euclidean Spaces}

\epigraph{``The idea of representation is one of the few great ideas in Mathematics.''}{Guowu Meng}

Before studying Euclidean spaces, we first review tensors and then introduce inner products.

\section{Tensor}

Let $V$ be a finite dimensional vector space over a field $\F$.
Then we have the following definitions.

\begin{definition}[$k$-forms]
	A \emph{$k$-form}\index{k-form@$k$-form} on $V$ is a multilinear map:
	\[
		\underbrace{V \times V \times \cdots \times V}_{k \text{ times}} \to \F
	\]
	which is linear in each argument.
	It is an element in $(V^*)^{\otimes k}$.
\end{definition}

More concretely, for 1-form, it is a linear functional on $V$, i.e.~an element in $V^*$.
It is also called \emph{covector}\index{covector}.
For 2-form, it is a bilinear map on $V$, i.e.~an element in $V^* \otimes V^*$.
To prove that the set of all 2-forms on $V$ is isomorphic to $V^* \otimes V^*$, we can consider the following diagram:
\begin{center}
	\begin{tikzcd}
		\Map^{\mathsf{ML}} (V \times V, \F) \arrow[r, "\equiv" description, phantom] \arrow[d, dashed] & \Hom(V, V^*) \arrow[d, "\vequiv" description, phantom] \\
		V^* \otimes V^* & \Hom(V, \F) \otimes V^* \arrow[l, "\equiv" description, phantom]
	\end{tikzcd}
\end{center}
Remember that $\Hom(V_1, V_2 \otimes V_3) \equiv \Hom(V_1, V_2) \otimes V_3$.

Moreover, we have the following two special types of 2-forms which are the elements inside the symmetric and exterior powers of $V^*$.

\begin{definition}[Symmetric and Skew-symmetric 2-forms]
	A 2-form $\omega : V \times V \to \F$ is called \emph{symmetric}\index{symmetric} if
	\[
		\omega (u, v) = \omega (v, u)
	\]
	for all $u, v \in V$.
	It is an element in $\mathcal{S}^2{V^*}$.
	The 2-form $\omega$ is called \emph{skew-symmetric}\index{skew-symmetric}, or antisymmetric, if
	\[
		\omega (u, v) = - \omega (v, u)
	\]
	for all $u, v \in V$.
	It is an element in ${\bigwedge}^2{V^*}$.
\end{definition}

Then we define the tensor spaces.

\begin{definition}[Tensor Spaces]
	Let $V$ be a finite dimensional vector space over a field $\F$.
	The \emph{tensor space of type $(r, s)$ on $V$}\index{tensor space} is defined as:
	\[
		\mathcal{T}^{r, s} V = \underbrace{V \otimes V \otimes \cdots \otimes V}_{r \text{ times}} \otimes \underbrace{V^* \otimes V^* \otimes \cdots \otimes V^*}_{s \text{ times}}
	\]
	Elements in $\mathcal{T}^{r, s} V$ are called \emph{tensors of type $(r, s)$ on $V$}, which is a mixed type if $r, s \neq 0$.
\end{definition}

If a tensor of type $(r, 0)$, then it is called a \emph{contravariant tensor}\index{contravariant tensor} or simply a \emph{tensor}\index{tensor}.
If a tensor of type $(0, s)$, then it is called a \emph{covariant tensor}\index{covariant tensor} or simply a \emph{form}\index{form}.
For $\mathcal{T}^{0, 0} V$, it is defined as $\F$ itself.
Any elements in $\mathcal{T}^{0, 0} V$ are \emph{scalar type}\index{scalar type} tensor on $V$, or simply \emph{scalars}\index{scalars}.

Then we know that $\End(V) \equiv V \otimes V^* \equiv \mathcal{T}^{1, 1} V$.
Therefore, any endomorphism on $V$ can be viewed as a tensor of type $(1, 1)$ on $V$, represented by $a^i_j$ with respect to a basis $\B_V = \{ \vec v_1, \vec v_2, \cdots, \vec v_n \}$ of $V$.
Here the upper index $i$ represents the contravariant part and the lower index $j$ represents the covariant part.
To know that what $a^i_j$ means, we can consider the following diagram:
\begin{center}
	\begin{tikzcd}
		V \arrow[r, "T"] \arrow[d, "{[-]_{\B_V}}" swap] & V \arrow[d, "{[-]_{\B_V}}"] \\
		\F^n \arrow[r, "{A = [a^i_j]_{\B_V}}" swap] & \F^n
	\end{tikzcd}
\end{center}
Then how to get the matrix representation $A = [a^i_j]_{\B_V}$ of $T$ with respect to the basis $\B_V$? We have:
\[
	\vec a_j = A \vec e_j, \qquad a^i_j = \vec e_i^T A \vec e_j = \hat e^i A \vec e_j = \langle \hat e^i, A \vec e_j \rangle.
\]
So we have $[a^i_j] = \langle \hat v^i, T \vec v_j \rangle$.
We can have an identification between $\End(V)$ and $\mathcal{T}^{1, 1} V$ as follows:
\[
	T \leftrightarrow T\vec v_j \otimes \hat v^j
\]

For covariant and contravariant, we have the following table:

\begin{center}
\begin{tabularx}{\textwidth}{X X}
	\toprule
	\textbf{Object} & \textbf{Transformation Type} \\
	\midrule
	Standard Basis Vector ($\vec e_i$) & Covariant \\
	\midrule
	Dual Basis Vector ($\hat e^i$) & Contravariant \\
	\midrule
	Component of a Vector ($v^i$) & Contravariant \\
	\midrule
	Component Basis Vector ($v_i$) & Covariant \\
	\bottomrule
\end{tabularx}
\end{center}

An object is considered as covariant if it transform in the same way as the basis vectors of the original vector space.
If you cannot understand it, make up some examples of scaling the vector spaces.

In general, an element $t \in \mathcal{T}^{r, s} V$ can be represented as:
\[
	t^{i_1 i_2 \cdots i_r}_{j_1 j_2 \cdots j_s} \vec v_{i_1} \otimes \vec v_{i_2} \otimes \cdots \otimes \vec v_{i_r} \otimes \hat v^{j_1} \otimes \hat v^{j_2} \otimes \cdots \otimes \hat v^{j_s}
\]
Note that the representation depends on the choice of basis $\B_V$ of $V$, i.e., the following two represents the same tensor with respect to different bases:
\[
	\left[t^{i_1 i_2 \cdots i_r}_{j_1 j_2 \cdots j_s}\right]_{\B_V} \sim \left[\tilde t^{\tilde i_1 \tilde i_2 \cdots \tilde i_r}_{\tilde j_1 \tilde j_2 \cdots \tilde j_s}\right]_{\widetilde{\B_V}}
\]
The two representations are related by the base change matrices::
\[
	(\tilde v_1, \tilde v_2, \cdots, \tilde v_n) = (v_1, v_2, \cdots, v_n) A, \quad A = [a^i_{\tilde j}]_{\B_V}^{\widetilde{\B_V}} \in \GL(V)
\]
\begin{remark}
	It is actually the right action of $\GL(V)$ on the set of all bases of $V$, $\B_V$:
	\[
		\B_V \times \GL(V) \to \B_V, \quad (v, A) \mapsto v A = \tilde v
	\]
\end{remark}
Then we have the following equation:
\[
	\tilde v_{\tilde j} = v_i a^i_{\tilde j}
\]
For $A^{-1} = [b_j^{\tilde i}]_{\widetilde{\B_V}}^{\B_V}$, we have $a^i_{\tilde j} b_j^{\tilde k} = \delta^{\tilde k}_{\tilde j}$ and $b_j^{\tilde i} a^j_{\tilde k} = \delta^{\tilde i}_{\tilde k}$.
Therefore, we have:
\[
	v_k = \tilde v_{\tilde j} b_k^{\tilde j}
\]
\begin{remark}
	For easier memorising, we use the calculus operators:
	\[
		\frac{\partial \tilde v_{\tilde j}}{\partial v_i} = a^i_{\tilde j}, \quad \frac{\partial v_k}{\partial \tilde v_{\tilde j}} = b_k^{\tilde j}
	\]
	To memorise it, we consider the lower indices in denominators (lower) will flip to the upper indices in numerators.
	(As lower twice, so flip to upper)

	Then we can use the chain rule to verify the two equations of $A$ and $A^{-1}$:
	\[
		\frac{\partial \tilde v_{\tilde j}}{\partial v_i} \frac{\partial v_k}{\partial \tilde v_{\tilde j}} = \delta^i_k
	\]
\end{remark}

Then we have the transformation rule for the representation of $t \in \mathcal{T}^{r, s} V$ under the base change from $\B_V$ to $\widetilde{\B_V}$:
\[
	\tilde t^{{\color{ocre} \tilde i_1 \tilde i_2 \cdots \tilde i_r}}_{{\color{red} \tilde j_1 \tilde j_2 \cdots \tilde j_s}} = \left({\color{ocre} b_{i_1}^{\tilde i_1} b_{i_2}^{\tilde i_2} \cdots b_{i_r}^{\tilde i_r}}\right) t^{{\color{ocre} i_1 i_2 \cdots i_r}}_{{\color{red} j_1 j_2 \cdots j_s}} \left({\color{red} a^{j_1}_{\tilde j_1} a^{j_2}_{\tilde j_2} \cdots a^{j_s}_{\tilde j_s}}\right)
\]

Given that $\B_V = \{ \vec v_1, \cdots, \vec v_n \}$ is a basis of $V$, then we can define a basis of $\mathcal{T}^{r, s} V$ as follows:
\[
	\B_{\mathcal{T}^{r, s} V} = \{ \vec v_{i_1} \otimes \vec v_{i_2} \otimes \cdots \otimes \vec v_{i_r} \otimes \hat v^{j_1} \otimes \hat v^{j_2} \otimes \cdots \otimes \hat v^{j_s} : 1 \leq i_1, i_2, \cdots, i_r, j_1, j_2, \cdots, j_s \leq n \}
\]
Then for symmetric and skew-symmetric $k$-forms, we have:
\begin{align*}
	\B_{\mathcal{S}^k V} &= \{ \vec v_{i_1} \vec v_{i_2} \cdots \vec v_{i_k} : 1 \leq i_1, i_2, \cdots, i_k \leq n \} \\
	\B_{{\bigwedge}^k V} &= \{ \vec v_{i_1} \wedge \vec v_{i_2} \wedge \cdots \wedge \vec v_{i_k} : 1 \leq i_1, i_2, \cdots, i_k \leq n \}
\end{align*}
Then ``honest'' definition of symmetric basis is:
\[
	\{ \vec v_{i_1} \vec v_{i_2} \cdots \vec v_{i_k} : 1 \leq i_1 \leq i_2 \leq \cdots \leq i_k \leq n \}
\]
but it is redundant.
We just have to make sure that the representation of any symmetric $k$-form is unique for a given basis.
For example, in 2-form case with the basis $\{ \vec e_i \otimes \vec e_j \}$, we originally have to write:
\[
	t = \sum_{1 \leq i \leq j \leq n} t_{ij} \vec e_i \otimes \vec e_j
\]
but this is ugly, so we just write:
\[
	t = t^{ij} \vec e_i \vec e_j
\]
with $t^{ij} = t^{ji}$.
If we ignored the condition on $t^{ij}$, then we have $a^{ij} = - a^{ji}$ such that:
\[
	t = t^{ij} \vec e_i \wedge \vec e_j + a^{ij} \vec e_i \wedge \vec e_j = (t^{ij} + a^{ij}) \vec e_i \wedge \vec e_j = 0
\]
As $a^{ij} = a^{ji} = - a^{ij}$.


Then for skew-symmetric basis, let say $t \in \B_{{\bigwedge}^k V}$, then we have:
\[
	t = t^{\mathcal{I}} \vec v_{\mathcal{I}} = t^{i_1 i_2 \cdots i_k} \vec v_{i_1} \wedge \vec v_{i_2} \wedge \cdots \wedge \vec v_{i_k}
\]
with $\mathcal{I} = (i_1, i_2, \cdots, i_k)$ being an ordered index set with $1 \leq i_1 < i_2 < \cdots < i_k \leq n$.
Then for any permutation $\sigma \in S_k$, to make sure it is unique, we require:
\[
	t^{\sigma(\mathcal{I})} = \sgn(\sigma) t^{\mathcal{I}}
\]
where $\sigma(\mathcal{I}) = (i_{\sigma(1)}, i_{\sigma(2)}, \cdots, i_{\sigma(k)})$.

In conclusion, we have to make sure that the representation of any symmetric or skew-symmetric $k$-form is unique for a given basis by the following conditions respectively:
\begin{align*}
	&\text{Symmetric:} \quad t^{i_1 i_2 \cdots i_k} = t^{i_{\sigma(1)} i_{\sigma(2)} \cdots i_{\sigma(k)}} \\
	&\text{Skew-symmetric:} \quad t^{i_1 i_2 \cdots i_k} = \sgn(\sigma) t^{i_{\sigma(1)} i_{\sigma(2)} \cdots i_{\sigma(k)}}
\end{align*}

\newpage

\section{Inner Product}

Let $V$ be a finite dimensional real linear space.
Then we have the following definitions.

\begin{definition}[Inner Product]
	An inner product on $V$ is a map $\langle -, - \rangle : V \times V \to \R$ such that
	\begin{enumerate}
		\item \emph{Bilinearity:} $\langle -, u \rangle$ and $\langle u, - \rangle$ are linear functionals on $V$ for all $u \in V$;
		\item \emph{Symmetry:} $\langle u, v \rangle = \langle v, u \rangle$ for all $u, v \in V$;
		\item \emph{Positive-definiteness:} $\langle v, v \rangle \geq 0$ for all $v \in V$ with equality if and only if $v = 0$.
	\end{enumerate}
\end{definition}

Note that an inner product on $V$ is a positive-definite symmetric 2-form on $V$.

\begin{definition}[Pseudo Inner Product]
	A pseudo inner product on $V$ is a non-degenerate symmetric bilinear form on $V$, i.e., an element $\langle-, -\rangle \in \mathcal{S}^2 V^*$ such that $\langle-, -\rangle_{\musNatural} : V \to V^*$ is isomorphic.
\end{definition}

Then a real linear space $V$ with an inner product $\langle -, - \rangle$ is called a \emph{Euclidean space}\index{Euclidean space}, denoted by $(V, \langle -, - \rangle)$.

\begin{definition}[Metric Space]
	A metric space is a non-empty set $X$ together with a metric structure, i.e., a distance function $d : X \times X \to \R$ that sends $(x, y)$ to $d(x, y)$ such that
	\begin{enumerate}
		\item \emph{Positivity:} $d(x, y) \geq 0$ for all $x, y \in X$ with equality if and only if $x = y$;
		\item \emph{Symmetry:} $d(x, y) = d(y, x)$ for all $x, y \in X$;
		\item \emph{Triangle Inequality:} $d(x, z) \leq d(x, y) + d(y, z)$ for all $x, y, z \in X$.
	\end{enumerate}
\end{definition}

If we want to combine the metric structure with the linear structure on $V$, we have to make sure that the distance function $d : V \times V \to \R$ satisfies the two additional properties in order to be compatible with the linear structure.
We would say the properties are \emph{harmonic}\index{harmonic} with the linear structure.

\begin{definition}[Normed Linear Space]
	A real normed linear space is a real linear space $V$ together with a compatible metric structure or a normed structure, i.e., a distance function $d : V \times V \to \R$ such that
	\begin{enumerate}
		\item \emph{Translation Invariance:} $d(u + w, v + w) = d(u, v)$ for all $u, v, w \in V$;
		\item \emph{Homogeneity:} $d(\alpha u, \alpha v) = |\alpha| d(u, v)$ for all $u, v \in V$ and $\alpha \in \R$.
	\end{enumerate}
	Then we can define the norm on $V$ as $\| v \| = d(v, 0)$ for all $v \in V$.
\end{definition}

Then a function $\| - \| : V \to \R$ that sends $v$ to $\| v \|$ is called a norm on $V$ if it satisfies:
\begin{enumerate}
	\item \emph{Positive-definiteness:} $\| v \| \geq 0$ for all $v \in V$ with equality if and only if $v = 0$;
	\item \emph{Homogeneity:} $\| \alpha v \| = |\alpha| \| v \|$ for all $v \in V$ and $\alpha \in \R$;
	\item \emph{Triangle Inequality:} $\| u + v \| \leq \| u \| + \| v \|$ for all $u, v \in V$.
\end{enumerate}
We can use the norm with the properties above to define the distance function by $d(x, y) = \| x - y \|$.

\begin{theorem}[Parallelogram Law]
	The parallelogram law states that the sum of the squares of the lengths of the four sides of a parallelogram equals the sum of the squares of the lengths of the two diagonals, i.e., with the following figure:
	\begin{center}
		\begin{tikzpicture}[scale=2]
			\draw (0, 0) coordinate (A) -- (2, 0) coordinate (B) -- (2.5, 1) coordinate (C) -- (0.5, 1) coordinate (D) -- cycle;
			\draw[violet, -latex, thick] (A) -- (C) node[pos=0.7, below right] {$u + v$};
			\draw[red, -latex, thick] (B) -- (D) node[pos=0.7, below left] {$u - v$};
			\draw[teal, -latex, thick] (A) -- (B) node[midway, below] {$u$};
			\draw[ocre, -latex, thick] (A) -- (D) node[midway, left] {$v$};
		\end{tikzpicture}
	\end{center}
	we have:
	\[
		\| u + v \|^2 + \| u - v \|^2 = 2 \| u \|^2 + 2 \| v \|^2
	\]
\end{theorem}

\begin{proposition}
	An inner product on $V$ is equivalence to a norm structure on $V$ which satisfies the parallelogram law.
\end{proposition}
\begin{proof}
	($\Rightarrow$) Let $(V, \langle -, - \rangle)$ be a Euclidean space.
	Then we can define the norm on $V$ as $\| v \| = \sqrt{\langle v, v \rangle}$ for all $v \in V$.
	Then we have:
	\begin{enumerate}
		\item \emph{Positive-definiteness:} $\| v \| = \sqrt{\langle v, v \rangle} \geq 0$ for all $v \in V$ with equality if and only if $v = 0$;
		\item \emph{Homogeneity:} $\| \alpha v \| = \sqrt{\langle \alpha v, \alpha v \rangle} = \sqrt{\alpha^2 \langle v, v \rangle} = |\alpha| \| v \|$ for all $v \in V$ and $\alpha \in \R$;
		\item \emph{Triangle Inequality:} By Cauchy-Schwarz inequality, we have:
		\begin{align*}
			\| u + v \| &= \sqrt{\langle u + v, u + v \rangle} = \sqrt{\langle u, u \rangle + \langle v, v \rangle + \langle u, v \rangle + \langle v, u \rangle} \\
			&= \sqrt{\| u \|^2 + \| v \|^2 + 2 \langle u, v \rangle} \\
			&\leq \sqrt{\| u \|^2 + \| v \|^2 + 2 \| u \| \| v \|} \\
			&= \sqrt{(\| u \| + \| v \|)^2} = \| u \| + \| v \|
		\end{align*}
		Therefore, the triangle inequality holds.
		\item \emph{Parallelogram Law:} We have:
		\begin{align*}
			\| u + v \|^2 + \| u - v \|^2 &= \langle u + v, u + v \rangle + \langle u - v, u - v \rangle \\
			&= \langle u, u \rangle + \langle v, v \rangle + \langle u, v \rangle + \langle v, u \rangle + \langle u, u \rangle + \langle v, v \rangle - \langle u, v \rangle - \langle v, u \rangle \\
			&= 2 \langle u, u \rangle + 2 \langle v, v \rangle = 2 \| u \|^2 + 2 \| v \|^2
		\end{align*}
	\end{enumerate}

	($\Leftarrow$) We define the inner product for all $u, v \in V$ as follows and the proof is left as an exercise:
	\[
		\langle u, v \rangle = \frac{1}{2} \left( \| u + v \|^2 - \| u \|^2 - \| v \|^2 \right)
	\]
	% TODO: Complete the proof
\end{proof}

\begin{theorem}[Cauchy-Schwarz Inequality]
	Let $(V, \langle -, - \rangle)$ be a Euclidean space.
	Then for all $u, v \in V$, we have:
	\[
		|\langle u, v \rangle| \leq \| u \| \| v \|
	\]
	with equality if and only if $u$ and $v$ are linearly dependent.
\end{theorem}
\begin{proof}
	Let $f(t) = \| tu + v \|^2 = \langle tu + v, tu + v \rangle = t^2 \| u \|^2 + 2t \langle u, v \rangle + \| v \|^2$ for all $t \in \R$.
	Then we have $f(t) \geq 0$ for all $t \in \R$.
	For $u = 0$, the inequality holds trivially.
	For $u \neq 0$, the quadratic function $f(t)$ has at most one real root, so its discriminant is less than or equal to zero:
	\[
		\Delta = 4 \langle u, v \rangle^2 - 4 \| u \|^2 \| v \|^2 \leq 0 \implies \langle u, v \rangle^2 \leq \| u \|^2 \| v \|^2
	\]
\end{proof}

\begin{definition}
	If both $u, v \in V$ are non-zero vectors in a Euclidean space $(V, \langle -, - \rangle)$, then the angle $\theta$ between $u$ and $v$ is defined as:
	\[
		\theta = \arccos{\left( \frac{\langle u, v \rangle}{\| u \| \| v \|} \right)}
	\]
	Moreover, if $\langle u, v \rangle = 0$, then we say that $u$ and $v$ are orthogonal.
\end{definition}

\newpage

\section{Orthogonality}

Let $V$ be a Euclidean space with inner product $\langle -, - \rangle$ and $W \subseteq V$ is a subspace of $V$.
Then we claim that $W$ inherits an Euclidean structure from $\langle -, - \rangle$ in $V$.
We can simply restrict the inner product $\langle -, - \rangle$ on $V$ to $W$:
\begin{center}
	\begin{tikzcd}
		W \times W \arrow[r, hook] \arrow[rr, bend right, "{\langle -, - \rangle}"'] & V \times V \arrow[r, "{\langle -, - \rangle}"] & \R
	\end{tikzcd}
\end{center}
Note that the restriction $\langle -, - \rangle$ is still an inner product on $W$.
Also, the positive-definiteness of $\langle -, - \rangle$ implies that $\langle -, - \rangle$ is non-degenerate, i.e., the map $\langle -, - \rangle_{\musNatural} : W \to W^*$ is isomorphism.
Note that $W$ and $W^*$ have the same dimension and it has a trivial kernel: $\langle u, - \rangle_W = 0$ implies $\langle u, u \rangle_W = 0$ implies $u = 0$.
Now, suppose $w = (w_1, \cdots, w_k)$ is a basis of $W$ and $w^* = (w_1^*, \cdots, w_k^*)$ is the dual basis of $W^*$, then we have the following diagram:
\begin{center}
	\begin{tikzcd}
		0 \arrow[r] & \ker(\lambda_w) \arrow[r] & V \arrow[r, "\lambda_w"', two heads] & \R^k \arrow[r] \arrow[l, bend right, "s"'] & 0 \\
		& & W \arrow[u, hook] \arrow[r, "{\langle -, - \rangle_{\musNatural}}", hook, two heads] & W^* \arrow[u, "{[-]_{w^*}}"', two heads, hook] \\[-3.6em]
		& & {\scriptstyle w_i} \arrow[r, mapsto] & {\scriptstyle \langle w_i, - \rangle}
	\end{tikzcd}
\end{center}
where $\lambda_w = \begin{bmatrix}
	\langle w_1, - \rangle \\
	\vdots \\
	\langle w_k, - \rangle
\end{bmatrix}$, and $s$ is a section of $\lambda_w$ with image $W$.
Then we have the decomposition:
\[
	V = \im{s} \oplus \ker(\lambda_w) = W \oplus \ker(\lambda_w)
\]
Note that it is an internal direct sum.
Then we define the orthogonal complement of $W$ in $V$ as follows.
\begin{definition}[Orthogonal Complement]
	The orthogonal complement of $W$ in $V$, denoted by $W^\perp$, is defined as:
	\[
		W^\perp = \{ v \in V \mid \langle v, w \rangle = 0 \text{ for all } w \in W \} = \{ v \in V \mid \langle v, w_i \rangle = 0 \text{ for all basis } w_i \in W \}
	\]
	Then we have the decomposition:
	\[
		V = W \oplus W^\perp
	\]
\end{definition}

Then any vector $v \in V$ can be uniquely decomposed as $v = w + w^\perp$ with $w = \proj_W(v) \in W$ and $w^\perp = \proj_{W^\perp}(v) \in W^\perp$.
The map $\proj_W : V \to W$ is called the orthogonal projection onto $W$ along $W^\perp$.
Take a look at the following figure:
\begin{center}
	\begin{tikzpicture}
		\draw (-0.1, 0.2) -- (-0.3, 0.1) -- (-0.2, -0.1);
		\draw[ocre] (-4, -2) -- (4, 2) node[right] {$W$};
		\draw[red] (-1.5, 3) node[above] {$W^\perp$} -- (1.5, -3);
		\draw[thick, -latex] (0, 0) -- (4, 0) node[right] {$\vec v$};
		\draw[ocre, -latex] (0, 0) -- (3.2, 1.6) node[midway, above, rotate=26.57] {$\scriptstyle w = \proj_W v$};
		\draw[red, -latex] (0, 0) -- (0.8, -1.6) node[midway, below, rotate=-63.43] {$\scriptstyle w^\perp = \proj_{W^\perp} v$};
		\draw[ocre, dashed] (4, 0) -- (3.2, 1.6);
		\draw[red, dashed] (4, 0) -- (0.8, -1.6);
	\end{tikzpicture}
\end{center}

Then we have the following properties of the orthogonal projection:
\begin{enumerate}
	\item $(\proj_W)^2 = \proj_W$;
	\item $\im{\proj_W} = W$;
	\item $\ker(\proj_W) = W^\perp$;
	\item $\proj_W + \proj_{W^\perp} = \id_V$.
\end{enumerate}

\begin{definition}[Orthonormal Basis]
	A basis $v$ is orthogonal if $\langle v_i, v_j \rangle = 0$ for all $i \neq j$.
	An orthogonal basis is orthonormal if $\| v_i \| = 1$ for all $i$.
\end{definition}

Then we have the following proposition.
\begin{proposition}
	For any Euclidean space $V$ with inner product, there exists an orthonormal basis of $V$.
	Moreover, there exists a linear isometric isomorphism between $V$ and $\R^n$ with the standard inner product, the dot product.
\end{proposition}
Note that $(\R^n, \cdot)$ is up to isomorphism the only Euclidean space with dimension $n$, where $\cdot$ denotes the standard dot product.

Moreover, if $w = (w_1, w_2, \cdots, w_k)$ is an orthonormal basis of $W$, then
\[
	\proj_W u = \sum_{i = 1}^k \langle w_i, u \rangle w_i
\]
for all $u \in V$.
In case $w$ is orthogonal but not orthonormal, then we have:
\[
	\proj_W u = \sum_{i = 1}^k \frac{\langle w_i, u \rangle}{\langle w_i, w_i \rangle} w_i
\]

\newpage

\section{Gram-Schmidt Process}

Let $w = (w_1, w_2, \cdots, w_k)$ be an orthonormal basis of $W \subseteq V$.
Then we have:
\[
	x = \underbrace{\sum_{i = 1}^k \langle w_i, x \rangle w_i}_{\in W} + \underbrace{x - \sum_{i = 1}^k \langle w_i, x \rangle w_i}_{\in W^\perp} = \proj_W x + \proj_{W^\perp} x.
\]
To show that $\proj_{W^\perp} x \in W^\perp$, it suffices to show that $\langle w_j, \proj_{W^\perp} x \rangle = 0$ for all $1 \leq j \leq k$:
\begin{align*}
	\langle w_j, \proj_{W^\perp} x \rangle &= \langle w_j, x - \sum_{i = 1}^k \langle w_i, x \rangle w_i \rangle \\
	&= \langle w_j, x \rangle - \sum_{i = 1}^k \langle w_i, x \rangle \langle w_j, w_i \rangle \\
	&= \langle w_j, x \rangle - \langle w_j, x \rangle = 0
\end{align*}
Note that the key step is to use the bilinearity of the inner product and the orthonormality of $w$.

Now, given any basis $x = (x_1, x_2, \cdots, x_n)$ of $V$, we can use the Gram-Schmidt process to construct an orthonormal basis $w = (w_1, w_2, \cdots, w_n)$ of $V$ by inductive argument.
The idea is: We have $V_n \supset V_{n - 1} \supset \cdots \supset V_2 \supset V_1 \supset V_0 = \{ 0 \}$ with the dimension $n, n - 1, \cdots, 2, 1, 0$ respectively.
Then we have $w_1$ as the orthonormal basis of $V_1$, then we can extend it to $w_1, w_2$ as the orthonormal basis of $V_2$, and so on and so forth until we reach $V_n = V$.

Then we consider the first two cases to illustrate the idea.
Let $v_1 = u_1$.
Then we have $w_1 = \frac{v_1}{\| v_1 \|}$ as the orthonormal basis of $V_1 = \Span\{u_1\}$.
Then we want to find the $w_2$ such that $w_1, w_2$ is the orthonormal basis of $V_2 = \Span\{u_1, u_2\}$.
We can consider the following diagram:
\begin{center}
	\begin{tikzpicture}
		\draw (-0.1, 0.2) -- (-0.3, 0.1) -- (-0.2, -0.1);
		\draw[ocre] (-4, -2) -- (4, 2) node[right] {$V_1$};
		\draw[red] (-1.5, 3) node[above] {$V_1^\perp$} -- (1.5, -3);
		\draw[thick, -latex] (0, 0) -- (1, 3) node[above] {$x_2$};
		\draw[thick, -latex] (0, 0) -- (3, 1.5) node[above, magenta] {$v_1$} node[below right] {$x_1$};
		\draw[magenta, -latex, thick] (0, 0) -- (-1, 2) node[left] {$v_2$};

		\draw[violet, -latex, very thick] (0, 0) -- (0.894, 0.447) node[below right] {$w_1$};
		\draw[violet, -latex, very thick] (0, 0) -- (-0.447, 0.894) node[left] {$w_2$};

		\draw[ocre, dashed] (1, 3) -- (2, 1) node[midway, above, rotate=-63.43] {$\scriptstyle \proj_{V_1} x_2$};
		\draw[red, dashed] (1, 3) -- (-1, 2) node[midway, above, rotate=26.57] {$\scriptstyle x_2 - \proj_{V_1} x_2$};
	\end{tikzpicture}
\end{center}
Then $v_2 = x_2 - \proj_{V_1} x_2 = x_2 - \langle w_1, x_2 \rangle w_1$ is orthogonal to $w_1$.
Note that $w_1$ is normalised.
Then we can normalise $v_2$ to get $w_2 = \frac{v_2}{\| v_2 \|}$.
Therefore, $w_1, w_2$ is the orthonormal basis of $V_2$.
Then for general $k$-th step, we have:
\[
	v_k = x_k - \sum_{i = 1}^{k - 1} \langle w_i, x_k \rangle w_i = x_k - \sum_{i = 1}^{k - 1} \frac{\langle v_i, x_k \rangle}{\langle v_i, v_i \rangle} v_i, \quad w_k = \frac{v_k}{\| v_k \|}
\]
given that $w_1, w_2, \cdots, w_{k - 1}$ is the orthonormal basis of $V_{k - 1} = \Span\{x_1, x_2, \cdots, x_{k - 1}\}$ and the orthogonal basis of $V_{k - 1}$, $v_1, v_2, \cdots, v_{k - 1}$.

Then there is a useful corollary of the Gram-Schmidt process, the $QR$ Decomposition.

Let $V$ be a Euclidean space.
We can interpret it as $(\R^n, \cdot)$ up to isomorphism.
Then we have a basis $(\vec x_1, \vec x_2, \cdots, \vec x_n)$ of $V$ and we can form an invertible matrix $A$ whose columns are the vectors $\vec x_1, \vec x_2, \cdots, \vec x_n$, i.e.,
\[
	A = \begin{bmatrix}
		| & | & & | \\
		\vec x_1 & \vec x_2 & \cdots & \vec x_n \\
		| & | & & |
	\end{bmatrix}
\]
Then we have an orthogonal basis $(\vec v_1, \cdots, \vec v_n)$ of $V$ and an orthonormal basis $(\vec w_1, \cdots, \vec w_n)$ obtained by the Gram-Schmidt process.
Then we should have an invertible matrix to convert between bases.
Then what is the matrix to convert from the original basis to the orthonormal basis?

Note that each $\vec x_k$ can be expressed as a linear combination of $\vec w_1, \cdots, \vec w_k$:
\[
	\vec x_k = \vec v_k + \sum_{i = 1}^{k - 1} \frac{\langle \vec v_i, \vec x_k \rangle}{\langle \vec v_i, \vec v_i \rangle} \vec v_i = \| \vec v_k \| \vec w_k + \sum_{i = 1}^{k - 1} \langle \vec w_i, \vec x_k \rangle \vec w_i
\]
Also, we can express $\vec x_k$ as follows:
\[
	\vec x_k = \begin{bmatrix}
		| & | & & | \\
		\vec w_1 & \vec w_2 & \cdots & \vec w_n \\
		| & | & & |
	\end{bmatrix} \begin{bmatrix}
		\langle \vec w_1, \vec x_k \rangle \\
		\langle \vec w_2, \vec x_k \rangle \\
		\vdots \\
		\langle \vec w_{k - 1}, \vec x_k \rangle \\
		\| \vec v_k \| \\
		0 \\
		\vdots \\
		0
	\end{bmatrix}
\]
Then we have the matrix equation:
\[
	\underbrace{\begin{bmatrix}
		| & | & & | \\
		\vec x_1 & \vec x_2 & \cdots & \vec x_n \\
		| & | & & |
	\end{bmatrix}}_{A} = \underbrace{\begin{bmatrix}
		| & | & & | \\
		\vec w_1 & \vec w_2 & \cdots & \vec w_n \\
		| & | & & |
	\end{bmatrix}}_{Q} \underbrace{\begin{bmatrix}
		\langle \vec w_1, \vec x_1 \rangle & \langle \vec w_1, \vec x_2 \rangle & \cdots & \langle \vec w_1, \vec x_n \rangle \\
		0 & \langle \vec w_2, \vec x_2 \rangle & \cdots & \langle \vec w_2, \vec x_n \rangle \\
		0 & 0 & \ddots & \vdots \\
		0 & 0 & 0 & \langle \vec w_n, \vec x_n \rangle
	\end{bmatrix}}_{R}
\]
which is called the \emph{QR Decomposition}\index{QR Decomposition} of $A$, where $Q$ is an orthogonal matrix and $R$ is an upper-triangular matrix with positive diagonal entries.
However, normally we denote the orthogonal matrix by $O$ instead of $Q$ and an upper-triangular matrix by $U$ instead of $R$.

\newpage

\section{Orthogonal Group and Special Orthogonal Group}

Let $V$ be a Euclidean space with inner product $\langle -, - \rangle$.
Then we view $V$ as a linear space, and we have $\Aut(V) = \GL(V)$.
If we view $V$ as a Euclidean space, then we have $\Aut(V) = \Orth(V) \subseteq \GL(V)$, where $\Orth(V)$ is the subgroup of $\GL(V)$ that respects the Euclidean structure, i.e., for all $T \in \Orth(V)$, we have:
\[
	\langle T(u), T(v) \rangle = \langle u, v \rangle
\]
for all $u, v \in V$, so length and angles are preserved under $T$.
Or equivalently, the following diagram commutes:
\begin{center}
	\begin{tikzcd}[column sep=normal]
		& V \times V \arrow[dr, "{\langle -, - \rangle}"] \\
		V \times V \arrow[ur, "T \times T"] \arrow[rr, "{\langle -, - \rangle}"'] & & \R
	\end{tikzcd}
\end{center}
We can also define the orthogonal group $\Orth(n)$ using this property.
Let $V = \R^n$ with the dot product.
Then for any $A \in \GL_n(\R)$, $A \in \Orth(n)$ if and only if $A$ satisfies:
\[
	\langle \vec a_i, \vec a_j \rangle = \langle A\vec e_i, A\vec e_j \rangle = \langle \vec e_i, \vec e_j \rangle = \delta_{ij}
\]
It is equivalent to say that $A^T A = I_n$, i.e., $A^T = A^{-1}$.
Therefore, we have:
\[
	\Orth(n) = \{ A \in \GL_n(\R) \mid A^T A = I_n \}
\]
Note that $\det(A^T) = \det(A)^T = \det(A)$.
Therefore, we have $\det(A)^2 = 1$ for all $A \in \Orth(n)$, i.e., $\det(A) = \pm 1$.

Then consider the following exact sequence:
\begin{center}
	\begin{tikzcd}
		1 \arrow[r] & \SL(V) \arrow[r, hook] & \GL(V) \arrow[r, "\det", two heads] & \R^\times \arrow[r] & 1
	\end{tikzcd}
\end{center}
where $\R^\times = \GL_1(\R) = \R \setminus \{ 0 \}$ is the multiplicative group of non-zero real numbers.
As for any automorphism $A \in \GL(V)$, we have a determinant $\det{A} \in \R^\times$, which is surjective. $\SL(V)$ is defined as the kernel of the determinant map, i.e., $\SL(V) = \{ A \in \GL(V) \mid \det{A} = 1 \}$.

Similarly, we have the special orthogonal group $\SO(V)$ as the subgroup of $\Orth(V)$ with determinant $1$:
\[
	\SO(V) = \{ A \in \Orth(V) \mid \det{A} = 1 \}
\]

\newpage

\section{Matrix Representation of Inner Products}

Let $V$ be a Euclidean space with inner product $\langle -, - \rangle$.
Then we can choose a basis $v = (v_1, v_2, \cdots, v_n)$ of $V$.
Then we have
\[
	x = x^i v_i = \begin{bmatrix}
		x^1 \\
		x^2 \\
		\vdots \\
		x^n
	\end{bmatrix}, \quad y = y^i v_i = \begin{bmatrix}
		y^1 \\
		y^2 \\
		\vdots \\
		y^n
	\end{bmatrix}
\]
Then the inner product $\langle x, y \rangle$ can be represented as:
\[
	\langle x, y \rangle = x^i y^j \langle v_i, v_j \rangle = x^T \omega y = x \cdot (\omega y)
\]
where we let $\omega = [\langle v_i, v_j \rangle]$ be the matrix representation of the inner product with respect to the basis $v$.
Then $\langle -, - \rangle = \cdot \omega -$.
To find the canonical form of the inner product, we left it to the next chapter.

\begin{proposition}[Spectral Theorem for Real Symmetric Matrices]\label{prop:spectral-theorem-real-symmetric-matrices}
	Let $A$ be a $n \times n$ real symmetric matrix.
	Then there exists an orthogonal matrix $O$ and a diagonal matrix $D$ such that:
	\[
		A = O D O^{-1} = O D O^T
	\]
	where the entries of $D$ are the eigenvalues of $A$.
	Or equivalently, there exists an orthonormal basis of $\R^n$ consisting of eigenvectors of $A$.
\end{proposition}
To prove this proposition, we would use the result in Hermitian spaces, so we leave the proof to the next chapter.
