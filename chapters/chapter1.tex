\chapter{Linear Spaces}

\section{Introduction}

Linear algebra originally arose from the study of systems of linear equations. Over time, it has evolved into a fundamental area of mathematics with applications in various fields such as physics, computer science, and economics. In this chapter, we will explore the concept of linear spaces, also known as vector spaces, which provide a framework for understanding linear combinations, subspaces, and linear transformations.

\section{Operations and Structures}

Before delving into linear spaces, it is essential to understand the basic operations and structures that underpin them.

\subsection{Operations on Sets}

There are several types of operations that can be performed on set $S$, including:

\begin{definition}[Unary Operation]\label{def:unary_operation}
  A \emph{unary operation} on a set $S$ is a map
  \begin{align*}
    f \colon S & \to S        \\
    a     & \mapsto f(a)
  \end{align*}
\end{definition}

\begin{example}
  Common examples of unary operations include:
  \begin{itemize}
    \item Logical negation operation $\neg$ on the set $\{ \mathsf{true}, \mathsf{false} \}$;
    \item Numeric negation operation $-$ on the set of real numbers $\R$;
    \item Complex conjugation operation $\overline{z}$ on the set of complex numbers $\C$.
  \end{itemize}
\end{example}

\begin{definition}[Binary Operation]\label{def:binary_operation}
  A \emph{binary operation} on a set $S$ is a map
  \begin{align*}
    \cdot \colon S \times S & \to S             \\
    (a, b)             & \mapsto a \cdot b
  \end{align*}
\end{definition}

\begin{example}
  A common example of a binary operation is the addition operation $+$ on the set of natural numbers $\N$ which assigns to each pair of natural numbers $(a, b)$ their sum $a + b$.
\end{example}

\subsection{Properties of Binary Operations}

There are several properties that binary operations may satisfy:

\begin{definition}[Associative]\label{def:associative}
  A \hyperref[def:binary_operation]{binary operation} $\cdot$ on a set $S$ is \emph{associative} if for all $a, b, c \in S$, we have
  \[ (a \cdot b) \cdot c = a \cdot (b \cdot c) \]
\end{definition}

\begin{example}
  The addition operation $+$ on the set of natural numbers $\N$ is associative since for all $a, b, c \in \N$, we have
  \[ (a + b) + c = a + (b + c) \]
\end{example}

\begin{definition}[Unital]\label{def:unital}
  A \hyperref[def:binary_operation]{binary operation} $\cdot$ on a set $S$ is \emph{unital} if there exists an element $e \in S$ such that for all $a \in S$, we have
  \[ e \cdot a = a = a \cdot e \]
\end{definition}

\begin{example}
  The multiplication operation $\cdot$ on the set of natural numbers $\N$ is unital with the identity element $1$ since for all $a \in \N$, we have
  \[ 1 \cdot a = a = a \cdot 1 \]
\end{example}

\begin{remark}
  Such an element $e$ must be unique if it exists and is called the two-sided \emph{identity element} of the operation. To see why, suppose there are two identity elements $e$ and $e'$. Then we have
  \[ e = e \cdot e' = e' \]
  Note that one-sided identity elements (left or right) may not be unique.
\end{remark}

\begin{definition}[Invertible]\label{def:invertible}
  A \hyperref[def:binary_operation]{binary operation} $\cdot$ on a set $S$ with identity element $e$ is \emph{invertible} if for each $a \in S$, there exists an element $b \in S$ such that
  \[ a \cdot b = e = b \cdot a \]
\end{definition}

\begin{remark}
  Note that invertibility requires the existence of an identity element.
\end{remark}

\begin{example}
  The addition operation $+$ on the set of integers $\Z$ is invertible since for each integer $a \in \Z$, there exists an integer $-a \in \Z$ such that
  \[ a + (-a) = 0 = (-a) + a \]
\end{example}

\begin{remark}
  Such an element $b$ must be unique if it exists and is called the two-sided \emph{inverse} of the element $a$, denoted by $a^{-1}$. To see why, suppose there are two inverses $b$ and $b'$. Then we have
  \[ b = e \cdot b = (a \cdot b') \cdot b = a \cdot (b' \cdot b) = a \cdot e = b' \]
  Note that one-sided inverses (left or right) may not be unique.
\end{remark}

\begin{definition}[Commutative]\label{def:commutative}
  A \hyperref[def:binary_operation]{binary operation} $+$ on a set $S$ is \emph{commutative} if for all $a, b \in S$, we have
  \[ a + b = b + a \]
\end{definition}

\begin{example}
  The addition operation $+$ on the set of natural numbers $\N$ is commutative since for all $a, b \in \N$, we have
  \[ a + b = b + a \]
\end{example}

\begin{definition}[Distributive]\label{def:distributive}
  A \hyperref[def:binary_operation]{binary operation} $\cdot$ on a set $S$ is \emph{distributive} over another \hyperref[def:binary_operation]{binary operation} $+$ on $S$ if for all $a, b, c \in S$, we have
  \[ a \cdot (b + c) = (a \cdot b) + (a \cdot c) \]
  and
  \[ (a + b) \cdot c = (a \cdot c) + (b \cdot c) \]
\end{definition}

The professor prefers to use the term ``harmonic'' instead of ``distributive''. Note that it is important to specify the order of the operations when discussing distributivity, as the two operations may not be commutative with each other.

\begin{example}
  The multiplication operation $\cdot$ on the set of integers $\Z$ is distributive over the addition operation $+$ since for all $a, b, c \in \Z$, we have
  \[ a \cdot (b + c) = (a \cdot b) + (a \cdot c) \]
  and
  \[ (a + b) \cdot c = (a \cdot c) + (b \cdot c) \]
\end{example}

\subsection{Algebraic Structures}

Most objects in mathematics can be described with the following template.
\begin{center}
  \it A \underline{\hspace{3cm}} is a set with a \underline{\hspace{3cm}} structure on it.
\end{center}

Some common algebraic structures include:

\begin{definition}[Monoidic Structure]\label{def:monoidic_structure}
  A \emph{monoidic structure} on a set $M$ is a \hyperref[def:binary_operation]{binary operation} $\cdot$ that is \hyperref[def:associative]{associative} and \hyperref[def:unital]{unital}. The pair $(M, \cdot)$ is called a \emph{monoid}.
\end{definition}

\begin{definition}[Groupic Structure]\label{def:groupic_structure}
  A \emph{groupic structure} on a set $G$ is a \hyperref[def:binary_operation]{binary operation} $\cdot$ that is \hyperref[def:associative]{associative}, \hyperref[def:unital]{unital}, and \hyperref[def:invertible]{invertible}. The pair $(G, \cdot)$ is called a \emph{group}.
\end{definition}

\begin{example}
  The pair $(\R \setminus \{ 0 \}, \times)$, where $\times$ is the multiplication operation on real numbers, forms a group since multiplication is associative, unital (with identity element $1$), and invertible (with inverse element $a^{-1} = \frac{1}{a}$ for each $a \in \R \setminus \{ 0 \}$). Note that $(\R, \times)$ is not a group since $0$ does not have an inverse.
\end{example}

\begin{definition}[Abelian Structure]\label{def:abelian_structure}
  An \emph{abelian structure} on a monoid or group $(A, +)$ is a \hyperref[def:binary_operation]{binary operation} $+$ that is also \hyperref[def:commutative]{commutative}. The pair $(A, +)$ is called an \emph{abelian monoid} or \emph{abelian group} respectively.
\end{definition}

\begin{example}
  The pair $(\Z, +)$, where $+$ is the addition operation on integers, forms an abelian group since addition is associative, unital (with identity element $0$), invertible (with inverse element $-a$ for each $a \in \Z$), and commutative.
\end{example}

\begin{definition}[Ringic Structure]\label{def:ringic_structure}
  A \emph{ringic structure} on a set $R$ is two \hyperref[def:binary_operation]{binary operations} $+$ and $\cdot$ such that
  \begin{itemize}
    \item $(R, +)$ is an \hyperref[def:abelian_structure]{abelian group};
    \item $(R, \cdot)$ is a \hyperref[def:monoidic_structure]{monoid}; and
    \item the operation $\cdot$ is \hyperref[def:distributive]{distributive} over the operation $+$.
  \end{itemize}
  The triple $(R, +, \cdot)$ is called a \emph{ring}.
\end{definition}

\begin{remark}
  In this book, we will only consider unital rings and refer to them simply as ``rings''.
\end{remark}

\begin{definition}[Commutative Ring]\label{def:commutative_ring}
  A \emph{commutative ring} is a \hyperref[def:ringic_structure]{ring} $(R, +, \cdot)$ where the operation $\cdot$ is also \hyperref[def:commutative]{commutative}.
\end{definition}

\begin{definition}[Field]\label{def:field}
  A \emph{field} is a \hyperref[def:commutative_ring]{commutative ring} $(F, +, \cdot)$ where the operation $\cdot$ is also \hyperref[def:invertible]{invertible} on $F \setminus \{ 0 \}$.
\end{definition}

\begin{example}
  The triples $(\Q, +, \times)$, $(\R, +, \times)$, and $(\C, +, \times)$, where $+$ is the addition operation and $\times$ is the multiplication operation on rational numbers, real numbers, and complex numbers respectively, all form fields.
\end{example}

\begin{example}[Finite Field]
  The set $\F_2 = \Z / 2\Z = \{ 0, 1 \}$ with XOR as addition and AND as multiplication forms a field. More generally, for any prime number $p$, the set $\F_p = \Z / p\Z = \{ 0, 1, 2, \ldots, p-1 \}$ with addition and multiplication defined modulo $p$ forms a field.
\end{example}

\section{Homomorphisms}

In mathematics, a \emph{homomorphism} is a structure-preserving map between two algebraic structures of the same type.

\begin{definition}[Monoid Homomorphism]\label{def:monoid_homomorphism}
  A \emph{monoid homomorphism} is a set map $\phi \colon M_1 \to M_2$ between two monoids $(M_1, \cdot)$ and $(M_2, \odot)$ which respects the \hyperref[def:monoidic_structure]{monoidic structure}, i.e., for all $a, b \in M_1$, we have
  \begin{itemize}
    \item $\phi(a \cdot b) = \phi(a) * \phi(b)$;
    \item $\phi(e_1) = e_2$, where $e_1$ and $e_2$ are the identity elements of $M_1$ and $M_2$ respectively.
  \end{itemize}
\end{definition}

\begin{definition}[Group Homomorphism]\label{def:group_homomorphism}
  A \emph{group homomorphism} is a set map $\phi \colon G_1 \to G_2$ between two groups $(G_1, \cdot)$ and $(G_2, \odot)$ which respects the \hyperref[def:groupic_structure]{groupic structure}, i.e., for all $a, b \in G_1$, we have
  \begin{itemize}
    \item $\phi(a \cdot b) = \phi(a) * \phi(b)$;
    \item $\phi(e_1) = e_2$, where $e_1$ and $e_2$ are the identity elements of $G_1$ and $G_2$ respectively;
    \item $\phi(a^{-1}) = {(\phi(a))}^{-1}$.
  \end{itemize}
\end{definition}

\begin{proposition}
  The second and third properties in the definition of group homomorphism are consequences of the first property.
\end{proposition}
\begin{proof}
  Let $\phi \colon G_1 \to G_2$ be a group homomorphism satisfying the first property. For any $a \in G_1$, we have
  \[ \phi(a) = \phi(a \cdot e_1) = \phi(a) * \phi(e_1). \]
  So $\phi(e_1)$ is the identity element of $G_2$, i.e., $\phi(e_1) = e_2$. Similarly, we have
  \[ e_2 = \phi(e_1) = \phi(a \cdot a^{-1}) = \phi(a) * \phi(a^{-1}). \]
  Thus, $\phi(a^{-1})$ is the inverse of $\phi(a)$, i.e., $\phi(a^{-1}) = {(\phi(a))}^{-1}$.
\end{proof}

For monoid homomorphisms, the second property cannot be derived from the first property. Consider the identity element $e_1$ in $M_1$. If we apply the first property, we get $\phi(e_1 \cdot e_1) = \phi(e_1) * \phi(e_1)$. This simplifies to $\phi(e_1) = \phi(e_1) * \phi(e_1)$, which does not necessarily imply that $\phi(e_1)$ is the identity element in $M_2$, i.e., $\phi(e_1) \neq e_2$. Therefore, the second property must be explicitly stated for monoid homomorphisms.

However in the case of group homomorphisms, the existence of inverses ensures that there is only one element that can be idempotent under the group operation, which is the identity element. Thus, for group homomorphisms, the second property can be derived from the first property.

\begin{definition}[Ring Homomorphism]\label{def:ring_homomorphism}
  A \emph{ring homomorphism} is a set map $\phi \colon R_1 \to R_2$ between two rings $(R_1, +, \cdot)$ and $(R_2, \oplus, \odot)$ which respects the \hyperref[def:ringic_structure]{ringic structure}, i.e., for all $a, b \in R_1$, we have
  \begin{itemize}
    \item $\phi(a + b) = \phi(a) \oplus \phi(b)$;
    \item $\phi(a \cdot b) = \phi(a) \odot \phi(b)$;
    \item $\phi(\id_{R_1}) = \id_{R_2}$, where $\id_{R_1}$ and $\id_{R_2}$ are the multiplicative identity elements of $R_1$ and $R_2$ respectively.
  \end{itemize}
\end{definition}

\begin{remark}
  Originally, there are 6 properties in the definition of ring homomorphism, including the preservation of additive identity, additive inverses and commutative property. However, it can be shown that these properties are consequences of the first property. Also, we do not include the trivial ring homomorphism, as it does not preserve the multiplicative identity.
\end{remark}

On top of homomorphisms, we have special types of homomorphisms.

\begin{definition}[Endomorphism]\label{def:endomorphism}
  An \emph{endomorphism} is a homomorphism $\phi \colon A \to A$ from an algebraic structure to itself.
\end{definition}

\begin{definition}[Isomorphisms]\label{def:isomorphism}
  An \emph{isomorphism} is a homomorphism $\phi \colon A \to B$ between two algebraic structures that has an inverse homomorphism $\phi^{-1} \colon B \to A$ such that $\phi \circ \phi^{-1} = \id_{B}$ and $\phi^{-1} \circ \phi = \id_{A}$.
\end{definition}

\begin{definition}[Automorphism]\label{def:automorphism}
  An \emph{automorphism} is an \hyperref[def:isomorphism]{isomorphism} $\phi \colon A \to A$ from an algebraic structure to itself.
\end{definition}

Several maps can form a set as below.

\begin{definition}[Homomorphism Set]\label{def:homomorphism_set}
  Given two algebraic structures $A$ and $B$ of the same type, the \emph{homomorphism set} from $A$ to $B$, denoted by $\Hom(A, B)$, is the set of all homomorphisms from $A$ to $B$.
\end{definition}

\begin{definition}[Endomorphism Ring]\label{def:endomorphism_ring}
  Given an \hyperref[def:abelian_structure]{abelian group} $(G, +)$, the \emph{endomorphism ring} of $G$, denoted by $\End(G)$, is the set of all \hyperref[def:endomorphism]{endomorphisms} from $G$ to itself, equipped with the pointwise addition and composition of functions as the two binary operations. The two operations are defined as follows:
  \begin{align*}
    + \colon \End{G} \times \End{G}     & \to \End{G}                                                                     \\
    (\phi, \psi)                   & \mapsto (\phi + \psi) \colon G \to G, \quad (\phi + \psi)(a) = \phi(a) + \psi(a)     \\
    \circ \colon \End{G} \times \End{G} & \to \End{G}                                                                     \\
    (\phi, \psi)                   & \mapsto (\phi \circ \psi) \colon G \to G, \quad (\phi \circ \psi)(a) = \phi(\psi(a))
  \end{align*}
  The identity element for the addition operation is the zero map $0 \colon G \to G$ defined by $0(a) = 0_G$ for all $a \in G$, where $0_G$ is the identity element of the group $(G, +)$. The identity element for the composition operation is the identity map $\id_G \colon G \to G$ defined by $\id_G(a) = a$ for all $a \in G$.
\end{definition}

\begin{remark}
  Endomorphisms in $\End(G)$ are group homomorphisms since $(G, +)$ is an abelian group. So $\End(G) = \Hom(G, G)$.
\end{remark}

\section{Linear Spaces}

A linear space, or vector space, is a set with a linear structure defined over a field. We then need to define what a linear structure is.

\begin{definition}[Linear Structure]\label{def:linear_structure}
  A \emph{linear structure} on a set $V$ over a \hyperref[def:field]{field} $F$ is a pair of \hyperref[def:binary_operation]{binary operations} $(+, \cdot)$ where $(V, +)$ is an \hyperref[def:abelian_structure]{abelian group} with a ring action $\cdot$ of $F$ on $(V, +)$. A ring action of $F$ on $(V, +)$ is equivalent to a \hyperref[def:ring_homomorphism]{ring homomorphism}
  \begin{align*}
    \cdot \colon F & \to \End(V)                                                              \\
    \alpha    & \mapsto \alpha \cdot \colon V \to V, \quad (\alpha \cdot)(v) = \alpha \cdot v
  \end{align*}
\end{definition}

\begin{remark}
  The actual definition of a ring action of $F$ over $(V, +)$ is a map
  \begin{align*}
    \cdot \colon F \times V & \to V                  \\
    (\alpha, v)        & \mapsto \alpha \cdot v
  \end{align*}
  such that it satisfies the following four properties for all $\alpha, \beta \in F$ and $u, v \in V$:
  \begin{itemize}
    \item Distributivity over vector addition: $\alpha \cdot (u + v) = \alpha \cdot u + \alpha \cdot v$;
    \item Distributivity over field addition: $(\alpha + \beta) \cdot v = \alpha \cdot v + \beta \cdot v$;
    \item Compatibility: $(\alpha \beta) \cdot v = \alpha \cdot (\beta \cdot v)$;
    \item Unital: $1_F \cdot v = v$, where $1_F$ is the multiplicative identity element of the field $F$.
  \end{itemize}
\end{remark}

In usual textbooks, there are 8 axioms in the definition of linear structure. For all $\alpha, \beta \in F$ and $u, v \in V$:
\begin{enumerate}[label=\arabic*.]
  \item Addition is associative: $(u + v) + w = u + (v + w)$;
  \item Addition is unital: there exists an element $0_V \in V$ such that $0_V + v = v = v + 0_V$;
  \item Addition is invertible: for each $v \in V$, there exists an element $-v \in V$ such that $v + (-v) = 0_V = (-v) + v$;
  \item Addition is commutative: $u + v = v + u$;
  \item Distributivity over vector addition: $\alpha \cdot (u + v) = \alpha \cdot u + \alpha \cdot v$;
  \item Distributivity over field addition: $(\alpha + \beta) \cdot v = \alpha \cdot v + \beta \cdot v$;
  \item Compatibility of scalar multiplication: $(\alpha \beta) \cdot v = \alpha \cdot (\beta \cdot v)$;
  \item Identity element of scalar multiplication: $1_F \cdot v = v$, where $1_F$ is the multiplicative identity element of the field $F$.
\end{enumerate}
The first four axioms ensure that $(V, +)$ is an abelian group. The fifth axiom describes the distributivity inside $\End(V)$, while the last three axioms corresponds to the properties of ring homomorphism from $F$ to $\End(V)$. Thus, the 8 axioms can be reduced to the 2 conditions in the definition of linear structure.

\begin{example}
  The field $F$ itself can be considered as a linear space over $F$ with the usual addition and multiplication operations. Here, the set $V$ is $F$, the addition operation $+$ is the field addition, and the scalar multiplication $\cdot$ is the field multiplication.
\end{example}

\begin{example}\label{ex:function_space}
  The set of all $F$-valued functions defined on a non-empty set $X$, i.e., $\{ f \colon X \to F \}$, denoted by $\Map(X, F)$ or $F^X$, forms a linear space over $F$ with the following operations:
  \begin{align*}
    + \colon \Map(X, F) \times \Map(X, F) & \to \Map(X, F)                                                                    \\
    (f, g)                           & \mapsto (f + g) \colon X \to F, \quad (f + g)(x) = f(x) + g(x)                         \\
    \cdot \colon F \times \Map(X, F)      & \to \Map(X, F)                                                                    \\
    (\alpha, f)                      & \mapsto (\alpha \cdot f) \colon X \to F, \quad (\alpha \cdot f)(x) = \alpha \cdot f(x)
  \end{align*}
\end{example}

\begin{remark}
  In fact, as long as the codomain is a linear space, the set of all functions from a non-empty set to that codomain forms a linear space with pointwise addition and scalar multiplication.
\end{remark}

\begin{example}
  The set of all finitely supported $F$-valued functions defined on a non-empty set $X$, i.e., $\{ f \colon X \to F \mid f(x) \neq 0_F \text{ for only finitely many } x \in X \}$, denoted by $F[X]$ or $\Map_{\fin}(X, F)$ or $F^{(X)}$, forms a linear space over $F$ with the same operations as in the previous example.
\end{example}

\begin{example}
  The formal power series ring $F[[x]]$ over $F$ forms a linear space over $F$ with the usual addition and multiplication operations on formal power series. Formal means that we treat the elements as symbols without considering their convergence.
\end{example}

\begin{example}
  The polynomial ring $F[x]$ over $F$ forms a linear space over $F$ with the usual addition and multiplication operations on polynomials.
\end{example}

\begin{example}
  The set of all \emph{column vectors} with $n$ entries from $F$, denoted by $F^n$, forms a linear space over $F$ with the operations defined entrywisely.
  \begin{align*}
    + \colon F^n \times F^n                                                                                                  & \to F^n                                                                & \cdot \colon F \times F^n                                         & \to F^n                                                                              \\
    (\vec{u}, \vec{v})                                                                                                  & \mapsto \vec{u} + \vec{v}                                              & (\alpha, \vec{v})                                            & \mapsto \alpha \cdot \vec{v}                                                         \\
    \left(\begin{bmatrix} u^1 \\ \vdots \\ u^n \end{bmatrix}, \begin{bmatrix} v^1 \\ \vdots \\ v^n \end{bmatrix}\right) & \mapsto \begin{bmatrix} u^1 + v^1 \\ \vdots \\ u^n + v^n \end{bmatrix} & \left(\alpha, \begin{bmatrix} v^1 \\ \vdots \\ v^n \end{bmatrix}\right) & \mapsto \begin{bmatrix} \alpha \cdot v^1 \\ \vdots \\ \alpha \cdot v^n \end{bmatrix}
  \end{align*}
\end{example}

\begin{remark}
  Here, we use superscripts to denote the entries of a column matrix due to the elements in vectors are \emph{contravariant}. That is, when we change the basis, the coordinates of the vectors change in the opposite way compared to the basis transformation. This is in contrast to \emph{covariant} elements, such as the entries of row matrices (or covectors), which change in the same way as the basis transformation. We will discuss covariance and contravariance in Chapter 4.
\end{remark}

\begin{example}
  The set of all \emph{matrices} with $m$ rows and $n$ columns from $F$, denoted by $\Mat_{m \times n}(F)$, forms a linear space over $F$ with the operations defined entrywisely.
\end{example}

\section{Linear Subspaces, Linear Combinations and Linear Span}

\begin{definition}[Linear Subspace]\label{def:linear_subspace}
  A \emph{linear subspace} of a linear space $(V, +, \cdot)$ over $F$ is a non-empty subset $W \subseteq V$ with the operations $+$ and $\cdot$ inherited from $V$ such that $(W, +, \cdot)$ is also a linear space over $F$.
\end{definition}

\begin{proposition}
  $W$ is a linear subspace of $V$ if and only if $W$ is non-empty and closed under the operations $+$ and $\cdot$, i.e., for all $u, v \in W$ and $\alpha \in F$, we have
  \begin{itemize}
    \item $u + v \in W$;
    \item $\alpha \cdot v \in W$.
  \end{itemize}
\end{proposition}
\begin{proof}
  If $W$ is a linear subspace of $V$, then by definition $V$ is non-empty, as it contains the zero vector. Also, since $(W, +, \cdot)$ is a linear space, it must be closed under the operations $+$ and $\cdot$.

  If $W$ is non-empty and closed under the operations $+$ and $\cdot$, then we can easily verify that $(W, +, \cdot)$ satisfies all the axioms of a linear space over $F$. It is left as an exercise to the reader to check the axioms.
\end{proof}

We can actually combine two propoties into one by considering linear combinations.

\begin{definition}[Linear Combination]\label{def:linear_combination}
  A \emph{linear combination} of vectors $v_1, v_2, \ldots, v_n$ in a linear space $V$ over $F$ is any vector of the form
  \[
    \alpha^1 v_1 + \alpha^2 v_2 + \cdots + \alpha^n v_n,
  \]
  where $\alpha^1, \alpha^2, \ldots, \alpha^n$ are scalars in $F$.
\end{definition}

To use linear combinations showing the condition for linear subspaces, we can consider the following example. We normally use $n = 2$ to proof the condition, and the general case can be proved by induction.

\begin{proposition}
  The intersection of any collection of linear subspaces of a linear space $V$ over $F$ is also a linear subspace of $V$.
\end{proposition}
\begin{proof}
  Let ${\{ W_i \}}_{i \in I}$ be a collection of linear subspaces of $V$, where $I$ is an index set. Define
  \[ W = \bigcap_{i \in I} W_i. \]
  Then we have to show that $W$ is a linear subspace of $V$. For any $i \in I$, we have $0_V \in W_i$ since $W_i$ is a linear space. Thus, $0_V \in W$, so $W$ is non-empty. Then, for any $u, v \in W$ and $\alpha, \beta \in F$, we have $u, v \in W_i$ for all $i \in I$. Since each $W_i$ is a linear space, we have $\alpha u + \beta v \in W_i$ for all $i \in I$. Thus, $\alpha u + \beta v \in W$. Therefore, $W$ is closed under the operations $+$ and $\cdot$. By the previous proposition, $W$ is a linear subspace of $V$.
\end{proof}

Then it is natural to ask: the union of any collection of linear subspaces of a linear space $V$ over $F$ is also a linear subspace of $V$? The answer is no in general. However, if we perform ``completion'', or technically taking the \emph{linear span}, we can get a linear subspace again and it is called the \emph{sum} of those linear subspaces.

\begin{definition}[Linear Span]\label{def:span}
  The \emph{linear span} of a subset $S$ of a linear space $V$ over $F$, denoted by $\spn_F(S)$ or simply $\spn(S)$, $\overline{S}$ or $\langle S \rangle$, is the completion of $S$ inside $V$ under \hyperref[def:linear_combination]{linear combinations}, which is
  \[ \spn(S) = \left\{ \sum_{i = 1}^{|S|} \alpha^i s_i \Biggm| \alpha^i \in F, s_i \in S \right\} \]
  where $|S|$ is the cardinality of the set $S$ (if $S$ is infinite, we only consider finite linear combinations). Equivalently, the linear span of $S$ is the smallest \hyperref[def:linear_subspace]{linear subspace} of $V$ that contains $S$. It can be written as
  \[
    \spn(S) = \bigcap_{i \in I} W_i \subseteq V,
  \]
  where ${\{ W_i \}}_{i \in I}$ is the collection of all linear subspaces of $W$ that contain $S$.
\end{definition}

\begin{definition}[Linear Spanning Set]\label{def:spanning_set}
  A subset $S$ of a linear space $V$ over $F$ is a \emph{linear spanning set}, or \emph{linear generating set}, of $V$ if its \hyperref[def:span]{linear span} is equal to $V$, i.e., $\spn S = V$.
\end{definition}

For simplicity, we may omit the word ``linear'' when there is no ambiguity.

\begin{example}
  Consider the linear space $F^3$ with vectors $\vec{e}_1$, $\vec{e}_2$, and $\vec{e}_3$. The set $S = \{ \vec{e}_1, \vec{e}_2, \vec{e}_1 + \vec{e}_2 \}$ is not a spanning set of $F^3$ since $\spn(S)$ is the same as $\spn \{\vec{e}_1, \vec{e}_2\}$. However, the set $T = \{ \vec{e}_1, \vec{e}_1 + \vec{e}_2, \vec{e}_1 + \vec{e}_2, \vec{e}_3 \}$ is a spanning set of $F^3$ since $\spn(T) = F^3$.
\end{example}

\begin{remark}
  If you have learnt linear algebra before, consider the matrix whose columns are the vectors in a spanning set, then the matrix must have full row rank.
\end{remark}

\begin{example}
  Consider the subset $S = \{ 1, x, x^2, \cdots \} \subset F[[x]]$. The linear span of $S$ is the polynomial ring $F[x]$, i.e., $\spn(S) = F[x]$. The reason is that any polynomial can be expressed as a finite linear combination of the elements in $S$, while any formal power series that is not a polynomial cannot be expressed as such.
\end{example}

\begin{definition}[Minimal Spanning Set]\label{def:minimal_spanning_set}
  A minimal spanning set of a linear space $V$ over $F$ is a \hyperref[def:spanning_set]{spanning set} $S$ of $V$ such that for any proper subset $S' \subset S$, we have $\spn(S') \subset \spn(S) = V$.
\end{definition}

\begin{remark}
  An ordered minimal spanning set is called a \emph{basis}, which will use a round bracket notation, e.g., $(v_1, v_2, \ldots, v_n)$.
\end{remark}

\section{Linear Independence}

\begin{definition}[Linear Independence]\label{def:linear_independence}
  The non-trivial \hyperref[def:linear_subspace]{subspaces} $W_1, W_2, \ldots, W_n$ of a linear space $V$ over $F$ are \emph{linearly independent} if there is one and only one way to express the zero vector $0_V$ as a \hyperref[def:linear_combination]{linear combination} of vectors from these subspaces, i.e., if
  \[
    w_1 + w_2 + \cdots + w_n = 0_V,
  \]
  where $w_i \in W_i$ for each $i = 1, 2, \ldots, n$, then we must have $w_1 = w_2 = \cdots = w_n = 0_V$.
\end{definition}

We also have a slightly weaker version of linear independence for future discussions.

\begin{definition}[Weakly Linear Independence]\label{def:weakly_linear_independence}
  The \hyperref[def:linear_subspace]{subspaces} $W_1, W_2, \ldots, W_n$ of a linear space $V$ over $F$ are \emph{weakly linearly independent} if the only way to express the zero vector $0_V$ as a \hyperref[def:linear_combination]{linear combination} of vectors from these subspaces is the trivial way, i.e., if
  \[
    w_1 + w_2 + \cdots + w_n = 0_V,
  \]
  where $w_i \in W_i$ for each $i = 1, 2, \ldots, n$, then we must have $w_1 = w_2 = \cdots = w_n = 0_V$.
\end{definition}

\begin{remark}
  Weak linear independence allows subspaces to be trivial, i.e., equal to $\{ 0_V \}$.
\end{remark}

\begin{definition}[Linearly Independent Set]\label{def:linear_independent_set}
  A subset $S$ of a linear space $V$ over $F$ is linearly independent if and only if there is only one way to express the zero vector $0_V$ as a linear combination of vectors from $S$, i.e., if
  \[
    \alpha^1 s_1 + \alpha^2 s_2 + \cdots + \alpha^n s_n = 0_V,
  \]
  where $s_i \in S$ and $\alpha^i \in F$ for each $i = 1, 2, \ldots, n$, then we must have $\alpha^1 = \alpha^2 = \cdots = \alpha^n = 0_F$.
\end{definition}

\begin{remark}
  Equivalently, a subset $S$ of a linear space $V$ over $F$ is linearly independent if no elements in $S$ can be expressed as a linear combination of other elements in $S$.
\end{remark}

Also, similar to minimal spanning sets, we have the following definition.

\begin{definition}[Maximal Linearly Independent Set]\label{def:maximal_linearly_independent_set}
  A maximal linearly independent set of a linear space $V$ over $F$ is a \hyperref[def:linear_independent_set]{linearly independent set} $S$ of $V$ such that for any proper superset $S' \supset S$, we have $S'$ is not linearly independent.  
\end{definition}

\begin{remark}
  A minimal spanning set and a maximal linearly independent set describe the same concept. We will use minimal spanning sets in this book. 
\end{remark}

\begin{example}
  Let $X$ be a non-empty set. For each $x \in X$, define the Kronecker delta function $\delta_x \colon X \to F$ by
  \begin{equation}\label{eq:delta_function}
    \delta_x(t) = \begin{cases}
      1, & \text{if } t = x; \\
      0, & \text{if } t \neq x.
    \end{cases}
  \end{equation}
  Clearly, $\delta_x$ is in $F[X]$ since it is finitely supported. The set $\deltaX = \{ \delta_x \mid x \in X \}$ is a linearly independent set in the linear space $F[X]$ over $F$. To show this, assume there exists a finite linear combination of other delta functions such that $\delta_x = \sum \alpha^t \delta_t$. Then we have $\delta_x(x) = 1$ and $\delta_x(x) = \sum \alpha^t \delta_t(x) = 0$, which shows a contradition. Moreover, it is a minimal spanning set of $F[X]$ since any finitely-supported function can be expressed as a finite linear combination of the functions in this set.
\end{example}

\section{Sum and Direct Sum of Linear Subspaces}

As we have mentioned before, the linear span of the union of several linear subspaces is again a linear subspace, which is called the sum of those linear subspaces.

\begin{definition}[Sum of Linear Subspaces]\label{def:sum}
  The \emph{sum} of the \hyperref[def:linear_subspace]{linear subspaces} $W_1, W_2, \ldots, W_n$ of a linear space $V$ over $F$, denoted by $W_1 + W_2 + \cdots + W_n$, is the \hyperref[def:span]{linear span} of their union, i.e.,
  \[
    W_1 + W_2 + \cdots + W_n = \spn(W_1 \cup W_2 \cup \cdots \cup W_n).
  \]
  Equivalently, the sum can be expressed as
  \[
    W_1 + W_2 + \cdots + W_n = \{ w_1 + w_2 + \cdots + w_n \mid w_i \in W_i, i = 1, 2, \ldots, n \}.
  \]
\end{definition}

\begin{definition}[Internal Direct Sum of Linear Subspaces]\label{def:internal_direct_sum}
  The \emph{internal direct sum} of the \hyperref[def:linear_subspace]{linear subspaces} $W_1, W_2, \ldots, W_n$ of a linear space $V$ over $F$, denoted by $W_1 \oplus W_2 \oplus \cdots \oplus W_n$, is their \hyperref[def:sum]{sum} $W_1 + W_2 + \cdots + W_n$ provided that the subspaces are \hyperref[def:weakly_linear_independence]{weakly linearly independent}, i.e., 
  \[
    W_1 \oplus W_2 \oplus \cdots \oplus W_n = W_1 + W_2 + \cdots + W_n,
  \]
  and for any $w \in W_1 \oplus W_2 \oplus \cdots \oplus W_n$, there exist unique vectors $w_i \in W_i$ for each $i = 1, 2, \ldots, n$ such that
  \[
    w = w_1 + w_2 + \cdots + w_n.
  \]
\end{definition}

The equivalent definition of internal direct sum is that the intersection of any subspace with the sum of the other subspaces is trivial, i.e., for each $i = 1, 2, \ldots, n$,
\[
  W_i \cap \left( \sum_{j \neq i} W_j \right) = \{ 0_V \}.
\]

There is also an \emph{external} version of direct sum which constructs a new linear space from several linear spaces. We normally use the symbol $=$ to denote internal direct sum decomposition such as $V = W_1 \oplus W_2 \oplus \cdots \oplus W_n$ and use the symbol $\cong$ to denote the external direct sum isomorphic to a linear space such as $V \cong W_1 \oplus W_2 \oplus \cdots \oplus W_n$.

\begin{definition}[External Direct Sum of Linear Spaces]\label{def:external_direct_sum}
  The \emph{external direct sum} of the linear spaces $W_1, W_2, \ldots, W_n$ over a field $F$, denoted by $W_1 \oplus W_2 \oplus \cdots \oplus W_n$, is the linear space defined as
  \[
    W_1 \oplus W_2 \oplus \cdots \oplus W_n = \{ (w_1, w_2, \ldots, w_n) \mid w_i \in W_i, i = 1, 2, \ldots, n \},
  \]
  with the vector addition and scalar multiplication defined componentwisely, i.e., for any $(w_1, w_2, \ldots, w_n), (w_1', w_2', \ldots, w_n') \in W_1 \oplus W_2 \oplus \cdots \oplus W_n$ and $\alpha \in F$,
  \begin{align*}
    (w_1, w_2, \ldots, w_n) + (w_1', w_2', \ldots, w_n') & = (w_1 + w_1', w_2 + w_2', \ldots, w_n + w_n'),                     \\
    \alpha \cdot (w_1, w_2, \ldots, w_n)                 & = (\alpha \cdot w_1, \alpha \cdot w_2, \ldots, \alpha \cdot w_n).
  \end{align*}
\end{definition}

For internal direct sum, $W_1 \oplus W_2 = \{ w_1 + w_2 \mid w_i \in W_i, i = 1, 2 \}$ with the vector addition and scalar multiplication inherited from $V$. For external direct sum, $W_1 \oplus W_2 = \{ (w_1, w_2) \mid w_i \in W_i, i = 1, 2 \}$ with the vector addition and scalar multiplication defined componentwisely.

\clearpage{}

\section{Exercise}

\begin{problem}
  On the logic set $X = \{\mathsf{true}, \mathsf{false}\}$, we have two binary operations: one is ``OR'', denoted by $\lor$, and the other is ``AND'',denoted by $\land$. If we use 1 to represent ``true'' and 0 to represent ``false'', then
  \begin{align*}
    1 \lor 0 = 0 \lor 1 = 0 \lor 0 = 0, \qquad 1 \lor 1 = 1 \\
    1 \land 1 = 1 \land 0 = 0 \land 1 = 0, \qquad 0 \land 0 = 0
  \end{align*}
  \begin{enumerate}
    \item Show that both $\lor$ and $\land$ are abelian monoid structures on $X$.
    \item Show that $\lor$ distributes with respect to $\land$.
    \item Show that $\land$ distributes with respect to $\lor$.
    \item Is $(X, \lor, \land)$ a ring? If not, can you modify $\lor$ to arrive at a new binary operation $\lor'$ such that $(X, \lor', \land)$ is a commutative ring with unity? If yes, is this ring a field?
  \end{enumerate}
\end{problem}

\begin{problem}
  Find a non-empty subset $X$ of $2 \times 2$ matrices over $\R$ such that 
  \begin{itemize}
    \item the set $X$ is closed under matrix multiplication, and 
    \item there are many left-identities, but there is no two-sided identity.
  \end{itemize}
\end{problem}

\begin{problem}\label{prob:function_space_homomorphisms}
  Let $F$ be a field and $X$ be a non-empty set. Recall that $\Map(X, F)$ is the set of $F$-valued functions on $X$ and $F[X]$ is the set of finitely-supported $F$-valued functions on $X$. Both $\Map(X, F)$ and $F[X]$ are linear spaces over the field $F$.

  Let $T \colon X \to Y$ be a set map and $T_* \colon F[X] \to F[Y]$ be the map such that 
  \[
      T_*(f)(y) = \sum_{x \in T^{-1}(y)} f(x), \qquad \forall y \in Y.
  \]

  In case $T^{-1}(y)$ is the empty set $\emptyset$, the sum is assumed to be 0. Please check that the sum above is well-defined and $T_*(f)$ has a finite-support.
  \begin{enumerate}
    \item Show that ${(1_X)}_* = 1_{F[X]}$ for all non-empty set $X$.
    \item Show that ${(TS)}_* = T_* S_*$ for all set maps $T$ and $S$ such that the composition $TS$ is defined.
    \item For any set map $T \colon X \to Y$, we have an induced map $T^* \colon \Map(Y, F) \to \Map(X, F)$ via the formula $T^*f = fT$. Show that $1_X^* = 1_{\Map(X, F)}$ and ${(TS)}^* = S^*T^*$.
    \item Can we get a natural map from $\Map(X, F)$ to $\Map(Y, F)$ or from $F[Y]$ to $F[X]$ for any set map $T \colon X \to Y$ between two infinite sets $X$ and $Y$?
  \end{enumerate}
\end{problem}

\begin{problem}
  Let $V$ be a linear space and $S$ be a spanning set for $V$. Show that $S$ is a minimal spanning set for $V$ $\iff$ $S$ is a linearly independent set. Note: $S$ here is not required to be finite.
\end{problem}